---
title: "Statistics"
output: html_notebook
---

## Load packages
```{r setup}
## Load Necessary packages
sapply(c("rentrez", "phyloseq", "tidyverse", "spider", "insect", "ape", "DECIPHER", "ggpubr", "RColorBrewer", "plotly", "ggforce", "patchwork", "viridis","ggridges","UpSetR"), require, character.only = TRUE)


# install.packages("devtools")
#devtools::install_github("alexpiper/taxreturn")
library(taxreturn)

## Get an NCBI api key for taxize queries
# use_entrez()

# After generating your key set it as ENTREZ_KEY in .Renviron.
# ENTREZ_KEY='1c0a0c4afa28448650a1450662a22c68f208'
#usethis::edit_r_environ()
```

## Make Phyloseq object

Following taxonomic assignment, the sequence table and taxonomic table are merged into a single phyloseq object alongside the sample info csv.

We then make a plot to evaluate the effectiveness of taxonomic assignment to each rank

```{r create PS, eval = FALSE}
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")

#Fix seqtab names -removing read name, sample number etc
rownames(seqtab.nochim) <- rownames(seqtab.nochim) %>% 
  str_split_fixed("_",n=Inf) %>%
    as_tibble() %>% 
  separate(V7, into="rep", sep = "\\.", extra = "drop") %>%
  unite(col=SampleID, c("V2","rep"),sep="-") %>%
  pull(SampleID) %>%
  str_replace(pattern="Rep", replacement="rep")

tax_plus <- readRDS("output/rds/tax_IdTaxaExact.rds") 

#### Rename problematic samples
rownames(seqtab.nochim)  <- rownames(seqtab.nochim) %>%
 str_replace_all("D250M1-", "D250M4REP-") %>% #Dont think this is necessart
 str_replace_all("D250M2-", "D250M5REP-") %>% # Makes sense on PCA
 str_replace_all("D250M3-", "D250M1REP-") %>% # Way off on PCA
 str_replace_all("D250M4-", "D250M2REP-") %>%
 str_replace_all("D250M5-", "D250M3REP-") %>%
 str_replace_all("D500M1-", "D500M4REP-") %>%
 str_replace_all("D500M2-", "D500M5REP-") %>%
 str_replace_all("D500M3-", "D500M1REP-") %>%
 str_replace_all("D500M4-", "D500M2REP-") %>%
 str_replace_all("D500M5-", "D500M3REP-") %>% # This should maybe be M2?
 str_replace_all("D1000M1-", "D1000M3REP-") %>%
 str_replace_all("D1000M2-", "D1000M4REP-") %>%
 str_replace_all("D1000M3-", "D1000M5REP-") %>%
 str_replace_all("D1000M4-", "D1000M1REP-") %>%
 str_replace_all("D1000M5-", "D1000M2REP-") %>%
 str_replace_all("REP", "")

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) 
#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))

if(nrow(seqtab.nochim) > nrow(sample_data(ps))){warning("Warning: All samples not included in phyloseq object, check sample names match the sample metadata")}

rownames(samdf)[which(!rownames(sample_data(ps))  %in% rownames(samdf))]

saveRDS(ps, "output/rds/ps_idtaxaExact.rds") 

#Rename synthetic orders
tax_table(ps)[,2][which(str_detect(tax_table(ps)[,7], "Synthetic"))] <- "Arthropoda"

ps <- ps %>%
  subset_samples(material %in% c("Drosophila Adults", "Drosophila Larvae", "Mixed Adults", "Mixed Larvae", "Synthetic", "Blank")) %>%
  subset_taxa(Phylum == "Arthropoda") %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) 

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

##Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
library(data.table)
summarise_taxa(ps, "Species", "sample_id") %>%
  filter(str_detect(sample_id, "NTC")) %>%
  #filter(str_detect(Species, "Drosophila|Scaptodrosophila")) %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

summarise_taxa(ps, "Genus", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

##Output fasta of all ASV's - Name each one by abundance + Taxonomic assignment

ps_to_fasta(ps, "output/all_taxa.fasta", rank="Species")
```


### Summarise taxonomic assignment

```{r sum taxa}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name", rank_names(ps)) %>%
  group_by(Rank) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(Reads_classified = sum(Abundance * !is.na(Name))) %>%
  mutate(Frac_reads = Reads_classified / sum(sample_sums(ps))) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

#Fraction of ASV's assigned to each taxonomic rank
sum_otu <- tax_table(ps) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

print(sum_reads)
print(sum_otu)


ps_test <- speedyseq::tax_glom(ps, "Species")
sum_test <- tax_table(ps_test) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)


unique_sp <- unique(tax_table(ps_test)[,7]) %>% unname() %>% as.data.frame() %>% filter(!str_detect(V1, "__"))

```


## Apply detection criteria

# Determine filtering threshold

We will use 3 different methods to estimate the cross contamination rate:

* Firstly we will use the synthetic mock community positive control
* Secondly we will use the estimate from the number of expected vs unapplied index combinations
* Thirdly, we will use ROC analysis of false positives and false negatives for all taxa in the mocks.

## estimate cross contamination using positive control

## Process synthetic mock positive control

Identification of taxa that are poorly represented in an unsupervised manner can identify taxa that will have little to no effect on downstream analysis. Sufficient removal of these "low prevalance" features can enhance many analysis by focusing statistical testing on taxa common throughout the data. However, for our dataset the problem with prevalence filtering for our dataset is that we dont have many replicates of each psyllid species, and therefore there are some high abundance but low prevalence ASV's we dont want to lose.


```{r}
#Process positive controls
Syn_taxa <- c("Synthetic_Acrididae", "Synthetic_Aphididae", "Synthetic_Apidae", "Synthetic_Cerambycidae", "Synthetic_Crambidae", "Synthetic_Culicidae","Synthetic_Drosophilidae","Synthetic_Nitidulidae","Synthetic_Siricidae","Synthetic_Tephritidae", "Synthetic_Thripidae", "Synthetic_Tortricidae", "Synthetic_Triozidae")
#
#, "Carpophilus_Synthetic1", "Carpophilus_Synthetic2", "Carpophilus_Synthetic3", "Carpophilus_Synthetic4", "Carpophilus_Synthetic5", #"Carpophilus_Synthetic6","Drosophila_Synthetic1", "Drosophila_Synthetic2", "Drosophila_Synthetic3", "Drosophila_Synthetic4", #"Drosophila_Synthetic5", "Drosophila_Synthetic6")
#Estimate switching from positive controls
pos_switchrate <- psmelt(subset_taxa(ps, Species %in% Syn_taxa)) %>%
  group_by(geo_loc_name) %>%
  summarise(Abundance = sum(Abundance))

##Plot synthetics
ps_syn <- subset_samples(ps, material=="Synthetic") %>%
  subset_taxa(Species %in% Syn_taxa) %>%
  filter_taxa(function(x) mean(x) > 0, TRUE)# %>% #Drop missing taxa from table
  microbiome::transform('compositional')

#Colour scheme
library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(13)

gg.syn <- plot_bar(ps_syn, fill="Species") +
  facet_grid(~target_subfragment,scales="free") + 
  scale_fill_manual(values=col) +
  theme_bw()
```

# Estimate rate using Expected vs observed combinations


# Estimate rate using known exp vs observed in mocks

From the use of the mock community we can see that the majority cross contamination must have come from DNA extraction and prior processes. Therefore we will also get an estimate 

```{r thresholds}
#Flag True positives

# Read in expected table
exp <- read_csv("sample_data/expected_quant.csv") %>%
  gather(Species, Abundance, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  filter(str_detect(X1,"D100M|D250M|D500M|D1000M|DLarv")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Species","Actual"))

# Mutate real one
sam <- ps %>%
  transform_sample_counts(function (x) x/sum(x)) %>%
  speedyseq::tax_glom("Species") %>%
  speedyseq::psmelt() %>%
  mutate(Species = str_replace(Species, pattern="Drosophila_mauritiana/simulans", replacement= "Drosophila_simulans")) %>%
  filter(Sample %in% exp$Sample) %>%
  arrange(Abundance) %>%
  distinct() %>%
  left_join(exp, by=c("Sample", "Species")) %>%
  mutate(switched = case_when(
    Abundance > 0 & Actual == 0 ~ TRUE,
    Abundance == 0 & is.na(Actual) ~ FALSE,
    Abundance > 0 & Actual > 0 ~ FALSE
  ))

# Need to make a confusion matrix instead

## Plot true and false carsonellas by run - this shows may need to filter seperately by run, as seqrun2 has much higher than others.
gg.switch <- sam %>% 
  group_by(switched) %>%
  filter(!is.na(switched)) %>%
  filter(Abundance > 0) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ggplot(aes(x=seqrun, y=freq, group=switched, fill=switched)) +
  geom_bar(stat="identity")


#to check if index switching could explain the multiple OTU's check if top=FALSE carsonella exist as top in another sample in the same run

thresholds <- seq(0,0.1,0.0001)

df <- data.frame(thresh = thresholds, TP= thresholds, FP = thresholds)
for(i in 1:length(thresholds)){
  filt <- sam %>%
    filter(Abundance > thresholds[i]) %>%
    group_by(switched) %>%
    summarise(sum=n())
  df$FP[i] <- filt$sum[2]
  df$TP[i] <- filt$sum[1]
}

gg.filt <- df %>%
  bind_rows() %>%
  gather(key=type, value=n, -thresh) %>%
  ggplot(aes(x=thresh, y=n, fill=type)) + 
  geom_density(stat="identity", alpha=0.5) + 
  #scale_x_continuous(breaks=seq(0,0.05,0.001)) +
  theme(axis.text.x = element_text(angle=45,hjust=1)) + 
  geom_vline(xintercept = 0.0035)+ 
  geom_vline(xintercept = 0.001)

print(gg.filt)

## Filter run 2 
run2_ps <- ps2 %>% 
  subset_samples(seqrun==2)

run2_pass <- run2_ps %>%
    transform_sample_counts(function (x) x/sum(x)) %>%  # Convert to proportions
    transform_sample_counts(function (x) (x > 0.001) * 1)

newotu1 <- otu_table(run2_ps) * otu_table(run2_pass)

run2_newps <- run2_ps
otu_table(run2_newps) <- otu_table(newotu1, taxa_are_rows = FALSE) 

#Filter run 1 and 3 
  
run13_ps <- ps2 %>%
    subset_samples(!seqrun == 2)
                   
run13_pass <- run13_ps %>%
    transform_sample_counts(function (x) x/sum(x)) %>%  # Convert to proportions
    transform_sample_counts(function (x) (x > 0.0005) * 1)

newotu2  <- otu_table(run13_ps) * otu_table(run13_pass)

run13_newps <- run13_ps
otu_table(run13_newps) <- otu_table(newotu2, taxa_are_rows = FALSE) 

# Create new phyloseq and drop missing taxa
ps3 <- merge_phyloseq(run2_newps, run13_newps) %>%
  filter_taxa(function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps3 <- prune_samples(sample_sums(ps3) >0 , ps3) # Drop empty samples

#Count number of overall taxa pre and post filtering
print(paste(ntaxa(ps2) - ntaxa(ps3), " taxa Dropped when using filtering threshold of: ", 0.0035, " for run 2 and ", 0.001, "for run 1 and 3"))

# Count number of carsonella OTU's per sample pre and post filtering
n_carson <- speedyseq::psmelt(ps2) %>% 
  filter(Genus == "Candidatus_Carsonella") %>%
  filter(Abundance > 0) %>%
  dplyr::select(Sample.Name, OTU) %>%
  group_by(Sample.Name) %>%
  add_tally() %>%
  dplyr::select(-OTU) %>%
  unique() %>%
  mutate(type = "pre") %>%
  bind_rows(.,speedyseq::psmelt(ps3) %>% 
  filter(Genus == "Candidatus_Carsonella") %>%
  filter(Abundance > 0) %>%
  dplyr::select(Sample.Name, OTU) %>%
  group_by(Sample.Name) %>%
  add_tally() %>%
  dplyr::select(-OTU) %>%
  unique() %>%
  mutate(type = "post"))

gg.ncarson <- ggplot(n_carson, aes(x=reorder(Sample.Name, -n), y=n)) + 
  geom_bar(stat="identity") +
  facet_grid(~type) + 
  xlab("Sample.Name") +
  ylab("Number of carsonella OTU's per sample") +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0))

print(gg.ncarson)

#check if any samples dont have carsonella
table(!n_carson$Sample.Name %in% speedyseq::psmelt(ps2)$Sample.Name)

## Get the name of taxa that dont have carsonella after filtering

```


## Occupancy modelling using replicates

### eDNAoccupancy
```{r edna occupancy}

library(eDNAoccupancy)
data(fungusDetectionData)
data(fungusSurveyData)

# Make detection table
ps_qual <- ps %>%
  speedyseq::tax_glom(taxrank="Species") %>%
  transform_sample_counts(function (x) (x > 0) * 1) %>%
  #merge_samples(group = "ExtractID") %>%
  speedyseq::psmelt() %>%
  select(Sample, Abundance, Species) %>%
  separate(Sample, into=c("site", "rep"), sep="-rep") %>%
  mutate(rep=paste0("pcr",rep)) %>%
  separate(site, into=c("site", "sample"), sep="-ex") %>%
  pivot_wider(names_from=rep, values_from = Abundance, values_fill=list(Abundance = 0)) 

det_suzukii <- ps_qual %>%
  mutate(site = str_replace(site, "DM", "D100M")) %>%
  filter(Species=="Drosophila_suzukii") %>%
  filter(str_detect(site,"D100M|D250M|D500M|D1000M")) %>%
  select(site, sample, pcr1, pcr2, pcr3) %>%
  mutate(sample = as.integer(sample)) %>%
  arrange(sample) %>%
  as.data.frame()

suzukii_detections = occData(det_suzukii, siteColName = 'site',
                            sampleColName = 'sample')

#Make covariate tables

# Read in expected table
exp <- read_csv("sample_data/expected_quant.csv") %>%
  gather(Species, Actual, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  filter(str_detect(X1,"D100M|D250M|D500M|D1000M|DLarv")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Species","Actual"))
# Add true presence

#Add community size covariate

commsize <- exp %>%
  group_by(Sample) %>%
  summarise(commsize = sum(Actual)) %>%
  mutate(Sample = str_replace_all(Sample, pattern="(-)(.*?)(?=$)", replacement=""))

survey_data <- samdf %>%
  remove_rownames() %>%
  mutate(Sample = str_replace_all(ExtractID, pattern="(-)(.*?)(?=$)", replacement="")) %>%
  select(Sample, geo_loc_name, material) %>%
  left_join(commsize, by="Sample") %>%
  rename(site = Sample) %>%
  filter(str_detect(site,"D100M|D250M|D500M|D1000M")) %>%
  unique()

# Add alpha diversity covariates

# Try using hill numbers for these https://github.com/anttonalberdi/hilldiv

#estimate richness
alpha_table <- phyloseq::estimate_richness(ps2)

# Calculate Faith's PD-index
pdtable <- picante::pd(as(phyloseq::otu_table(ps2), "matrix"), phyloseq::phy_tree(ps2), include.root = F)




 ## number of detections per sample
 head(suzukii_detections$y)
 ## number of PCR replicates per sample
 head(suzukii_detections$K)

#We fit a multi-scale occupancy model without covariates and print a summary of the parameter estimates using the following code.

 set.seed(69)
 fit = occModel(detectionMats=suzukii_detections, niter=11000,
                niterInterval=5000)
 posteriorSummary(fit, burnin=1000, mcError=TRUE)


## Center and scale numeric-valued covariate measurements
survey_data.sc = scaleData(survey_data)

set.seed(0157)
fit = occModel(formulaSite          = ~ 1,
               formulaSiteAndSample = ~ commsize,
               formulaReplicate     = ~ commsize,
               detectionMats        = suzukii_detections,
               siteData             = survey_data.sc,
               niter                = 6000,
               niterInterval        = 2000,
               siteColName = 'site'
               )
posteriorSummary(fit, burnin=1000, mcError=TRUE)


#If we want to assess whether the Markov chain used to compute these estimates appears to have converged, trace plots of the parameters may be created as follows (Fig.~\ref{fig:TracePlotFungusAnalysis}).
plotTrace(fit, c('beta.(Intercept)', 'alpha.(Intercept)', 'alpha.frogs',
            'delta.(Intercept)'),  burnin=1000)

#Autocorrelation plots of the parameters are created similarly
 plotACF(fit, c('beta.(Intercept)', 'alpha.(Intercept)', 'alpha.frogs',
             'delta.(Intercept)'),  burnin=1000)

#After inspection of these plots, suppose we decide that the MCMC algorithm needs to be run longer, either to eliminate the transient portion of the Markov chain or to reduce Monte Carlo errors in the parameter estimates.  We can resume the MCMC algorithm for the currently fitted model as follows.
 fit = updateOccModel(fit, niter=5000, niterInterval=2000)
 posteriorSummary(fit, burnin=1000,  mcError=TRUE)

 #These estimates of the parameters are computed using the updated Markov chain containing \rinline{fit$niterations} iterations.  The Monte Carlo errors in these parameter estimates are slightly lower, but the estimates are otherwise similar to those computed with only \rinline{fit$niterations-5000} iterations.

#In addition to estimating posterior summaries of the model's formal parameters, we also may be interested in estimating posterior summaries of derived parameters.  For example, in the second model fitted to the \emph{Bd} data, the probability of eDNA occurrence in ponds was assumed to be constant ($\psi$), the conditional probability of eDNA occurrence in samples was assumed to be a function of the frog density index \code{frogs}, and the conditional probability of eDNA detection was assumed to be constant ($p$).  The posterior medians of these derived parameters are estimated as follows.

psi = posteriorSummaryOfSiteOccupancy(fit, burnin=1000)
theta = posteriorSummaryOfSampleOccupancy(fit, burnin=1000)
p = posteriorSummaryOfDetection(fit, burnin=1000)

 ## output estimates of posterior medians
 cbind(psi=psi$median, theta=theta$median[,1], p=p$median[,1])

 frogs = fungusSurveyData[, 'frogs']
 plot(frogs, theta$median[,1], ylim=c(0,1), xlim=c(0,0.8), cex=2)
 segments(frogs, theta$lower[,1], frogs, theta$upper[,1], lwd=2)
 
#One way to assess the relative importance of such estimated relationships is to compare competing models using model-selection criteria.  For example, we compute the PPLC and WAIC criteria for the previously fitted model as follows.
 posteriorPredictiveLoss(fit, burnin=1000)
 WAIC(fit, burnin=1000)

```

###Seak occupancy

Seak occupancy modelling - The model is fitted within a Bayesian framework and any of the model parameters can be functions of covariates. The implemented algorithm performs Bayesian variable selection and the output includes posterior summaries of all parameters as well as posterior probabilities of inclusion (see examples for a more detailed description of how to interpret the output).

The model has been developed for single species qPCR data. The data are the number of positive qPCRs (eDNA score) for each water sample collected at surveyed sites. The model allows us to estimate the probability of species presence at each survey site, while accounting for the probabilities of a false positive and false negative error at stage 1 (field) and stage 2 (lab).



Occupancy modelling calculates the probability of detection from a number of replicates. ie - 6 replicates positive = 100% probaiblity , 5/6 = 85% probability

Nested levels = Replicated extractions, replicated PCRs .

Could fit a single specied


For model taking into accoutn false poositives and negatives: https://seak.shinyapps.io/eDNA/

This requires format

Columns = Sample (extraciton replicate ) so 2 columns
Rows = site (trap)
cell value = number of positive PCR replicates (out of 3)
Column 3 = True presence or absense 1 or 0
column 4,5,6 etc other covariates. Sequencing depth, community size, species richness,trap type, phylogenetic richness... etc



```{r occupancy}
#convert to presence/absense
ps_qual <- ps %>%
  speedyseq::tax_glom(taxrank="Species") %>%
  transform_sample_counts(function (x) (x > 0) * 1) %>%
  merge_samples(group = "ExtractID") %>%
  speedyseq::psmelt() %>%
  dplyr::select(Sample, Abundance, Species) %>%
  separate(Sample, into=c("Sample", "exrep"), sep="-ex") %>%
  pivot_wider(names_from=exrep, values_from = Abundance, values_fill=list(Abundance = 0)) %>%
  dplyr::select(Sample, Species, `1`, `2`,)

# Read in expected table
exp <- read_csv("sample_data/expected_quant.csv") %>%
  gather(Species, Actual, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  #filter(str_detect(X1,"D100M|D250M|D500M|D1000M|DLarv")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Species","Actual"))
# Add true presence

true_pres <- exp %>%
  mutate(Sample = str_replace_all(Sample, pattern="(-)(.*?)(?=$)", replacement="")) %>%
  mutate(Actual = case_when(
    Actual > 0 ~ 1,
    Actual == 0 ~ 0
  )) %>%
  unique()
  
#Add community size covariate - need to estimate for others

commsize <- exp %>%
  group_by(Sample) %>%
  summarise(commsize = sum(Actual)) %>%
  mutate(Sample = str_replace_all(Sample, pattern="(-)(.*?)(?=$)", replacement=""))

# add seqdepth covariate

#filterdepth

#richness
richness <- ps_qual %>%
  mutate(Sample = str_replace(Sample, "DM", "D100M")) %>%
  ungroup() %>%
  mutate(total = `1`+`2`) %>%
  filter(total > 0) %>%
  group_by(Sample) %>%
  summarise(richness= n())  

# Species evenness

# Phylogenetic diversity


#trap_type
comm_type <- exp %>%
  mutate(Sample = str_replace_all(Sample, pattern="(-)(.*?)(?=$)", replacement="")) %>%
  mutate(comm_type = case_when(
    str_detect(Sample, "D100M|D250M|D500M|D1000M") ~ "Mock",
    str_detect(Sample, "DLarv") ~ "Larvae",
    str_detect(Sample, "ACV") ~ "ACV",
    str_detect(Sample, "FF") ~ "FF",
    str_detect(Sample, "SPD") ~ "SPD",
    str_detect(Sample, "DC") ~ "DC",
  )) %>%
  dplyr::select(Sample, comm_type) %>%
  unique()
  

# Get suzukii only
det_suzukii <- ps_qual %>%
  mutate(Sample = str_replace(Sample, "DM", "D100M")) %>%
  left_join(true_pres, by=c("Sample", "Species"))%>%
  left_join(commsize, by="Sample") %>%
  left_join(richness, by="Sample") %>%
  left_join(comm_type, by="Sample") %>%
  filter(Species == "Drosophila_suzukii") %>%
  dplyr::select(-Species) %>%
  unique() %>%
  mutate(Actual = replace_na(Actual, 0))  %>%
  filter(!is.na(commsize)) %>%
  mutate(commsize = scale(commsize, center = TRUE, scale = TRUE))  %>%
  mutate(random = scale(rbinom(nrow(.), 100, 0.7), center = TRUE, scale=TRUE)) %>%
  mutate(random2 = scale(rbinom(nrow(.), 100, 0.4), center = TRUE, scale=TRUE)) %>%
  mutate(richness = scale(richness, center = TRUE, scale = TRUE))  %>%
  filter(!str_detect(Sample, "DLarv")) %>%
  #set_rownames(.$Sample) #%>%
  dplyr::select(-Sample)
  
write.csv(det_suzukii, "test_occupancy_suzukii.csv", row.names=FALSE)

```

For sequencing depth covariate could you just rarefy to certain depth, recalculate detection, then rbind a longer table together in a loop
Same with filtering threshold



these could be fit seperately for our 3 target species

Then we can look at how the detection probability changes across different filtering thresholds used to remove index switching?


To test if probability of detection differs as a function of sample water volume, we used the eDNAoccupancy R package (version 0.2.4; Dorazio & Erickson, 2017) to model probabilities of eDNA detection. This package fits Bayesian, multi‐scale occupancy models to our data, which included three, nested levels of sampling: stream location, replicated water samples collected from each stream, and subsamples (i.e., PCR technical replicates) of each water sample. 



https://onlinelibrary.wiley.com/doi/full/10.1002/edn3.23

https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00156.x

https://www.pnas.org/content/pnas/116/18/8931.full.pdf

Based on the overall model, a total of seven water samples was
required to achieve >95% detection probabilities of S. mansoni
eDNA at water sample level (θ = 0.35) [as calculated by using
the equation P = 1 − (1 − θ)
n (27)]. By using the same approach,
the model-based estimated number of qPCR replicates required
to achieve detection probabilities >95% ranged from three to
nine replicates between sites



## Process replicates 

In this section we will estimate within sample consistency using Kulczynski distance, which is a presence/absense distance measure.

To ensure the reproducibility of detection, all PCR replicates that had a high Kulczynski distance to other replicates within the same sample were removed.

Following this, we only retained ASV's that were present in at least 2 different replicates from each sample

Adapted from Mike mclarens code: https://github.com/benjjneb/dada2/issues/745

This could probably go before taxonomic assignment?

## Look at taxon overlap between replicates

Venn diagrams? - Heatmap of presence absense

Species accumulation curve- adding more extraction reps, pcr reps, biological samples, sequence depth

Want to show - Different extraction replicates adds more - but this is probably due to contamination
PCR replicates doesnt add more


## Look at species accumulation curves
```{r replicate reproducibility}

#Rarefy replicates to same read depth
nspec <- specnumber(otu_table(ps1)) # observed number of species
raremax <- min(rowSums(otu_table(ps.reps)))
rare <- rrarefy(otu_table(ps.reps),raremax)

#PCA of rarefied samples
raredist <- vegan::vegdist(rare, method="euclidean")

r.pcx <- prcomp(raredist)

pc_samp <- data.frame(SampleID = rownames(r.pcx$x), r.pcx$x[, 1:2])%>%
  left_join(samdf, by="SampleID")
pc_otu <- data.frame(OTU = rownames(r.pcx$rotation), r.pcx$rotation[, 1:2])

# calculate percent variance explained for the axis labels
pc1 <- round(r.pcx$sdev[1]^2/sum(r.pcx$sdev^2),2)
pc2 <- round(r.pcx$sdev[2]^2/sum(r.pcx$sdev^2),2)
pc_xlab <- paste("PC1: ", pc1, sep="")
pc_ylab <- paste("PC2: ", pc2, sep="")

library(ggplot2)
gg.reps <- ggplot(data=pc_samp, aes(x=PC1, y=PC2, colour=SampleID, label=SampleID)) + 
  geom_point(alpha=0.5, size=3) +
  geom_text() +
  #geom_point(data=pc_otu,aes(PC1, PC2)) +
  theme_bw() + 
  geom_hline(yintercept = 0, linetype=2) +  
  geom_vline(xintercept = 0, linetype=2) +
  xlab(pc_xlab) + 
  ylab(pc_ylab) +
  theme(legend.position = "none")+
  coord_fixed(ratio=pc2/pc1) # Scale plot by variance explained

print(gg.reps)

```

## Merge replicates

```{r replicates}
#ps <- readRDS("output/rds/ps_idtaxaExact.rds")

#Handle replicataes
rm_samples <- c("Undetermined")
ps1 <- subset_samples(ps, sample_names(ps) !=rm_samples) # Drop Undetermined reads
ps1 <- prune_samples(sample_sums(ps1)>=20, ps1) # Drop empty samples

#Plot read differences between replicates
gg.reps <- plot_bar(ps1, x="Sample", y="Abundance", fill="Genus") +
  facet_grid(~sample_id, drop=TRUE, scales="free_x") +
  geom_hline(yintercept=1000)

#Calculate for each primer
#ps1 <- subset_samples(ps1,target_subfragment=="fwhF2-fwhR2n")
#ps1 <- subset_samples(ps1,target_subfragment=="fwhF2-HexCOIR4")

#Calculate kulczynski distance - A presence absense measure of detection 

  kdi <- phyloseq::distance(ps1, method="kulczynski")
  
  kdimap <- as.data.frame(as.matrix(kdi)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)
  
  #Make heatmap plot
  gg.kdimap <- ggplot(data = kdimap, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("kulczynski distance")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank(),
          legend.position = "none")

# Merge replicates
  ps.merged <- ps1 %>%
    merge_samples(group = "ExtractID")

## keeping only those ASVs that occur in 2 replicates
## Create a matrix of 0s and 1s indicating whether the taxon count should be
## allowed, or should be set to 0.
#ps.merged.ok <- ps1 %>%
#    transform_sample_counts(function (x) (x > 0) * 1) %>%  #Summarise presence/absense across reps
#    merge_samples(group = "ExtractID") %>% #Merge reps
#    transform_sample_counts(function (x) (x > 1) * 1) #Only keep those occuring in 2 or more replicates
#
###Test export
###    write.csv(psmelt(ps.merged.ok),"test.csv")
##    
### Multiply the counts by the 0-1 matrix
#newotu <- otu_table(ps.merged) * otu_table(ps.merged.ok)
#
#otu_table(ps.merged) <- otu_table(newotu, taxa_are_rows = FALSE)
#

#This loses the sample metadata - Need to add it agian
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE)  %>%
  #filter(FCID== "CK3HD2") %>% # change for other runs
  filter(!duplicated(ExtractID))  %>%
  magrittr::set_rownames(.$ExtractID) %>%
  dplyr::select(c("sample_id", "ExtractID",
                  "geo_loc_name", "material", 
                  "target_subfragment", "F_primer", "R_primer",
                  "FCID", "seq_platform_ID"))

sample_data(ps.merged) <- samdf
ps.merged <- filter_taxa(ps.merged, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table


# After merging kulczynski distance 

  kdi2 <- phyloseq::distance(ps.merged, method="kulczynski")
  
  kdimap2 <- as.data.frame(as.matrix(kdi2)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)
  
  #Make heatmap plot
  gg.kdimap2 <- ggplot(data = kdimap2, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("kulczynski distance post replicate merge")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1)) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank(),
          legend.position = "none")
  

```

## Check for concordance between mock and real


```{r PCA}
# Read in expected table
exp <- read_csv("sample_data/expected_quant.csv") %>%
  gather(Species, Abundance, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  filter(str_detect(X1,"D100M|D250M|D500M|D1000M|DLarv")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Species","Actual"))

# Prepare observed table
sam <- ps.merged %>%
  speedyseq::tax_glom("Species") %>%
  transform_sample_counts(function (x) x/sum(x)) %>%
  speedyseq::psmelt() %>%
  mutate(Species = str_replace(Species, pattern="Drosophila_mauritiana/simulans", replacement= "Drosophila_simulans")) %>%
  mutate(Species = str_replace(Species, pattern="Drosophila_albomicans", replacement= "Drosophila_immigrans")) %>%
  filter(Sample %in% exp$Sample) %>%
  arrange(Abundance) %>%
  distinct() %>%
  select(Sample, Species, Abundance)

# Join tables
joint <- sam %>%
  filter(Species %in% exp$Species) %>%
  group_by(Species, Sample) %>% 
  summarise(Abundance = sum(Abundance)) %>%
  ungroup() %>%
  #mutate(Type = ifelse(Sample %in% controls, "Est", "Eval")) %>%
  bind_rows(exp %>%
              mutate(Actual = replace_na(Actual, 0)) %>%
              rename(Abundance = Actual) %>%
              filter(str_detect(Sample, pattern="-ex1"))  %>%
              mutate(Sample = str_replace_all(Sample, pattern="-ex1", replacement="-A")
            
            )) %>%
  group_by(Sample) %>%  
  mutate_at(vars(Abundance), ~ . / sum(.)) %>% # Convert to proportions
  #mutate(Abundance = Abundance + 0.0001) %>% # Add pseudocount
  #mutate(Abundance = clr(Abundance)) %>%
  ungroup()

# PCA
joint_pca <- joint %>%
  pivot_wider(
    names_from = Species, #  Switch this to transpose
    values_from = Abundance,
    values_fill = list(Abundance=0),
  ) %>%
  mutate(Extract = Sample %>% str_replace_all(pattern="(-)(.*?)(?=$)", replacement="")) %>%
  mutate(Mock = Sample %>% 
           str_replace_all(pattern="(-)(.*?)(?=$)", replacement="") %>%
           str_replace_all(pattern="(^)(.*?)(?<=M)", replacement="")) %>%
  nest(data = everything()) %>%
  mutate(pca = map(data, ~ prcomp(.x %>% select(-Sample, -Extract, -Mock), 
                                  center = TRUE, scale = TRUE)),
         pca_aug = map2(pca, data, ~augment(.x, data = .y))) 

joint_pca$pca %>%
  map(~tidy(.x, data = .y, "pcs")) %>%
  as.data.frame() %>%
  ggplot(aes(x = PC, y=percent)) +
  geom_bar(stat="identity") + labs(x="PC",y="Variance Explained")


library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(25)

joint_pca %>%
  mutate(
    pca_graph = map2(
      .x = pca,
      .y = data,
      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,
                 loadings.label.repel = FALSE,
                 data = .y, label = TRUE,
                 label.label = "Sample",
                 label.repel = TRUE, 
                 colour='Extract') +
       #theme_bw() +
        labs(x = "Principal Component 1",
             y = "Principal Component 2",
             title = "First two principal components of PCA for expected and observed") +
        coord_fixed()#+
       # scale_colour_manual(values=col)
    )
  ) %>%
  pull(pca_graph)


## Heirarchial clustering


# Output drosophila summary
ps.merged %>% 
  subset_taxa(Family=="Drosophilidae") %>%
  microbiome::transform("compositional") %>%
  summarise_taxa("Species", "sample_id") %>%
  filter(!str_detect(sample_id, "NTC")) %>%
  #filter(str_detect(Species, "Drosophila|Scaptodrosophila")) %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/drosophila_spp_sum.csv")

```

# Detection of targets


# Other taxa

## Alpha diversity

Phylogenetic diversity (Faiths PD)
Species diversity
Shannon diversity

#

```{r prevalence-assessment}
# Calculate taxon prevalence across the data set
prevdf <- apply(X = otu_table(ps.merged), MARGIN = ifelse(taxa_are_rows(ps.merged), yes = 1, no = 2),FUN = function(x){sum(x > 0)})

# Add taxonomy and total read counts to prevdf - change this to tidyverse code
prevdf <- data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps.merged), tax_table(ps.merged))

#Prevalence plot
gg.prev <- subset(prevdf, Order %in% get_taxa_unique(ps.merged, "Order")) %>%
  ggplot(aes(TotalAbundance, Prevalence / nsamples(ps.merged),color=Genus)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_x_log10() +
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Order) +
  theme(legend.position="none") +
  ggtitle("Phylum Prevalence in All Samples\nColored by Family")

gg.prev
```

## ASV Filtering

* remove samples with low reads
* Remove all OTUS with an abundance less than 0.01 (or index switch estimate)
  -This could be better estimated with an ROC curve of false positives and false negatives in mock communities?
* Remove OTUs that are found in less than 20% of samples- For Quantitative analysis not for detection



# Part 1: Primer comparison

## Comparison between primers for detection

Doesnt seem to be any scaptodrosophila? need to do exact matching

```{r}
#For run 1
#rm_samples <- c("Undetermined")
#ps1 <- subset_samples(ps, sample_names(ps) !=rm_samples) # Drop Undetermined reads
#ps1 <- prune_samples(sample_sums(ps1)>=20, ps1) # Drop empty samples
#ps2 <- tax_glom(ps1,taxrank="Species") # Change to ps.merged for later runs

#Agglomerate to species
ps2 <- speedyseq::tax_glom(ps.merged,taxrank="Species") # Change to ps.merged for later runs

#For run2 
rm_samples <- c("fwhF2-fwhR2n-CM4","fwhF2-HexCOIR4-CM4")
ps2 <- subset_samples(ps2, !sample_names(ps2) %in% rm_samples) # Drop Undetermined reads

ps2 <- filter_taxa(ps2, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps2 <- prune_samples(sample_sums(ps2)>=20, ps2) # Drop empty samples


#Rough barplot
col2 <- colorRampPalette(brewer.pal(11, "Spectral"))(71)

rm_samples <- sample_names(ps2)[which(str_detect(sample_names(ps2),"blank"))]
psbar <- subset_samples(ps2, !sample_names(ps2) %in% rm_samples) # Drop Undetermined reads

Fig1 <- plot_bar(psbar,fill="Species") +
  theme_bw() +
    #scale_fill_manual(values=col2) + 
  theme(legend.position="none",
        axis.text.x = element_text(angle=90))
#Change sample names for plotting
#
#sample_names(ps2) <- sample_names(ps2) %>%
##  str_split_fixed("-",n=3) %>%
#  as_tibble() %>%
#  pull(V3)

gg.hmap <- plot_heatmap(ps2, "jaccard", "jsd", taxa.label="Species", na.value=NA, taxa.order="Family")   +
  theme_bw() +
  theme(axis.text.x = element_text(angle=60, hjust=1),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        panel.spacing =unit(0.2, "lines"),
        legend.position = "bottom",
        legend.direction = "horizontal") +
  facet_grid(~target_subfragment,scales="free",space="free", drop=TRUE) +
    scale_fill_distiller(palette="Reds", direction= 1, trans = 'log10',  na.value = NA)


#Summarise this also with a jaccard distance comparison!
library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(20)

ps.ord <- subset_taxa(ps2, !Species %in% Syn_taxa)
ps.ord <- filter_taxa(ps.ord, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps.ord <- prune_samples(sample_sums(ps.ord)>=20, ps2) # Drop empty samples
GP.ord <- ordinate(ps.ord, "NMDS", "jaccard")
p1 = plot_ordination(ps.ord, GP.ord, type="samples", color="ExtractID", title="Comparison of 2 primer sets") + 
  theme_bw() +  
  theme(legend.position = "none") +
  #scale_color_manual(values=col)+ 
  geom_point(size=3) 

#,guide=FALSE
```

## Comparison between primers for bias

As part of the mock community analysis, we wish to determine taxonomic bias by looking at observed vs expected reads. To do this, we load dummy sequence, taxonomy, and sample data tables and create a seperate phyloseq object, which will later be merged

This loads a dummy sequence table, taxonomy table, and sample data table and merges it into the existing phyloseq object

Conducted as per: https://mikemc.github.io/metacal/articles/tutorial.html


The clr-transformed values are scale-invariant; that is the same ratio is expected to be obtained in a sample with few read counts or an identical sample with many read counts, only the precision of the clr estimate is affected. 

The G(x) cannot be determined for sparse data without deleting, replacing or estimating the 0 count values. Fortunately, there are acceptable methods of dealing with 0 count values as both point estimates using zCompositionsR package

```{r Drosophila bias}
#devtools::install_github("mikemc/metacal")
library(metacal)
library(tidyverse)

ps_bias <- subset_samples(ps2, geo_loc_name %in% c("Colony","Red Hill"))
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <- "Carpophilus_nr.dimidiatus"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"
ps_bias <- tax_glom(ps_bias, taxrank="Species")

primers <- as.character(unique(sample_data(ps2)$target_subfragment ))
plist <- vector("list", length(primers))
names(plist) = primers

i=1
for (i in 1:length(primers)){
  ps_primer <- subset_samples(ps_bias,sample_data(ps_bias)$target_subfragment  == primers[i])
  ps_primer <- filter_taxa(ps_primer, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
  
sam <- psmelt(ps_primer) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species)

exp <- read_csv("sample_data/Test_expected_quant.csv") %>%
  gather(Species,Abundance,-X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  mutate(X1 = str_split_fixed(X1,"-rep",n=2) %>%
           as_tibble()%>% 
           pull(V1)) %>%
  distinct %>%
  filter(str_detect(X1,"D100M")) %>% ###CHANGE TO CM for carpophilus D100M for DROS
  drop_na()
colnames(exp) <- c("Sample","Taxon","Actual")

#Join tables 
joint <- sam %>%
  filter(Taxon %in% exp$Taxon) %>%
  #filter(Abundance > 0) %>%
  left_join(exp, by = c("Sample","Taxon"))%>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual)

#Build error matrix
error_mat <- build_matrix(joint, Sample, Taxon, Error)

#Estimate bias
bias <- center(error_mat, enframe = TRUE) %>%
    dplyr::rename(Bhat = Center)

#Estimate uncertainty in bias estimate
bootreps <- bootrep_center(error_mat) %>%
    dplyr::rename(Bhat = Center)
bootreps.summary <- bootreps %>%
    group_by(Taxon) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))
bias0 <- left_join(bias, bootreps.summary, by = "Taxon")

#Test plot bias

p <- ggplot(bias0, aes(Taxon, y=Gm_mean-1,fill=Taxon)) +
    geom_bar(stat="identity")+
    geom_errorbar(aes(ymin = (Gm_mean-1) - (Gm_se-1), ymax = (Gm_mean-1) + (Gm_se-1), width=0.2)) +
    geom_point(aes(y=Gm_mean-1)) +
    scale_fill_brewer(palette="Spectral")+
    scale_colour_brewer(palette="Spectral") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0)) + 
  ylab("Bias") +
  ggtitle(primers[i]) +
  expand_limits(y = c(-2, 4)) +
  theme(legend.position = "none")
 
plist[[i]] = p
 
#Get pairwise bias
bias.pw <- bias %>%
    compute_ratios(group_vars = c()) %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":"))

#Get pairwise boostrap estimates
bootreps.pw <- bootreps %>%
    compute_ratios(group_vars = ".id")
summary.pw <- bootreps.pw %>%
    group_by(Taxon.x, Taxon.y) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))
bias.pw0 <- left_join(bias.pw, summary.pw, by = c("Taxon.x", "Taxon.y"))

#Plot bias estimates
ratios <- joint %>%
    compute_ratios %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":")) %>%
    filter(!is.nan(Error), Taxon.x < Taxon.y)
ratios.pred <- bias.pw0 %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":")) %>%
    filter(Taxon.x < Taxon.y)

gg.bias <- ggplot(ratios, aes(Pair, Error, color = Sample)) +
    geom_hline(yintercept = 1, color = "grey") +
    geom_pointrange(data = ratios.pred, aes(y = Bhat, 
            ymin = Bhat / Gm_se^2, ymax = Bhat * Gm_se^2), 
        color = "black") +
    geom_jitter(width = 0.2) +
    scale_y_log10() +
    coord_flip()

}
#Need to set a 

library(patchwork)
plist[[1]] + plist[[2]] 

plist[[1]] + plist[[2]] + plist[[3]] + plist[[4]] 

```


# Dros bias seperate primers
```{r Drosophila bias}
#devtools::install_github("mikemc/metacal")
library(metacal)
library(tidyverse)

##Test for plot of 3 primers - to change back remvoe below and change pcr_primers back to target_subregion below
ps2 <- tax_glom(ps,taxrank="Species")

ps_bias <- subset_samples(ps2, feature %in% c("Drosophilidae","Cherry"))
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <- "Carpophilus_nr.dimidiatus"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"
ps_bias <- tax_glom(ps_bias, taxrank="Species")

primers <- as.character(unique(sample_data(ps2)$pcr_primers ))
plist <- vector("list", length(primers))
names(plist) = primers

i=1
for (i in 1:length(primers)){
  ps_primer <- subset_samples(ps_bias,sample_data(ps_bias)$pcr_primers  == primers[i])
  ps_primer <- filter_taxa(ps_primer, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
  
sam <- psmelt(ps_primer) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species)

exp <- read_csv("sample_data/Test_expected_quant.csv") %>%
  gather(Species,Abundance,-X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  mutate(X1 = str_replace(X1, pattern="-rep",replacement="_Rep")) %>% # Remove for merged
 # mutate(X1 = str_split_fixed(X1,"-rep",n=2) %>%
 #          as_tibble()%>%    #Put back in for merged
 #          pull(V1)) %>%
#  distinct %>%
  filter(str_detect(X1,"D100M")) %>% ###CHANGE TO CM for carpophilus D100M for DROS
  drop_na()
colnames(exp) <- c("Sample","Taxon","Actual")

#Join tables 
joint <- sam %>%
  filter(Taxon %in% exp$Taxon) %>%
  #filter(Abundance > 0) %>%
  left_join(exp, by = c("Sample","Taxon"))%>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual)

#Build error matrix
error_mat <- build_matrix(joint, Sample, Taxon, Error)

#Estimate bias
bias <- center(error_mat, enframe = TRUE) %>%
    dplyr::rename(Bhat = Center)

#Estimate uncertainty in bias estimate
bootreps <- bootrep_center(error_mat) %>%
    dplyr::rename(Bhat = Center)
bootreps.summary <- bootreps %>%
    group_by(Taxon) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))
bias0 <- left_join(bias, bootreps.summary, by = "Taxon")

#Test plot bias

p <- ggplot(bias0, aes(Taxon, y=Gm_mean-1,fill=Taxon)) +
    geom_bar(stat="identity")+
    geom_errorbar(aes(ymin = (Gm_mean-1) - (Gm_se-1), ymax = (Gm_mean-1) + (Gm_se-1), width=0.2)) +
    geom_point(aes(y=Gm_mean-1)) +
    scale_fill_brewer(palette="Spectral")+
    scale_colour_brewer(palette="Spectral") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0)) + 
  ylab("Bias") +
  ggtitle(primers[i]) +
  expand_limits(y = c(-2, 4)) +
  theme(legend.position = "none")
 
plist[[i]] = p

}
#Need to set a 

library(patchwork)
(plist[[1]] + plist[[2]] + plist[[3]]) / (plist[[4]] + plist[[5]] + plist[[6]])
```


```{r calibrate bias}
sam <- psmelt(ps_primer) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species) #%>%
 #filter(str_detect(ExtractID,"D100M|Chery")) ###CHANGE TO CM for carpophilus D100M for DROS

exp <- read_csv("sample_data/Test_expected_quant.csv") %>%
  gather(Species,Abundance,-X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  mutate(X1 = str_split_fixed(X1,"-rep",n=2) %>%
           as_tibble()%>%
           pull(V1)) %>%
  distinct %>%
  filter(str_detect(X1,"D100M")) %>% ###CHANGE TO CM for carpophilus D100M for DROS
  drop_na()
colnames(exp) <- c("Sample","Taxon","Actual")

#Subset to only those taxa desired for estimation
#controls <- c("fwhF2-fwhR2n-CM1","fwhF2-fwhR2n-CM2","fwhF2-fwhR2n-CM3","fwhF2-fwhR2n-CM5")#
#exp <- filter(exp, Sample %in% controls)
#sam <- sam %>%
#    mutate(Type = ifelse(Sample %in% controls, "Control", ""))

#Join tables 
joint <- sam %>%
  filter(Taxon %in% exp$Taxon) %>%
#  mutate(inmock = ifelse(Sample %in% controls, "Control", "")) %>%
  left_join(exp, by = c("Sample","Taxon"))%>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual)

#Build error matrix from just control samples
error_mat <- build_matrix(joint, Sample, Taxon, Error) #%>% filter(Type=="Control")

#Estimate bias
bias <- center(error_mat, enframe = TRUE) %>%
    dplyr::rename(Bhat = Center)

#Subset to only taxa in controls
control_taxa <- unique(exp$Taxon) 
#Remove d immigrans
control_taxa <- control_taxa[!str_detect(control_taxa,pattern="Drosophila_immigrans")]

#Calibration
cal <- joint %>%
    left_join(bias, by = "Taxon") %>%
    mutate(Calibrated = Abundance / Bhat)


cal.prop <- cal %>%
    filter(Taxon %in% control_taxa) %>%
    group_by(Sample) %>%
    mutate_at(vars(Abundance, Calibrated,Actual), ~ . / sum(.)) 
#Visualise proportions before and after

plot_df <- cal.prop %>%
    #filter(Sample %in% samples) %>%
    gather("Type", "Proportion", "Abundance", "Calibrated", "Actual") %>%
    mutate(Type = factor(Type, c("Actual","Abundance", "Calibrated")))

gg.cal <- ggplot(plot_df, aes(Type, Proportion, fill = Taxon)) +
    geom_col() +
    facet_grid(~Sample) +
    theme_bw() +
    scale_fill_brewer(palette = "Spectral")


#also make another plot showing the bias in the estimates from the amount of controls used - see https://mikemc.github.io/mgs-bias-manuscript/analysis/costea2017-analysis.html


#Plot expected vs observed

correction <- cal.prop %>% select(Sample,Taxon,Actual,Abundance,Calibrated)

correction <- correction %>%
  gather(Type,Abundance,-Sample,-Taxon,-Actual)

g.cor <-ggplot(correction, aes(x=Actual,y=Abundance)) +
  geom_point(aes(fill=Type),size=3,alpha=0.8,shape=21,stroke=1,color="black") + 
  geom_abline(slope=1, intercept = 0) +
  stat_cor(aes(color=Type), label.x = 0.1)  + 
  xlim(0,1) + 
  ylim(0,1) + 
  scale_fill_manual(values=c("#A9A9A9","#ae0707")) + 
  theme_pubr() + 
  ylab("Observed")


#Might be worth trying this again with all genes in together?

#p1 <- g.cor
#p2 <- g.cor
#p3 <- g.cor
#p4 <- g.cor

pnocal  <- ggplot((correction %>% filter(Type=="Abundance")), aes(x=Actual,y=Abundance)) +
  geom_point(aes(fill=Type),size=3,alpha=0.8,shape=21,stroke=1,color="black") + 
  geom_abline(slope=1, intercept = 0) +
  stat_cor(aes(color=Type), label.x = 0.1)  + 
  xlim(0,1) + 
  ylim(0,1) + 
  scale_fill_manual(values=c("#A9A9A9","#ae0707")) + 
  theme_pubr() + 
  ylab("Observed")

#Output Pre- subset barplot

ps_presub <-  subset_samples(ps_primer, sample_names(ps_primer) =="fwhF2-fwhR2n-DM6SPD") # Drop Undetermined reads
ps_presub <- prune_samples(sample_sums(ps_presub)>=20, ps_presub) # Drop empty samples
ps_presub <- filter_taxa(ps_presub, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table

col2 <- colorRampPalette(brewer.pal(11, "Spectral"))(26)

Fig1 <- plot_bar(ps_presub,fill="Species") +
  theme_bw() +
    scale_fill_manual(values=col2)


```

```{r}

# Part 2: Compositional data analysis
library(CoDaSeq)

#fwhf2-fwhr2n 
ps_primer <- subset_samples(ps.merged,sample_data(ps1)$target_subfragment =="fwhF2-fwhR2n")

# filter the dataset
f <- codaSeq.filter(as.data.frame(otu_table(ps_primer)), min.reads=1000, min.prop=0.005, min.occurrence=0, samples.by.row=FALSE)


# replace 0 values with an estimate
f.n0 <- cmultRepl(f, method="CZM", label=0)

# generate the CLR values
f.clr <- codaSeq.clr(f.n0)

#aitch <- acomp(as.data.frame(otu_table(ps_primer)) +1) # Add pseudocount
# proportions closed between 0 and 1 
#bal = clr(aitch) # isometric log-ratios 

adi = vegdist(f.clr, method="euclidean") # Aitchison dissimilarity matrix 

adimap <- as.data.frame(as.matrix(adi)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)

#Set exact matches to 1
#adimap$Distance[adimap$rowname==adimap$colname] <- 1

#gg.adimap <- NULL
gg.adimap <- ggplot(data = adimap, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("Aitchinson distance")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank())


#Get group consistency

groups <- str_split_fixed(rownames(f.clr),"-",n=4) %>%
  as_tibble %>%
  unite(col="sample", c("V1","V2","V3"), sep="-") %>%
  pull(sample) %>%
  unique()



outlier <- codaSeq.outlier(f.clr)
  
outlier.list
for (i in groups){
  samples <- f.clr[which(str_detect(rownames(f.clr),pattern=groups[1])),]
  s.var <- codaSeq.outlier(samples,plot.me=FALSE)
  
  plot(density(s.var),main=groups[1],xlab="Var Fraction", ylab="Density")
  
}

#principle component

# perform a singular value decomposition
pcx <- prcomp(f.clr)
# plot a PCA biplot
# calculate percent variance explained for the axis labels
pc1 <- round(pcx$sdev[1]^2/sum(pcx$sdev^2),2)
pc2 <- round(pcx$sdev[2]^2/sum(pcx$sdev^2),2)
xlab <- paste("PC1: ", pc1, sep="")
ylab <- paste("PC2: ", pc2, sep="")
biplot(pcx, cex=c(0.6,0.4), var.axes=F,
    scale=1, xlab=xlab, ylab=ylab)


#Plot of the relationship between the OTUs from the SVD. Here each OTU is colored red if it was identified as having an effect size greater than 1

# calculation of SVD in R_Block_2
plot(pcx$rotation[,1], pcx$rotation[,2],
    main="loadings", xlab=xlab, cex=0.5,
    ylab=ylab, col=rgb(0,0,0,0.3), pch=19)
points(pcx$rotation[,1][high.e],
    pcx$rotation[,2][high.e], cex=0.8,
    col=rgb(1,0,0,0.5))
points(pcx$rotation[c("39306","39235"),1],
    pcx$rotation[c("39306","39235"),2],
    cex=0.5, col=rgb(0,0,1,1), pch=19)


#Cluster analysis

# anosim between groups using Aitchison distance
dist.clr <- dist(E.clr)
ano <- anosim(dist.clr, conds, permutations=999)
# coloring from https://rpubs.com/gaston/dendrograms
# make the dendrogram
hc <- as.dendrogram(hclust(dist.clr, method="ward.D2"))
hcd <- hclust(dist.clr, method="ward.D2")
# function to get color labels
colLab <- function(n) {
    if (is.leaf(n)) {
        a <- attributes(n)
        #labCol <- labelColors[clusMember[which(names(clusMember) == a$label)]]
        labCol <- if (grepl("ak", a$label) == TRUE ) "red" else "blue"
        attr(n, "nodePar") <- c(a$nodePar, lab.col = labCol)
    }
    n
}
# using dendrapply
clusDendro = dendrapply(hc, colLab)
plot(clusDendro, main="Aitchison distance, Ward.D2 Cluster")


#If we have found differences - Univariate tests using AlDex

par(mfrow=c(1,2))
plot(f.e$diff.win, f.e$diff.btw,
    pch=19, col=rgb(0,0,0,0.3),
    cex=0.5, main="effect plot",
    xlab="Dispersion", ylab="Difference")
points(f.e$diff.win[low.p],
    f.e$diff.btw[low.p],
    pch=19, col=rgb(0,0,1,0.5),
    cex=0.5)
points(f.e$diff.win[high.e],
    f.e$diff.btw[high.e],
    col=rgb(1,0,0,0.5), cex=0.8)
abline(0,1, lty=2, lwd=2,col="grey")
abline(0,-1, lty=2, lwd=2,col="grey")
plot(f.e$effect, f.t$we.eBH, log="y",
    pch=19, col=rgb(0,0,0,0.3),
    main="E vs p", xlab="effect size",
    ylab="E(p.adjust)", cex=0.5)
points(f.e$effect[low.p], f.t$we.eBH[low.p],
    pch=19, col=rgb(0,0,1,0.5),cex=0.5)
points(f.e$effect[high.e], f.t$we.eBH[high.e],
    col=rgb(1,0,0,0.5), cex=0.8)

```
