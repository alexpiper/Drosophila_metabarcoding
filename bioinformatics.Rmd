---
title: "Drosophila Metabarcoding"
author: "A.M. Piper"
date: "2019/04/05"
output:
  
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE,fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/workprojects/PHD/Metabarcoding/Dros_metabarcoding')
setwd('C:/Users/ap0y/Dropbox/workprojects/PHD/Metabarcoding/Dros_metabarcoding')
opts_chunk$set(dev = 'png')
```

# Install and setup directories 

This pipeline requires various R packages to be installed prior to running. These are obtained from CRAN, Bioconductor and Github. The taxreturn and seqateurs packageR package also provide wrappers around other software packages for QC. For convenience we will download and install these software in a new folder called “bin”

```{r install & Load packages} 
#Set required packages
.cran_packages <- c("ggplot2",
                    "gridExtra",
                    "tidyverse", 
                    "scales",
                    "stringdist",
                    "patchwork",
                    "vegan",
                    "ggpubr",
                    "seqinr",
                    "patchwork",
                    "stringi",
                    "phangorn",
                    "filesstrings")
.bioc_packages <- c("dada2",
                    "phyloseq",
                    "DECIPHER",
                    "savR",
                    "Biostrings",
                    "ShortRead",
                    "ngsReports")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

devtools::install_github("alexpiper/taxreturn")
devtools::install_github("alexpiper/seqateurs")

library(taxreturn)
library(seqateurs)

#Install blast
blast_install(dest.dir = "bin")

#Install bbmap
bbmap_install(dest.dir = "bin")

#Install fastqc
fastqc_install(dest.dir = "bin")

# Create directories
if(!dir.exists("data")){dir.create("data", recursive = TRUE)}
if(!dir.exists("reference")){dir.create("reference", recursive = TRUE)}
if(!dir.exists("output/logs")){dir.create("output/logs", recursive = TRUE)}
if(!dir.exists("output/fasta")){dir.create("output/fasta", recursive = TRUE)}
if(!dir.exists("output/csv")){dir.create("output/csv", recursive = TRUE)}
if(!dir.exists("output/rds")){dir.create("output/rds", recursive = TRUE)}
```

# Calculate index switch rate

BASH:
```{bash demultiplex 1 mismatch}
###BASH###
#load module
module load bcl2fastq2/2.20.0-foss-2018b

#raise amount of available file handles
ulimit -n 4000

#Miseq Run 1 - Testing Primers
bcl2fastq -p 12 --runfolder-dir /group/sequencing/190331_M03633_0310_000000000-CB3DR  \
--output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run1_CB3DR \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run1_CB3DR/SampleSheet.csv \
--no-lane-splitting --barcode-mismatches 0

#Miseq Run 2 - Testing replicate tagged primers & SynMock
bcl2fastq -p 12 --runfolder-dir /group/sequencing/190628_M03633_0331_000000000-CK3HD  \
--output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run2_CK3HD \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run2_CK3HD/SampleSheet.csv \
--no-lane-splitting --barcode-mismatches 0

#Miseq Run 3 - Ladder spike ins
bcl2fastq -p 12 --runfolder-dir /group/sequencing/190722_M03633_0336_000000000-CJKFJ/ \
--output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run3_CJKFJ \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run3_CJKFJ/SampleSheet_CJKFJ.csv \
--no-lane-splitting --barcode-mismatches 0
```

## Set analysis parameters and create sample sheet
In order to track samples and relevant QC statistics throughout the metabarcoding pipeline, we will first create a new samplesheet from our input samplesheets. This function requires both the SampleSheet.csv used for the sequencing run, and the runParameters.xml, both of which can be obtained from the demultiplexed sequencing run folder/

We will also add the relevant primers at this stage to this sheet:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2_P5    ACACTCTTTCCCTACACGACGCTCTTCCGATCT     GGDACWGGWTGAACWGTWTAYCCHCC

REVERSE PRIMER:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2n_P7   GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT  GTRATWGCHCCDGCTARWACWGG
    
```{r create samplesheet}
runs <- dir("data/") #Find all directories within data
SampleSheets <- paste0("data/", runs, "/SampleSheet.csv")
runParameters <- paste0("data/", runs, "/runParameters.xml")

## Create new samplesheet with all run information
samdf <- seqateurs::create_samplesheet(SampleSheet = SampleSheets, runParameters = runParameters)

# Add primers & twin tag sequences to samdf 
samdf <- samdf %>%
  #filter(FCID %in% c("CK3HD", "CJKFJ", "HLVKYDMXX")) %>%
  mutate(count =3 ) %>%
  uncount(count) %>%
  rownames_to_column("replicate") %>%
  mutate(replicate = case_when(
    str_detect(replicate, "\\.1") ~ 2,
    str_detect(replicate, "\\.2") ~ 3,
    TRUE ~ 1
  )) %>%
  filter(!(FCID == "CB3DR" & replicate > 1)) %>% #First flowcell didnt use replication
  mutate(
    Fprimer = case_when(
    str_detect(Sample_ID, "fwhF2") & replicate == 1 & FCID == "CB3DR"  ~  "GGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(Sample_ID, "BF1") & replicate == 1 & FCID == "CB3DR"  ~  "ACWGGWTGRACWGTNTAYCC",
    str_detect(Sample_ID, "SauronS878") & replicate == 1 & FCID == "CB3DR"  ~  "GGDRCWGGWTGAACWGTWTAYCCNCC",
    #Replicated samples
    str_detect(Sample_ID, "fwhF2") & replicate == 1 & !FCID == "CB3DR"  ~ "GAGGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(Sample_ID, "fwhF2") & replicate == 2 & !FCID == "CB3DR" ~  "TGTGGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(Sample_ID, "fwhF2") & replicate == 3 & !FCID == "CB3DR" ~  "AGAAGGDACWGGWTGAACWGTWTAYCCHCC",
    # final runs - samples werent named with primers
    replicate == 1 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "GAGGDACWGGWTGAACWGTWTAYCCHCC",
    replicate == 2 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "TGTGGDACWGGWTGAACWGTWTAYCCHCC",
    replicate == 3 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "AGAAGGDACWGGWTGAACWGTWTAYCCHCC",
    ),
    Rprimer = case_when(
    str_detect(Sample_ID, "fwhR2n") & replicate == 1 & FCID == "CB3DR"  ~  "GTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "BR1") & replicate == 1 & FCID == "CB3DR"  ~  "ARYATDGTRATDGCHCCDGC",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 1 & FCID == "CB3DR"  ~  "TATDGTRATDGCHCCNGC",
    #Replicated samples
    str_detect(Sample_ID, "fwhR2n") & replicate == 1 & !FCID == "CB3DR"  ~ "ACGTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "fwhR2n") & replicate == 2 & !FCID == "CB3DR" ~  "TCCGTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "fwhR2n") & replicate == 3 & !FCID == "CB3DR" ~  "CTGCGTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 1 & !FCID == "CB3DR"  ~ "GCTATDGTRATDGCHCCNGC",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 2 & !FCID == "CB3DR" ~  "AGGTATDGTRATDGCHCCNGC",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 3 & !FCID == "CB3DR" ~  "CACGTATDGTRATDGCHCCNGC",
    # final runs - samples werent named with primers
    replicate == 1 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "ACGTRATWGCHCCDGCTARWACWGG",
    replicate == 2 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "TCCGTRATWGCHCCDGCTARWACWGG",
    replicate == 3 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "CTGCGTRATWGCHCCDGCTARWACWGG",
    ),
    twintagF = case_when(
      !FCID == "CB3DR"~ substr(Fprimer, 1, 8), # Use first 8 characters of F primer as F tag
      FCID == "CB3DR" ~ as.character(NA)
    ),
    twintagR = case_when(
      !FCID == "CB3DR"~ substr(Rprimer, 1, 8), # Use first 8 characters of F primer as F tag
      FCID == "CB3DR" ~ as.character(NA)
    ),
    Investigator_Name = "Alexander Piper",
    Assay = "Metabarcoding"
)

write_csv(samdf, "sample_data/Sample_info.csv")
```


# Quality checks:

We will conduct 3 quality checks. Firstly a check of the entire sequence run, followed by a sample level quality check to identify potential issues with specific samples. And then a calculation of the index switching rate by summarising correctly assigned vs missasigned indices.

```{r QC}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Set parameters
barcodemismatch <- 0 # set the barcode mismatch used during demultiplexing (illumina default is 1)

i=1
for (i in 1:length(runs)){
  ## Run level quality check using savR - Not compatible with novaseq
  path <- paste0("data/", runs[i], "/")
  fc <- savR(path)
  qc.dir <- paste0("output/logs/", runs[i],"/" )
  dir.create(qc.dir, re)
  write_csv(correctedIntensities(fc), paste0(qc.dir, "correctedIntensities.csv"))
  write_csv(errorMetrics(fc), paste0(qc.dir, "errorMetrics.csv"))
  write_csv(extractionMetrics(fc), paste0(qc.dir, "extractionMetrics.csv"))
  write_csv(qualityMetrics(fc), paste0(qc.dir, "qualityMetrics.csv"))
  write_csv(tileMetrics(fc), paste0(qc.dir, "tileMetrics.csv"))

  avg_intensity <- fc@parsedData[["savCorrectedIntensityFormat"]]@data %>%
    group_by(tile, lane) %>%
    summarise(Average_intensity = mean(avg_intensity)) %>% 
    ungroup() %>%
    mutate(side = case_when(
      str_detect(tile, "^11") ~ "Top",
      str_detect(tile, "^21") ~ "Bottom"
        ))%>%
    ggplot(aes(x=lane, y=as.factor(tile), fill=Average_intensity)) +
    geom_tile() +
    facet_wrap(~side, scales="free") +
    scale_fill_viridis_c()
  
  pdf(file=paste(qc.dir, "/avgintensity.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  plot(avg_intensity)
  try(dev.off(), silent=TRUE)
  
  pdf(file=paste(qc.dir, "/PFclusters.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  pfBoxplot(fc)
  try(dev.off(), silent=TRUE)

  for (lane in 1:fc@layout@lanecount) {
  pdf(file=paste(qc.dir, "/QScore_L", lane, ".pdf", sep=""), width = 11, height = 8 , paper="a4r")
      qualityHeatmap(fc, lane, 1:fc@directions)
  try(dev.off(), silent=TRUE)
  } 
}

## Sample level quality check using fastqc
for (i in 1:length(runs)){
  path <- paste0("data/", runs[i], "/")
  qc.dir <- paste0("output/logs/", runs[i],"/FASTQC" )
  dir.create(qc.dir, recursive=TRUE)
  #qc_out <- seqateurs::fastqc(fq.dir = path, qc.dir	= qc.dir, fastqc.path = "bin/FastQC/fastqc", threads=2) 
  writeHtmlReport(qc.dir, overwrite = TRUE, gcType ="Genome",  quiet=FALSE) # requires PANDOC!
  #filesstrings::file.move(paste0(qc.dir, "ngsReports_Fastqc.html"), paste0("output/logs/", runs[i],"/"))
}

## Calculate index switching
for (i in 1:length(runs)){
  path <- paste0("data/", runs[i], "/")
  qc.dir <- paste0("output/logs/", runs[i] )
  run_data <- samdf %>%
    filter(FCID == runs[i])

  indices <- sort(list.files(path, pattern="_R1_", full.names = TRUE)) %>%
    purrr::set_names() %>%
    purrr::map(seqateurs::summarise_index) %>%
    bind_rows(.id="Sample_Name")%>%
    arrange(desc(Freq)) %>% 
    dplyr::mutate(Sample_Name = Sample_Name %>% 
                    str_replace(pattern = "^(.*)\\/", replacement="") %>%
                    str_replace(pattern = "(?:.(?!_R1_))+$", replacement=""))

  
  index <- indices %>% 
  	filter(!str_detect(Sample_Name, "Undetermined")) %>%
  	pull(index)
  index2 <- indices %>% 
  	filter(!str_detect(Sample_Name, "Undetermined")) %>%
  	pull(index2)

  ##Create mismatch, if demultiplexing was conducted with barcodemismatch
  if(barcodemismatch > 0){
    index <- purrr::map(index, create_mismatch, barcodemismatch) %>%
      unlist()
    index2 <- purrr::map(index2, create_mismatch, barcodemismatch) %>%
      unlist()
  } 
  
  #Create all possible switched combinations
  combos <- unique(expand.grid(index, index2)) %>%
    set_colnames(c("index", "index2"))

  #get unused combinations resulting from index switching - NEED TO ENUMERATE MISMATCH
  switched <- left_join(combos, indices, by=c("index", "index2")) %>%
    drop_na()
  
  other_reads <- anti_join(indices,combos, by=c("index", "index2")) %>%
    summarise(sum = sum(Freq)) %>%
    pull(sum)
  
  ##Summary of index switching rate
  exp_rate <- switched %>% 
    filter(!str_detect(Sample_Name, "Undetermined"))
  obs_rate <- switched %>% 
    filter(str_detect(Sample_Name,"Undetermined"))
  switch_rate <- (sum(obs_rate$Freq)/sum(exp_rate$Freq))
    message(switch_rate)

  #Plot switching
  
  gg.switch <- ggplot(data = switched, aes(x = index, y = index2), stat="identity") +
    geom_tile(aes(fill = Freq),alpha=0.8)  + 
    scale_fill_viridis_c(name="log10 Reads", begin=0.1, trans="log10")+
    theme(axis.text.x = element_text(angle=90, hjust=1), 
          plot.title=element_text(hjust = 0.5),
          plot.subtitle =element_text(hjust = 0.5)#,
          #legend.position = "none"
          ) +
    scale_x_discrete(limits=index)+
    scale_y_discrete(limits=index2)+
    labs(title= runs[i], subtitle = paste0(
      "Total Reads: ", sum(indices$Freq),
      ", Switch rate: ", sprintf("%1.4f%%", switch_rate*100),
      ", other Reads: ", other_reads)) 
  pdf(file=paste(qc.dir, "/", runs[i], "_switchrate.pdf", sep=""), width = 11, height = 8 , paper="a4r")
      plot(gg.switch)
  try(dev.off(), silent=TRUE)
  
  }
```


# Demultiplex by primer & trim

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Prior to begining this workflow, samples were demultiplexed and illumina adapters were removed by the MiSeq software, however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

In this study there were 2 amplicons of different size, and 3 different replicate primers of each. For this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files.

## fwhF2-fwhR2n amplicon:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2T1_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	  GAGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T2_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	 TGTGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T3_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	AGAAGGDACWGGWTGAACWGTWTAYCCHCC
    
REVERSE PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2nT1_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	ACGTRATWGCHCCDGCTARWACWGG
    fwhR2nT2_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	TCCGTRATWGCHCCDGCTARWACWGG
    fwhR2nT3_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	CTGCGTRATWGCHCCDGCTARWACWGG
    
```{r primer trimming , message=FALSE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

#Demultiplex samples
runs <- unique(samdf$FCID)

i=1

#Create vectors to track reads
trimmed <- vector("list", length=length(runs))
demux <- vector("list", length=length(runs))

for (i in 1:length(runs)){
  path <- paste0("data/", runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  qc.dir <- paste0("output/logs/", runs[i],"/" )

  run_data <- samdf %>%
    filter(FCID == runs[i])
  
  #Get primer sequences
  primers <- c(unique(run_data$F_seq), unique(run_data$R_seq))
  
  #Check if samples were twin tagged - these require extra round of demultiplexing
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
      demuxpath <- file.path(path, "00_demux") # Filtered forward files go into the path/filtered/ subdirectory
      dir.create(demuxpath)
      
      fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
      fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))
      
      demux[[i]] <- bbdemux(install="bin/bbmap", fwd=fastqFs, rev=fastqRs, Fbarcodes = unique(run_data$twintagF),
                    Rbarcodes = unique(run_data$twintagR), degenerate=TRUE, out.dir=demuxpath, threads=1 ,
                    mem=4,  hdist=0, overwrite=TRUE, tidylog = TRUE)
      
      demux_fastqs <- sort(list.files(paste0(demuxpath), pattern="_R1R2_", full.names = TRUE))

      trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd = demux_fastqs,
              primers = primers, checkpairs = FALSE,
              degenerate = TRUE, out.dir="01_trimmed", trim.end = "left",
              kmer=NULL, tpe=TRUE, tbo=TRUE,
              ordered = TRUE, mink = FALSE, hdist = 2,
              maxlength =(max(run_data$Fread, run_data$Rread) - sort(nchar(primers), decreasing = FALSE)[1]) +5, 
              overwrite = TRUE, quality = FALSE, quiet=FALSE)
      
      #Re-split interleaved fastq's
      trimmedpath <- file.path(demuxpath, "trimmed") # Filtered forward files go into the path/filtered/ subdirectory
      trimmed_fastqs <- sort(list.files(trimmedpath, pattern="_R1R2_", full.names = TRUE))
      bbsplit(install="bin/bbmap", files=trimmed_fastqs, overwrite=TRUE)
  
      
  } else if (twintagged == FALSE) {
    
    fastqFs <- sort(list.files(paste0(path), pattern="_R1_", full.names = TRUE))
    fastqRs <- sort(list.files(paste0(path), pattern="_R2_", full.names = TRUE))
    if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))

    trimpath <- file.path(path, "01_trimmed") # Filtered forward files go into the path/filtered/ subdirectory
      dir.create(trimpath)
      
      trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd = fastqFs, rev = fastqRs,
                primers = primers, checkpairs = FALSE,
                degenerate = TRUE, out.dir="01_trimmed", trim.end = "left",
                kmer=NULL, tpe=TRUE, tbo=TRUE,
                ordered = TRUE, mink = FALSE, hdist = 2,
                maxlength =(max(run_data$Fread, run_data$Rread) - sort(nchar(primers), decreasing = FALSE)[1]) +5,
                overwrite = TRUE, quality = FALSE, quiet=FALSE)

  }
  # QC - sequence lengths
  pre_trim <- plot_lengths(dir=path, aggregate=TRUE, sample=1e5)
  post_trim <- plot_lengths(dir=paste0(path, "/01_trimmed/"), aggregate=TRUE, sample=1e5)

  pdf(file=paste(qc.dir, "/readlengths.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  plot(pre_trim)
  plot(post_trim)
  try(dev.off(), silent=TRUE)
  
  trim_summary <- trimmed[[i]] %>% 
    mutate(perc_reads_remaining = signif(((output_reads / input_reads) * 100), 2),
           perc_bases_remaining = signif(((output_bases / input_bases) * 100), 2)
           ) %>%
    filter(!is.na(perc_reads_remaining))
    
  message(paste0(signif(mean(trim_summary$perc_reads_remaining, na.rm = TRUE), 2),
                 "% of reads and ",
                 signif(mean(trim_summary$perc_bases_remaining, na.rm = TRUE), 2),
                 "% of bases remaining for ", runs[i]," after trimming"))
  
  # Print warning for each sample
  for(w in 1:nrow(trim_summary)){
    if (trim_summary[w,]$perc_reads_remaining < 10) {message(paste0("WARNING: Less than 10% bases remaining for ",trim_summary[w,]$sample), "Check primers are correct")}
  }
}
# Track reads
read_tracker <- samdf %>% 
  select(Sample_ID, FCID) %>%
  left_join(
    trimmed %>%
    purrr::set_names(runs) %>%
    bind_rows(.id="FCID") %>%
    mutate(sample = str_replace(basename(sample), pattern="_S.*$", replacement="")) %>%
    dplyr::rename(Sample_ID = sample),
  by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```
  
  
## Plot read quality & lengths
  
  
```{r QA plot, eval = TRUE, cache= TRUE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Plotting parameters
readQC_aggregate <- TRUE
readQC_subsample <-  12
amplicon = 205 # Set to maximum size between the two primers. If working with variable barcode lengths, set to readlength

for (i in 1:length(runs)){
  run_data <- samdf %>%
    filter(FCID == runs[i])

  path <- paste0("data/", runs[i], "/01_trimmed" )
 
  ##Get trimmed files, accounting for empty files (28 indicates empty sample)
  trimmedFs <- sort(list.files(path, pattern="_R1_", full.names = TRUE))
  trimmedFs <- trimmedFs[file.size(trimmedFs) > 28]

  #Choose a random subsample for quality checks
  sampleF <- sample(trimmedFs, readQC_subsample) #NOTE - need to have option to pass
  sampleR <- sampleF %>% str_replace(pattern="_R1_", replacement = "_R2_")
  
  #Estimate an optimat trunclen
  truncLen <- estimate_trunclen(sampleF, sampleR, maxlength=amplicon)

  #Plot qualities
  gg.Fqual <- plot_quality(sampleF) +
    geom_vline(aes(xintercept=truncLen[1]), colour="blue") +
    annotate("text", x = truncLen[1]-10, y =2, label = paste0("Suggested truncLen = ", truncLen[1]), colour="blue") +
    ggtitle(paste0(runs[i], " Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25))
  gg.Fee <- plot_maxEE(sampleF) + 
    geom_vline(aes(xintercept=truncLen[1]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =-3, label = paste0("Suggested truncLen = ", truncLen[1]), colour="blue") +
    ggtitle(paste0(runs[i], " Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")
  gg.Rqual <- plot_quality(sampleR) + 
    geom_vline(aes(xintercept=truncLen[2]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =2, label = paste0("Suggested truncLen = ", truncLen[2]), colour="blue") +
    ggtitle(paste0(runs[i], " Reverse Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")
  gg.Ree <- plot_maxEE(sampleR) +
    geom_vline(aes(xintercept=truncLen[2]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =-3, label = paste0("Suggested truncLen = ", truncLen[2]), colour="blue") +
    ggtitle(paste0(runs[i], " Reverse Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")

  Qualplots <- (gg.Fqual + gg.Rqual) / (gg.Fee + gg.Ree)
  
  #output plots
  pdf(paste0("output/logs/",runs[i],"/",runs[i], "_prefilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(Qualplots)
  try(dev.off(), silent=TRUE)
}

```

This has output a prefilt_quality.pdf plot for each of the runs analysed in the logs folder. On the top is the quality score per cycle, and on the bottom is the cumulative expected errors (calculated as EE = sum(10^(-Q/10)) on a log scale. For the quality plot, the median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. For the maxEE lines, the red lines showing the expected error filter options. The blue vertical line on both plots shows the suggested truncLen option automatically determined.

Ensure that the blue suggested trunclen looks reasonable before continuing. Truncating length will reduce the number of reads violating the expected error filter, and therefore increase the number of reads proceding through the pipeline. The reverse reads will generally have lower quality, and therefore a lower truncLen than the forward reads.

## Filter and trim

This stage will use read truncation and max expected error function to remove low quality reads and read tails. All reads containing N bases will also be removed. this will output _postfilt_quality.pdf in the logs folder to determine how sucessfull it has been in cleaning up the quality.


```{r filter and trim}
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)
filtered_out <- vector("list", length=length(runs))

# Set important variables for trimming
maxEE <- 1 #Filter reads above Expected errors (EE = sum(10^(-Q/10))). Set higher for poor quality sequences.
rm.lowcomplex <- 0 # Remove low-complexity, set higher for NovaSeq and other 2 colour platforms
amplicon = 205 # Set to maximum size between the two primers. If working with variable barcode lengths, set to readlength

# Estimate best length to truncate forward and reverse reads to
truncLen <- estimate_trunclen(sampleF, sampleR, maxlength=amplicon)

for (i in 1:length(runs)){
  
   run_data <- samdf %>%
    filter(FCID == runs[i])
  
  path <- paste0("data/", runs[i], "/01_trimmed" )
  
  filtpath <- paste0("data/", runs[i], "/02_filtered" ) # Filtered forward files go into the path/filtered/ subdirectory
  dir.create(filtpath)
  
  fastqFs <- sort(list.files(path, pattern="R1_001.*"))
  fastqRs <- sort(list.files(path, pattern="R2_001.*"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- filterAndTrim(fwd = file.path(path, fastqFs), filt = file.path(filtpath, fastqFs),
                                      rev = file.path(path, fastqRs), filt.rev = file.path(filtpath, fastqRs),
                                      maxEE = maxEE, truncLen = truncLen, rm.lowcomplex = rm.lowcomplex,
                                      rm.phix = TRUE, matchIDs = TRUE, id.sep = "\\s",
                                      multithread = TRUE, compress = TRUE, verbose = TRUE)

  # post filtering plot
  filtFs <- sort(list.files(filtpath, pattern="R1_001.*", full.names = TRUE))
  sampleF <- sample(filtFs, readQC_subsample)
  sampleR <- sampleF %>% str_replace(pattern="R1_001", replacement = "R2_001")
  
  p1 <- plotQualityProfile(sampleF, aggregate = readQC_aggregate) +
    ggtitle(paste0(runs[i]," Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25))
  p2 <- plotQualityProfile(sampleR, aggregate = readQC_aggregate) + 
    ggtitle(paste0(runs[i]," Reverse Reads"))+
    scale_x_continuous(breaks=seq(0,300,25))
  
  #output plots
  if (!dir.exists("output/logs/")){ dir.create("output/logs/")}
  pdf(paste0("output/logs/", runs[i],"/",runs[i], "_postfilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  try(dev.off(), silent=TRUE)
  
  filtered_summary <- filtered_out[[i]] %>% 
    as.data.frame() %>%
    rownames_to_column("sample") %>%
    mutate(reads_remaining = signif(((reads.out / reads.in) * 100), 2)) %>%
    filter(!is.na(reads_remaining))
    
  message(paste0(signif(mean(filtered_summary$reads_remaining, na.rm = TRUE), 2), "% of reads remaining for ", runs[i]," after filtering"))
  
  # Print warning for each sample
  for(w in 1:nrow(filtered_summary)){
    if (filtered_summary[w,]$reads_remaining < 10) {
      message(paste0("WARNING: Less than 10% reads remaining for ", trim_summary[w,]$sample), "Check filtering parameters ")
    } 
  }
  
}
# Track reads
read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(
    filtered_out %>%
    map(as_tibble, rownames=NA) %>%
    map(rownames_to_column, var="Sample_ID") %>%
    purrr::set_names(runs) %>%
    bind_rows(.id="FCID") %>%
    mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement="")) %>%
    dplyr::rename(filter_input = reads.in, filter_output = reads.out),
  by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```


# Sequence processing

## Infer sequence variants for each run

The divisive partition algorithm is initial- ized by placing all unique sequences into a single partition and assigning the most abundant sequence as the center of that partition. All unique sequences are then compared to the center of their partition, error rates are calculated and stored, and the abundance p-value is calculated for each unique sequence. If the smallest p-value, after Bonferroni correction, falls below the user-settable threshold OMEGA_A, a new partition is formed with the unique sequence with the smallest p-value as its center, and all unique sequences are compared to the center of that new partition. After a new partition is formed, every unique sequence is
allowed to join the partition most likely to have produced it (i.e., the partition that produces the highest expected number of that unique sequence). At that point, the division procedure iterates, with each iteration consisting of identifying the unique sequence with the smallest p-value, forming a new partition with that sequence as its center, and reshuffling sequences to their most likely partition. Division continues until all abundance p-values are greater than
OMEGA_A; i.e., all unique sequences are consistent with being produced by amplicon sequencing the center of their partition. The inferred composition of the sample is then the set of central sequences and the corresponding total abundances of those parti- tions (alternatively, each read is denoised by replacing it with the central


DADA2 depends on a param- eterized error model (the 16 × 41 transition probabilities, for example, p(A→C, 35)), but if parameters are not known a priori then DADA2 can estimate them from the data. Given an inferred partition of the amplicon sequences, DADA2 records the mismatches between every sequence and the center of its partition and counts each type of mismatch (for example, the number of A→C mismatches where Q = 35). The resulting table of observed mismatches represents the errors inferred by DADA2 and can be used to estimate the parameters of the error model. 

DADA2’s default parameter estimation method is to perform a weighted loess fit to the regularized log of the observed mismatch rates as a function of their quality, separately for each transition type (for example, A→C mismatches are fit separately from A→G mismatches).

The problem with novaseq data is the binned quality scores.

NovaSeq error rate conversions

0-2 -> 2
3-14 -> 12
15-30 -> 23
31-40 -> 37

However, the error rate estimation function is a modular part of the algorithm, and users can provide their own R function to estimate the parameters of the error model from the observed mismatches if they prefer a different method.

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check. In this plot you generally want to see if the fitted error rates (black line) reasonably fit the observations (black points) and generally decrease with increasing Q (towards right of plot)

```{r Learn error rates }
set.seed(666) # set random seed for reproducability
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Set parameters
nbases = 1e+08 # Minimum number of total bases to use for error rate - increase if samples are deep sequenced (>1M reads per sample)
randomize = TRUE # Pick samples randomly to learn errors
pool = "pseudo" # Higher accuracy for low abundance at expense of runtime. Set to FALSE for a faster run

dada_out <- vector("list", length=length(runs))

for (i in seq(along=runs)){
  
  run_data <- samdf %>%
    filter(FCID == runs[i])
  
  #Check if run used twin tags
  filtpath <- paste0("data/", runs[i], "/02_filtered" )
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
    filtpath <- paste0("data/", runs[i], "/demux/trimmed/filtered" )
  } else if (twintagged == FALSE) {
    filtpath <- paste0("data/", runs[i], "/trimmed/filtered" )
  }
  
  filtFs <- list.files(filtpath, pattern="R1_001.*", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.*", full.names = TRUE)
  
 
  # Learn error rates from a subset of the samples and reads (rather than running self-consist with full dataset)
  errF <- learnErrors(filtFs, multithread = TRUE, nbases = nbases, randomize = randomize, qualityType = "FastqQuality", verbose=TRUE)
  errR <- learnErrors(filtRs, multithread = TRUE, nbases = nbases, randomize = randomize, qualityType = "FastqQuality", verbose=TRUE)
  
  #check if any run uses a platform with binned quality scores - if so enforce monotonicity
  binnedqual <-  any(run_data$seq_platform %in% c("Novaseq", "Nextseq"))
  if (binnedqual == TRUE){
  enforce_mono <- function(err){
    err.mat <- getErrors(err, detailed=TRUE)
    for(trans in c("A2C", "A2G", "A2T", "C2A", "C2G", "C2T", "G2A", "G2C", "G2T", "T2A", "T2C", "T2G")) {
      #Transform each error rate that is below the model value at the max Q score (40) to the model value at that max Q score.
      err.mat$err_out[trans,] <- pmax(err.mat$err_out[trans,], err.mat$err_out[trans,ncol(err.mat$err_out)])
    }
    return(err.mat)
  }
  
  mono.errmatF <- enforce_mono(errF)
  mono.errmatR <- enforce_mono(errR)
  
  print(plotErrors(mono.errmatF, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Forward Reads"))
  print(plotErrors(mono.errmatR, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Reverse Reads"))

  errF <- mono.errmatF
  errR <- mono.errmatR
  }
  
  #write out errors for diagnostics
  write_csv(as.data.frame(errF$trans), paste0("output/logs/", runs[i],"/",runs[i],"_errF_observed_transitions.csv"))
  write_csv(as.data.frame(errF$err_out), paste0("output/logs/", runs[i],"/",runs[i],"_errF_inferred_errors.csv"))
  write_csv(as.data.frame(errR$trans), paste0("output/logs/", runs[i],"/",runs[i],"_errR_observed_transitions.csv"))
  write_csv(as.data.frame(errR$err_out), paste0("output/logs/", runs[i],"/",runs[i],"_errR_inferred_errors.csv"))
  
  #output error plots to see how well the algorithm modelled the errors in the different runs
  p1 <- plotErrors(errF, nominalQ = TRUE) + ggtitle(paste0(runs[i], " Forward Reads"))
  p2 <- plotErrors(errR, nominalQ = TRUE) + ggtitle(paste0(runs[i], " Reverse Reads"))
  pdf(paste0("output/logs/", runs[i],"/",runs[i],"_errormodel.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  try(dev.off(), silent=TRUE)
  
   #Error inference and merger of reads
  dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool = pool, verbose = TRUE)
  dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool = pool, verbose = TRUE)
  saveRDS(dadaFs, paste0("output/logs/", runs[i],"/", runs[i], "_dadaFs.rds"))
  saveRDS(dadaRs, paste0("output/logs/", runs[i],"/", runs[i], "_dadaRs.rds"))

  # merge reads
  mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE, minOverlap = 12, trimOverhang = TRUE) 
  bind_rows(mergers, .id="Sample") %>%
    mutate(Sample = str_replace(Sample, pattern="_S.*$", replacement="")) %>%
    write_csv(paste0("output/logs/",runs[i],"/",runs[i], "_mergers.csv"))
  
  #Construct sequence table
  seqtab <- makeSequenceTable(mergers)
  saveRDS(seqtab, paste0("output/rds/", runs[i], "_seqtab.rds"))

  # Track reads
  getN <- function(x) sum(getUniques(x))
  dada_out[[i]] <- cbind(sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN)) %>%
    magrittr::set_colnames(c("dadaFs", "dadaRs", "merged")) %>%
    as.data.frame() %>%
    rownames_to_column("Sample_ID") %>%
    mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement=""))
}

# Track reads
read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(dada_out %>%
            purrr::set_names(runs) %>%
            bind_rows(.id="FCID"),
            by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```

## Merge Runs, Remove Chimeras and filter

All the below filters increase the proportion of reads classified to lower levels compared to higher levels

```{r merge runs and remove chimeras}
seqtabs <- list.files("output/rds/", pattern="seqtab.rds", full.names = TRUE)

# If multiple seqtabs present, merge.
if(length(seqtabs) > 1){
  st.all <- mergeSequenceTables(tables=seqtabs)
} else if(length(seqtabs) == 1) {
  st.all <- readRDS(seqtabs)
}

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)
message(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

#cut to expected size allowing for some codon indels
seqtab.cut <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% 200:210]
message(paste0("Identified ",
               length(colnames(seqtab.nochim))  - length(colnames(seqtab.cut)),
               " incorrectly sized sequences out of ", length(colnames(seqtab.nochim)) , " input sequences."))
message(paste(sum(seqtab.cut)/sum(seqtab.nochim),"of the abundance remaining after cutting to expected size"))

#Filter for homology with the target marker
fwh_ref <-  ape::read.dna("reference/fwh_insecta_aligned_curated.fasta", format="fasta")
model <- aphid::derivePHMM(fwh_ref)

seqs <- as.DNAbin(DNAStringSet(colnames(seqtab.cut)))
seqtab.homo <- taxreturn::clean_seqs(seqs, minscore = 100, shave = FALSE, model = model)

#Filter sequences containing stop codons
seqs <- DNAStringSet(getSequences(seqtab.homo))
codon_filt <- codon_filter(seqs)
seqtab.final <- seqtab.homo[,colnames(seqtab.homo) %in% codon_filt]
message(paste0("Identified ",
               length(colnames(seqtab.homo))  - length(colnames(seqtab.final)),
               " sequences containing stop codon out of ", length(colnames(seqtab.homo)) , " input sequences."))
message(paste(sum(seqtab.final)/sum(seqtab.homo),"of the abundance remaining after removing seqs with stop codons "))

saveRDS(seqtab.final, "output/rds/seqtab_final.rds")

# summarise cleanup
cleanup <- st.all %>%
  as.data.frame() %>%
  pivot_longer( everything(),
    names_to = "OTU",
    values_to = "Abundance") %>%
  group_by(OTU) %>%
  summarise(Abundance = sum(Abundance)) %>%
  mutate(length  = nchar(OTU)) %>%
  mutate(type = case_when(
    !OTU %in% getSequences(seqtab.nochim) ~ "Chimera",
    !OTU %in% getSequences(seqtab.cut) ~ "Incorrect size",
    !OTU %in% getSequences(seqtab.final) ~ "Stop codons",
    TRUE ~ "Real"
  )) 
write_csv(cleanup, "output/logs/chimera_summary.csv")

#Read Tracker
read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(as.data.frame(cbind(rowSums(st.all),
                                rowSums(seqtab.nochim),
                                rowSums(seqtab.cut),
                                rowSums(seqtab.final))) %>%
              magrittr::set_colnames(c("seqtab", "chimera_filt", "size_filt", "seqtab_final")) %>%
              rownames_to_column("Sample_ID") %>%
              mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement="")),
            by="Sample_ID")

write_csv(read_tracker, "output/logs/read_tracker.csv")

# Output length distribution plots
gg.abundance <- ggplot(cleanup, aes(x=length, y=Abundance, fill=type))+
              geom_bar(stat="identity") + 
              ggtitle("Abundance of sequences")

gg.unique <- ggplot(cleanup, aes(x=length, fill=type))+
            geom_histogram() + 
            ggtitle("Number of unique sequences")

pdf(paste0("output/logs/seqtab_length_dist.pdf"), width = 11, height = 8 , paper="a4r")
  plot(gg.abundance / gg.unique)
try(dev.off(), silent=TRUE)
```

## Assign taxonomy with IDTAXA & Exact matching

We will use the IDTAXA algorithm of Murali et al 2018 - https://doi.org/10.1186/s40168-018-0521-5

This requires training on a curated reference database - The pre-trained file can be found in the reference folder, alternatively see the taxreturn scripts to curate a reference database and train a new classifier.

Folllowing assignment with IDTAXA, we will also use exact matching with a reference database to assign more sequences (including the synthetic positive controls) to species level

```{r IDTAXA}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")
ranks <-  c("Root", "Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") # ranks of interest

#Classify using IDTAXA
trainingSet <- readRDS("reference/idtaxa.rds")
dna <- DNAStringSet(getSequences(seqtab.final)) # Create a DNAStringSet from the ASVs
ids <- IdTaxa(dna, trainingSet, processors=1, threshold = 60, verbose=TRUE) 

# Output plot of ids
pdf(paste0("output/logs/idtaxa.pdf"), width = 11, height = 8 , paper="a4r")
  plot(ids)
try(dev.off(), silent=TRUE)

#Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
tax <- t(sapply(ids, function(x) {
    taxa <- paste0(x$taxon,"_", x$confidence)
    taxa[startsWith(taxa, "unclassified_")] <- NA
    taxa
  })) %>%
  purrr::map(unlist) %>%
  stri_list2matrix(byrow=TRUE, fill=NA) %>%
  magrittr::set_colnames(ranks) %>%
  magrittr::set_rownames(getSequences(seqtab.final)) %>%
  as.data.frame()  %T>%
  write.csv("output/logs/idtaxa_results.csv") %>%  #Write out logfile with confidence levels
  mutate_all(str_replace,pattern="(?:.(?!_))+$", replacement="") %>%
  taxreturn::propagate_tax(from="Phylum") %>% #Propagate high order ranks to unassigned ASV'
  magrittr::set_rownames(getSequences(seqtab.final)) %>%
  as.matrix()

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxa.rds") 
```

## Make phylogenetic tree

```{r phylogenetic tree}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")

seqs <- getSequences(seqtab.final)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

library(phangorn)
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)

#Fit NJ tree
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit <- pml(treeNJ, data=phang.align)

#Fit ML tree
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))

# Write phytree to disk
saveRDS(fitGTR, "output/rds/phytree.rds") 

#Output newick tree
write.tree(fitGTR$tree, file="output/tree.nwk")
```


## Make phyloseq object

```{r }
seqtab.final <- readRDS("output/rds/seqtab_final.rds")

#Extract start of sequence names
rownames(seqtab.final) <- str_replace(rownames(seqtab.final), pattern="_S.*$", replacement="")

tax <- readRDS("output/rds/tax_IdTaxa.rds") 

phy <- readRDS("output/rds/phytree.rds")$tree

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) %>%
  filter(!duplicated(Sample_ID)) %>%
  magrittr::set_rownames(.$Sample_ID) 

#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax), sample_data(samdf),
               otu_table(seqtab.final, taxa_are_rows = FALSE), phy_tree(phy))

if(nrow(seqtab.final) > nrow(sample_data(ps))){
  message("Warning: the following samples were not included in phyloseq object, check sample names match the sample metadata")
  message(rownames(seqtab.final)[!rownames(seqtab.final) %in% sample_names(ps)])
}

saveRDS(ps, "output/rds/ps.rds") 

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

#Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
library(data.table)
summarise_taxa(ps, "Species", "Sample_ID") %>%
  spread(key="Sample_ID", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

summarise_taxa(ps, "Genus", "Sample_ID") %>%
  spread(key="Sample_ID", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

#Output fasta of all ASV's
seqateurs::ps_to_fasta(ps, "output/all_taxa.fasta", rank="Species")
```


# Output fate of reads

```{r fate}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name", rank_names(ps)) %>%
  group_by(Rank, Sample_ID) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(Reads_classified = sum(Abundance * !is.na(Name))) %>% 
  pivot_wider(names_from = "Rank",
              values_from = "Reads_classified")

read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(sum_reads, by="Sample_ID")
write_csv(read_tracker, "output/logs/read_tracker.csv")


gg.readtracker <- read_tracker %>%
  dplyr::rename(trimmed_reads = output_reads) %>%
  dplyr::mutate(input_reads = input_reads / 2,
                trimmed_reads = trimmed_reads / 2) %>%
  pivot_longer(3:tail(colnames(.), 1),
               names_to = "type",
               values_to = "value") %>%
  mutate(stage= case_when(
    type %in% c("input_reads","input_bases", "ktrimmed_bases", "ktrimmed_reads", "trimmed_reads", "output_bases") ~ "BBDuk",
    type %in% c("filter_input","filter_output") ~ "FilterAndTrim",
    type %in% c("dadaFs","dadaRs", "merged") ~ "DADA",
    type %in% c("seqtab","chimera_filt", "size_filt", "seqtab_final") ~ "Seqtab",
    type %in% c("Root", "Kingdom", "Phylum", "Class","Order", "Family", "Genus", "Species") ~ "taxonomy",
  )) %>%
  ggplot(aes(x=type, y=value, fill=stage)) +
  geom_bar(stat="identity") +
  scale_x_discrete(limits = c("input_reads", "trimmed_reads", "filter_input", "filter_output",
                              "dadaFs", "dadaRs", "merged","seqtab", "chimera_filt", "size_filt", "seqtab_final",
                              "Kingdom", "Phylum", "Class","Order", "Family", "Genus", "Species")) +
  facet_wrap(~Sample_ID) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0.5)) +
  scale_fill_brewer(palette="Spectral")
  
pdf(paste0("output/logs/read_tracker.pdf"), width = 11, height = 8 , paper="a4r")
  gg.readtracker
try(dev.off(), silent=TRUE)

```
