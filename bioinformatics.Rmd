---
title: "Drosophila Metabarcoding"
title: "Bioinformatics"
author: "Alexander Piper"
date: "`r Sys.Date()`"
output:
  
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
editor_options: 
  chunk_output_type: console
---
# Setup & Packages 

This pipeline requires various R packages to be installed prior to running. These are obtained from CRAN, Bioconductor and Github. The taxreturn and seqateurs packageR package also provide wrappers around other software packages for QC. For convenience we will download and install these software in a new folder called “bin”

```{r setup, include=TRUE}
# Knitr global setup - change eval to true to run code
library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE,fig.show = "hold", fig.keep = "all")

#Set required packages
.cran_packages <- c("ggplot2",
                    "gridExtra",
                    "tidyverse", 
                    "tidymodels",
                    "scales",
                    "stringdist",
                    "patchwork",
                    "vegan",
                    "ggpubr",
                    "seqinr",
                    "stringi",
                    "phangorn",
                    "filesstrings")
.bioc_packages <- c("dada2",
                    "phyloseq",
                    "DECIPHER",
                    "savR",
                    "Biostrings",
                    "ShortRead",
                    "ngsReports")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

devtools::install_github("alexpiper/taxreturn")
devtools::install_github("alexpiper/seqateurs")

library(taxreturn)
library(seqateurs)

#Install blast
blast_install(dest.dir = "bin")

#Install bbmap
bbmap_install(dest.dir = "bin")

#Install fastqc
fastqc_install(dest.dir = "bin")

# Create directories
if(!dir.exists("data")){dir.create("data", recursive = TRUE)}
if(!dir.exists("reference")){dir.create("reference", recursive = TRUE)}
if(!dir.exists("output/logs")){dir.create("output/logs", recursive = TRUE)}
if(!dir.exists("output/fasta")){dir.create("output/fasta", recursive = TRUE)}
if(!dir.exists("output/csv")){dir.create("output/csv", recursive = TRUE)}
if(!dir.exists("output/rds")){dir.create("output/rds", recursive = TRUE)}

#Source themes
source('R/themes.R')
```


# Demultiplex libraries

```{bash demultiplex }
###BASH###
#load module
module load bcl2fastq2/2.20.0-foss-2018b

#raise amount of available file handles
ulimit -n 4000

#Miseq Run CB3DR - Primer testing
bcl2fastq -p 12 --runfolder-dir /group/sequencing/190331_M03633_0310_000000000-CB3DR  \
--output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run1_CB3DR \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run1_CB3DR/SampleSheet.csv \
--no-lane-splitting --barcode-mismatches 0

#Miseq Run CK3HD - Testing replicate tagged primers & SynMock
bcl2fastq -p 12 --runfolder-dir /group/sequencing/190628_M03633_0331_000000000-CK3HD  \
--output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run2_CK3HD \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run2_CK3HD/SampleSheet.csv \
--no-lane-splitting --barcode-mismatches 0

#Miseq Run CJKFJ - Ladder spike ins
bcl2fastq -p 12 --runfolder-dir /group/sequencing/190722_M03633_0336_000000000-CJKFJ/ \
--output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run3_CJKFJ \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run3_CJKFJ/SampleSheet_CJKFJ.csv \
--no-lane-splitting --barcode-mismatches 0

#Novaseq Run HLVKYDMXX - Came already demultiplexed
```

## Set analysis parameters and create sample sheet
In order to track samples and relevant QC statistics throughout the metabarcoding pipeline, we will first create a new samplesheet from our input samplesheets. This function requires both the SampleSheet.csv used for the sequencing run, and the runParameters.xml, both of which can be obtained from the demultiplexed sequencing run folder/

We will also add the relevant primers at this stage to this sheet:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2_P5    ACACTCTTTCCCTACACGACGCTCTTCCGATCT     GGDACWGGWTGAACWGTWTAYCCHCC

REVERSE PRIMER:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2n_P7   GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT  GTRATWGCHCCDGCTARWACWGG
    
```{r create samplesheet}
runs <- dir("data/") #Find all directories within data
SampleSheets <- paste0("data/", runs, "/SampleSheet.csv")
runParameters <- paste0("data/", runs, "/runParameters.xml")

## Create new samplesheet with all run information
samdf <- seqateurs::create_samplesheet(SampleSheet = SampleSheets, runParameters = runParameters)

# Add primers & twin tag sequences to samdf 
samdf <- samdf %>%
  #filter(FCID %in% c("CK3HD", "CJKFJ", "HLVKYDMXX")) %>%
  mutate(count =3 ) %>%
  uncount(count) %>%
  rownames_to_column("replicate") %>%
  mutate(replicate = case_when(
    str_detect(replicate, "\\.1") ~ 2,
    str_detect(replicate, "\\.2") ~ 3,
    TRUE ~ 1
  )) %>%
  filter(!(FCID == "CB3DR" & replicate > 1)) %>% #First flowcell didnt use replication
  mutate(
    Fprimer = case_when(
    str_detect(Sample_ID, "fwhF2") & replicate == 1 & FCID == "CB3DR"  ~  "GGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(Sample_ID, "BF1") & replicate == 1 & FCID == "CB3DR"  ~  "ACWGGWTGRACWGTNTAYCC",
    str_detect(Sample_ID, "SauronS878") & replicate == 1 & FCID == "CB3DR"  ~  "GGDRCWGGWTGAACWGTWTAYCCNCC",
    #Replicated samples
    str_detect(Sample_ID, "fwhF2") & replicate == 1 & !FCID == "CB3DR"  ~ "GAGGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(Sample_ID, "fwhF2") & replicate == 2 & !FCID == "CB3DR" ~  "TGTGGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(Sample_ID, "fwhF2") & replicate == 3 & !FCID == "CB3DR" ~  "AGAAGGDACWGGWTGAACWGTWTAYCCHCC",
    # final runs - samples werent named with primers
    replicate == 1 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "GAGGDACWGGWTGAACWGTWTAYCCHCC",
    replicate == 2 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "TGTGGDACWGGWTGAACWGTWTAYCCHCC",
    replicate == 3 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "AGAAGGDACWGGWTGAACWGTWTAYCCHCC",
    ),
    Rprimer = case_when(
    str_detect(Sample_ID, "fwhR2n") & replicate == 1 & FCID == "CB3DR"  ~  "GTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "BR1") & replicate == 1 & FCID == "CB3DR"  ~  "ARYATDGTRATDGCHCCDGC",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 1 & FCID == "CB3DR"  ~  "TATDGTRATDGCHCCNGC",
    #Replicated samples
    str_detect(Sample_ID, "fwhR2n") & replicate == 1 & !FCID == "CB3DR"  ~ "ACGTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "fwhR2n") & replicate == 2 & !FCID == "CB3DR" ~  "TCCGTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "fwhR2n") & replicate == 3 & !FCID == "CB3DR" ~  "CTGCGTRATWGCHCCDGCTARWACWGG",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 1 & !FCID == "CB3DR"  ~ "GCTATDGTRATDGCHCCNGC",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 2 & !FCID == "CB3DR" ~  "AGGTATDGTRATDGCHCCNGC",
    str_detect(Sample_ID, "HexCOIR4") & replicate == 3 & !FCID == "CB3DR" ~  "CACGTATDGTRATDGCHCCNGC",
    # final runs - samples werent named with primers
    replicate == 1 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "ACGTRATWGCHCCDGCTARWACWGG",
    replicate == 2 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "TCCGTRATWGCHCCDGCTARWACWGG",
    replicate == 3 & FCID %in% c("CJKFJ", "HLVKYDMXX") ~  "CTGCGTRATWGCHCCDGCTARWACWGG",
    ),
    twintagF = case_when(
      !FCID == "CB3DR"~ substr(Fprimer, 1, 8), # Use first 8 characters of F primer as F tag
      FCID == "CB3DR" ~ as.character(NA)
    ),
    twintagR = case_when(
      !FCID == "CB3DR"~ substr(Rprimer, 1, 8), # Use first 8 characters of F primer as F tag
      FCID == "CB3DR" ~ as.character(NA)
    ),
    Investigator_Name = "Alexander Piper",
    Assay = "Metabarcoding"
)

write_csv(samdf, "sample_data/Sample_info.csv")
```


# Quality checks:

We will conduct 2 quality checks here
* Check of the entire sequence run quality
* Sample level check using fastqc

```{r QC}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Set parameters
barcodemismatch <- 0 # set the barcode mismatch used during demultiplexing (illumina default is 1)

i=1
for (i in 1:length(runs)){
  ## Run level quality check using savR - Not compatible with novaseq
  path <- paste0("data/", runs[i], "/")
  fc <- savR(path)
  qc.dir <- paste0("output/logs/", runs[i],"/" )
  dir.create(qc.dir, re)
  write_csv(correctedIntensities(fc), paste0(qc.dir, "correctedIntensities.csv"))
  write_csv(errorMetrics(fc), paste0(qc.dir, "errorMetrics.csv"))
  write_csv(extractionMetrics(fc), paste0(qc.dir, "extractionMetrics.csv"))
  write_csv(qualityMetrics(fc), paste0(qc.dir, "qualityMetrics.csv"))
  write_csv(tileMetrics(fc), paste0(qc.dir, "tileMetrics.csv"))

  avg_intensity <- fc@parsedData[["savCorrectedIntensityFormat"]]@data %>%
    group_by(tile, lane) %>%
    summarise(Average_intensity = mean(avg_intensity)) %>% 
    ungroup() %>%
    mutate(side = case_when(
      str_detect(tile, "^11") ~ "Top",
      str_detect(tile, "^21") ~ "Bottom"
        ))%>%
    ggplot(aes(x=lane, y=as.factor(tile), fill=Average_intensity)) +
    geom_tile() +
    facet_wrap(~side, scales="free") +
    scale_fill_viridis_c()
  
  pdf(file=paste(qc.dir, "/avgintensity.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  plot(avg_intensity)
  try(dev.off(), silent=TRUE)
  
  pdf(file=paste(qc.dir, "/PFclusters.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  pfBoxplot(fc)
  try(dev.off(), silent=TRUE)

  for (lane in 1:fc@layout@lanecount) {
  pdf(file=paste(qc.dir, "/QScore_L", lane, ".pdf", sep=""), width = 11, height = 8 , paper="a4r")
      qualityHeatmap(fc, lane, 1:fc@directions)
  try(dev.off(), silent=TRUE)
  } 
}

## Sample level quality check using fastqc
for (i in 1:length(runs)){
  path <- paste0("data/", runs[i], "/")
  qc.dir <- paste0("output/logs/", runs[i],"/FASTQC" )
  dir.create(qc.dir, recursive=TRUE)
  #qc_out <- seqateurs::fastqc(fq.dir = path, qc.dir	= qc.dir, fastqc.path = "bin/FastQC/fastqc", threads=2) 
  writeHtmlReport(qc.dir, overwrite = TRUE, gcType ="Genome",  quiet=FALSE) # requires PANDOC!
  #filesstrings::file.move(paste0(qc.dir, "ngsReports_Fastqc.html"), paste0("output/logs/", runs[i],"/"))
}
  
```


# Demultiplex by primer & trim

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Prior to begining this workflow, samples were demultiplexed and illumina adapters were removed by the MiSeq software, however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

In this study there were 2 amplicons of different size, and 3 differently tagged versions of these to indicate PCR replicates. To demultiplex these extra tags and trim these primers we will use the wrapper functions for the BBTools software contained in the seqateurs package

## fwhF2-fwhR2n amplicon:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2T1_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	  GAGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T2_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	 TGTGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T3_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	AGAAGGDACWGGWTGAACWGTWTAYCCHCC
    
REVERSE PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2nT1_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	ACGTRATWGCHCCDGCTARWACWGG
    fwhR2nT2_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	TCCGTRATWGCHCCDGCTARWACWGG
    fwhR2nT3_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	CTGCGTRATWGCHCCDGCTARWACWGG
    
```{r primer trimming , message=FALSE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

#Demultiplex samples
runs <- unique(samdf$FCID)

i=1

#Create vectors to track reads
trimmed <- vector("list", length=length(runs))
demux <- vector("list", length=length(runs))

for (i in 1:length(runs)){
  path <- paste0("data/", runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  qc.dir <- paste0("output/logs/", runs[i],"/" )

  run_data <- samdf %>%
    filter(FCID == runs[i])
  
  #Get primer sequences
  primers <- c(unique(run_data$Fprimer), unique(run_data$Rprimer))
  
  #Check if samples were twin tagged - these require extra round of demultiplexing
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
      demuxpath <- file.path(path, "00_demux") # Filtered forward files go into the path/filtered/ subdirectory
      dir.create(demuxpath)
      
      fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
      fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))
      
      demux[[i]] <- bbdemux(install="bin/bbmap", fwd=fastqFs, rev=fastqRs, Fbarcodes = unique(run_data$twintagF),
                    Rbarcodes = unique(run_data$twintagR), degenerate=TRUE, out.dir=demuxpath, threads=1 ,
                    mem=4,  hdist=0, overwrite=TRUE)
      
      demux_fastqs <- sort(list.files(paste0(demuxpath), pattern="_R1R2_", full.names = TRUE))

      trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd = demux_fastqs,
              primers = primers, checkpairs = FALSE,
              degenerate = TRUE, out.dir="01_trimmed", trim.end = "left", # Need to make this output up a directory, currently its a subdir of 00_demux
              kmer=NULL, tpe=TRUE, tbo=TRUE,
              ordered = TRUE, mink = FALSE, hdist = 2,
              maxlength =(max(run_data$Fread, run_data$Rread) - sort(nchar(primers), decreasing = FALSE)[1]) +5, 
              overwrite = TRUE, quality = FALSE, quiet=FALSE)
      
      #Re-split interleaved fastq's
      trimmedpath <- file.path(path, "01_trimmed") 
      trimmed_fastqs <- sort(list.files(trimmedpath, pattern="_R1R2_", full.names = TRUE))
      bbsplit(install="bin/bbmap", files=trimmed_fastqs, overwrite=TRUE)
  
      
  } else if (twintagged == FALSE) {
    
    fastqFs <- sort(list.files(paste0(path), pattern="_R1_", full.names = TRUE))
    fastqRs <- sort(list.files(paste0(path), pattern="_R2_", full.names = TRUE))
    if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))

    trimpath <- file.path(path, "01_trimmed")
      dir.create(trimpath)
      
      trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd = fastqFs, rev = fastqRs,
                primers = primers, checkpairs = FALSE,
                degenerate = TRUE, out.dir="01_trimmed", trim.end = "left",
                kmer=NULL, tpe=TRUE, tbo=TRUE,
                ordered = TRUE, mink = FALSE, hdist = 2,
                maxlength =(max(run_data$Fread, run_data$Rread) - sort(nchar(primers), decreasing = FALSE)[1]) +5,
                overwrite = TRUE, quality = FALSE, quiet=FALSE)

  }
  # QC - sequence lengths
  #pre_trim <- plot_lengths(dir=path, aggregate=TRUE, sample=1e5)
  #post_trim <- plot_lengths(dir=paste0(path, "/01_trimmed/"), aggregate=TRUE, sample=1e5) # Seems to be a bug in this

  #pdf(file=paste(qc.dir, "/readlengths.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  #plot(pre_trim)
  #plot(post_trim)
  #try(dev.off(), silent=TRUE)
  
  trim_summary <- trimmed[[i]] %>% 
    mutate(perc_reads_remaining = signif(((output_reads / input_reads) * 100), 2),
           perc_bases_remaining = signif(((output_bases / input_bases) * 100), 2)
           ) %>%
    filter(!is.na(perc_reads_remaining))
    
  message(paste0(signif(mean(trim_summary$perc_reads_remaining, na.rm = TRUE), 2),
                 "% of reads and ",
                 signif(mean(trim_summary$perc_bases_remaining, na.rm = TRUE), 2),
                 "% of bases remaining for ", runs[i]," after trimming"))
  
  # Print warning for each sample
  for(w in 1:nrow(trim_summary)){
    if (trim_summary[w,]$perc_reads_remaining < 10) {message(paste0("WARNING: Less than 10% bases remaining for ",trim_summary[w,]$sample), "Check primers are correct")}
  }
}
# Track reads
read_tracker <- samdf %>% 
  select(Sample_ID, FCID) %>%
  left_join(
    trimmed %>%
    purrr::set_names(runs) %>%
    bind_rows(.id="FCID") %>%
    mutate(sample = str_replace(basename(sample), pattern="_S.*$", replacement="")) %>%
    dplyr::rename(Sample_ID = sample),
  by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```
  
  
## Plot read quality & lengths
  
  
```{r QA plot, eval = TRUE, cache= TRUE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Plotting parameters
readQC_aggregate <- TRUE
readQC_subsample <-  12
amplicon = 205 # Set to maximum size between the two primers. If working with variable barcode lengths, set to readlength

for (i in 1:length(runs)){
  run_data <- samdf %>%
    filter(FCID == runs[i])

  path <- paste0("data/", runs[i], "/01_trimmed" )
 
  ##Get trimmed files, accounting for empty files (28 indicates empty sample)
  trimmedFs <- sort(list.files(path, pattern="_R1_", full.names = TRUE))
  trimmedFs <- trimmedFs[file.size(trimmedFs) > 28]

  #Choose a random subsample for quality checks
  sampleF <- sample(trimmedFs, readQC_subsample) #NOTE - need to have option to pass
  sampleR <- sampleF %>% str_replace(pattern="_R1_", replacement = "_R2_")
  
  #Estimate an optimat trunclen
  truncLen <- estimate_trunclen(sampleF, sampleR, maxlength=amplicon)

  #Plot qualities
  gg.Fqual <- plot_quality(sampleF) +
    geom_vline(aes(xintercept=truncLen[1]), colour="blue") +
    annotate("text", x = truncLen[1]-10, y =2, label = paste0("Suggested truncLen = ", truncLen[1]), colour="blue") +
    ggtitle(paste0(runs[i], " Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25))
  gg.Fee <- plot_maxEE(sampleF) + 
    geom_vline(aes(xintercept=truncLen[1]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =-3, label = paste0("Suggested truncLen = ", truncLen[1]), colour="blue") +
    ggtitle(paste0(runs[i], " Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")
  gg.Rqual <- plot_quality(sampleR) + 
    geom_vline(aes(xintercept=truncLen[2]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =2, label = paste0("Suggested truncLen = ", truncLen[2]), colour="blue") +
    ggtitle(paste0(runs[i], " Reverse Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")
  gg.Ree <- plot_maxEE(sampleR) +
    geom_vline(aes(xintercept=truncLen[2]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =-3, label = paste0("Suggested truncLen = ", truncLen[2]), colour="blue") +
    ggtitle(paste0(runs[i], " Reverse Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")

  Qualplots <- (gg.Fqual + gg.Rqual) / (gg.Fee + gg.Ree)
  
  #output plots
  pdf(paste0("output/logs/",runs[i],"/",runs[i], "_prefilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(Qualplots)
  try(dev.off(), silent=TRUE)
}

## These parameters need to be the same for all samples!

```

This has output a prefilt_quality.pdf plot for each of the runs analysed in the logs folder. On the top is the quality score per cycle, and on the bottom is the cumulative expected errors (calculated as EE = sum(10^(-Q/10)) on a log scale. For the quality plot, the median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. For the maxEE lines, the red lines showing the expected error filter options. The blue vertical line on both plots shows the suggested truncLen option automatically determined.

Ensure that the blue suggested trunclen looks reasonable before continuing. Truncating length will reduce the number of reads violating the expected error filter, and therefore increase the number of reads proceding through the pipeline. The reverse reads will generally have lower quality, and therefore a lower truncLen than the forward reads.

## Filter and trim

This stage will use read truncation and max expected error function to remove low quality reads and read tails. All reads containing N bases will also be removed. this will output _postfilt_quality.pdf in the logs folder to determine how sucessfull it has been in cleaning up the quality.


```{r filter and trim}
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)
filtered_out <- vector("list", length=length(runs))

# Set important variables for trimming
maxEE <- 1 #Filter reads above Expected errors (EE = sum(10^(-Q/10))). Set higher for poor quality sequences.
rm.lowcomplex <- 0 # Remove low-complexity, set higher for NovaSeq and other 2 colour platforms
amplicon = 205 # Set to maximum size between the two primers. If working with variable barcode lengths, set to readlength

# Estimate best length to truncate forward and reverse reads to
truncLen <- 120

for (i in 1:length(runs)){
  
   run_data <- samdf %>%
    filter(FCID == runs[i])
  
  path <- paste0("data/", runs[i], "/01_trimmed" )
  
  filtpath <- paste0("data/", runs[i], "/02_filtered" ) # Filtered forward files go into the path/filtered/ subdirectory
  dir.create(filtpath)
  
  fastqFs <- sort(list.files(path, pattern="R1_001.*"))
  fastqRs <- sort(list.files(path, pattern="R2_001.*"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- filterAndTrim(fwd = file.path(path, fastqFs), filt = file.path(filtpath, fastqFs),
                                      rev = file.path(path, fastqRs), filt.rev = file.path(filtpath, fastqRs),
                                      maxEE = maxEE, truncLen = truncLen, rm.lowcomplex = rm.lowcomplex,
                                      rm.phix = TRUE, matchIDs = TRUE, id.sep = "\\s",
                                      multithread = TRUE, compress = TRUE, verbose = TRUE)

  # post filtering plot
  filtFs <- sort(list.files(filtpath, pattern="R1_001.*", full.names = TRUE))
  sampleF <- sample(filtFs, readQC_subsample)
  sampleR <- sampleF %>% str_replace(pattern="R1_001", replacement = "R2_001")
  
  p1 <- plotQualityProfile(sampleF, aggregate = readQC_aggregate) +
    ggtitle(paste0(runs[i]," Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25))
  p2 <- plotQualityProfile(sampleR, aggregate = readQC_aggregate) + 
    ggtitle(paste0(runs[i]," Reverse Reads"))+
    scale_x_continuous(breaks=seq(0,300,25))
  
  #output plots
  if (!dir.exists("output/logs/")){ dir.create("output/logs/")}
  pdf(paste0("output/logs/", runs[i],"/",runs[i], "_postfilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  try(dev.off(), silent=TRUE)
  
  filtered_summary <- filtered_out[[i]] %>% 
    as.data.frame() %>%
    rownames_to_column("sample") %>%
    mutate(reads_remaining = signif(((reads.out / reads.in) * 100), 2)) %>%
    filter(!is.na(reads_remaining))
    
  message(paste0(signif(mean(filtered_summary$reads_remaining, na.rm = TRUE), 2), "% of reads remaining for ", runs[i]," after filtering"))
  
  # Print warning for each sample
  for(w in 1:nrow(filtered_summary)){
    if (filtered_summary[w,]$reads_remaining < 10) {
      message(paste0("WARNING: Less than 10% reads remaining for ", trim_summary[w,]$sample), "Check filtering parameters ")
    } 
  }
  
}
# Track reads
filtered <- filtered_out %>%
    map(as_tibble, rownames=NA) %>%
    map(rownames_to_column, var="Sample_ID") %>%
    purrr::set_names(runs) %>%
    bind_rows(.id="FCID") %>%
    mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement="")) 

write_csv(filtered, "output/logs/filtered.csv")
```


# Sequence processing

## Infer sequence variants for each run

The divisive partition algorithm is initial- ized by placing all unique sequences into a single partition and assigning the most abundant sequence as the center of that partition. All unique sequences are then compared to the center of their partition, error rates are calculated and stored, and the abundance p-value is calculated for each unique sequence. If the smallest p-value, after Bonferroni correction, falls below the user-settable threshold OMEGA_A, a new partition is formed with the unique sequence with the smallest p-value as its center, and all unique sequences are compared to the center of that new partition. After a new partition is formed, every unique sequence is
allowed to join the partition most likely to have produced it (i.e., the partition that produces the highest expected number of that unique sequence). At that point, the division procedure iterates, with each iteration consisting of identifying the unique sequence with the smallest p-value, forming a new partition with that sequence as its center, and reshuffling sequences to their most likely partition. Division continues until all abundance p-values are greater than
OMEGA_A; i.e., all unique sequences are consistent with being produced by amplicon sequencing the center of their partition. The inferred composition of the sample is then the set of central sequences and the corresponding total abundances of those parti- tions (alternatively, each read is denoised by replacing it with the central


DADA2 depends on a param- eterized error model (the 16 × 41 transition probabilities, for example, p(A→C, 35)), but if parameters are not known a priori then DADA2 can estimate them from the data. Given an inferred partition of the amplicon sequences, DADA2 records the mismatches between every sequence and the center of its partition and counts each type of mismatch (for example, the number of A→C mismatches where Q = 35). The resulting table of observed mismatches represents the errors inferred by DADA2 and can be used to estimate the parameters of the error model. 

DADA2’s default parameter estimation method is to perform a weighted loess fit to the regularized log of the observed mismatch rates as a function of their quality, separately for each transition type (for example, A→C mismatches are fit separately from A→G mismatches).

The problem with novaseq data is the binned quality scores.

NovaSeq error rate conversions

0-2 -> 2
3-14 -> 12
15-30 -> 23
31-40 -> 37

However, the error rate estimation function is a modular part of the algorithm, and users can provide their own R function to estimate the parameters of the error model from the observed mismatches if they prefer a different method.

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check. In this plot you generally want to see if the fitted error rates (black line) reasonably fit the observations (black points) and generally decrease with increasing Q (towards right of plot)

# Alternative to enforncing monotonicity:
https://github.com/benjjneb/dada2/issues/938

```{r Learn error rates }
set.seed(666) # set random seed for reproducability
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Set parameters
nbases = 1e+9 # Minimum number of total bases to use for error rate - increase if samples are deep sequenced (>1M reads per sample)
randomize = TRUE # Pick samples randomly to learn errors
pool = "pseudo" # Higher accuracy for low abundance at expense of runtime. Set to FALSE for a faster run

dada_out <- vector("list", length=length(runs))
i=1
for (i in 1:length(runs)){
  
  run_data <- samdf %>%
    filter(FCID == runs[i])
  
  filtpath <- paste0("data/", runs[i], "/02_filtered" )
  
  # Load forward reads
  filtFs <- list.files(filtpath, pattern="R1_001.*", full.names = TRUE)
  filtFs <- filtFs[!str_detect(filtFs, "Undetermined")]
  
  # Load reverse reads
  filtRs <- list.files(filtpath, pattern="R2_001.*", full.names = TRUE)
  filtRs <- filtRs[!str_detect(filtRs, "Undetermined")]
 
  # Learn error rates from a subset of the samples and reads (rather than running self-consist with full dataset)
  errF <- learnErrors(filtFs, multithread = TRUE, nbases = nbases, randomize = randomize, qualityType = "FastqQuality", verbose=TRUE)
  errR <- learnErrors(filtRs, multithread = TRUE, nbases = nbases, randomize = randomize, qualityType = "FastqQuality", verbose=TRUE)
  
  #check if any run uses a platform with binned quality scores - if so enforce monotonicity
  binnedqual <-  any(str_detect(run_data$InstrumentName, "^A|^N")) #Novaseq starts with A, Nextseq with N
  if (binnedqual == TRUE){
  enforce_mono <- function(err){
    err.mat <- getErrors(err, detailed=TRUE)
    for(trans in c("A2C", "A2G", "A2T", "C2A", "C2G", "C2T", "G2A", "G2C", "G2T", "T2A", "T2C", "T2G")) {
      #Transform each error rate that is below the model value at the max Q score (40) to the model value at that max Q score.
      err.mat$err_out[trans,] <- pmax(err.mat$err_out[trans,], err.mat$err_out[trans,ncol(err.mat$err_out)])
    }
    return(err.mat)
  }
  
  mono.errmatF <- enforce_mono(errF)
  mono.errmatR <- enforce_mono(errR)
  
  print(plotErrors(mono.errmatF, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Forward Reads"))
  print(plotErrors(mono.errmatR, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Reverse Reads"))

  errF <- mono.errmatF
  errR <- mono.errmatR
  }
  
  #write out errors for diagnostics
  write_csv(as.data.frame(errF$trans), paste0("output/logs/", runs[i],"/",runs[i],"_errF_observed_transitions.csv"))
  write_csv(as.data.frame(errF$err_out), paste0("output/logs/", runs[i],"/",runs[i],"_errF_inferred_errors.csv"))
  write_csv(as.data.frame(errR$trans), paste0("output/logs/", runs[i],"/",runs[i],"_errR_observed_transitions.csv"))
  write_csv(as.data.frame(errR$err_out), paste0("output/logs/", runs[i],"/",runs[i],"_errR_inferred_errors.csv"))
  
  #output error plots to see how well the algorithm modelled the errors in the different runs
  p1 <- plotErrors(errF, nominalQ = TRUE) + ggtitle(paste0(runs[i], " Forward Reads"))
  p2 <- plotErrors(errR, nominalQ = TRUE) + ggtitle(paste0(runs[i], " Reverse Reads"))
  pdf(paste0("output/logs/", runs[i],"/",runs[i],"_errormodel.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  try(dev.off(), silent=TRUE)
  
   #Error inference and merger of reads
  dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool = pool, verbose = TRUE)
  dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool = pool, verbose = TRUE)
  saveRDS(dadaFs, paste0("output/logs/", runs[i],"/", runs[i], "_dadaFs.rds"))
  saveRDS(dadaRs, paste0("output/logs/", runs[i],"/", runs[i], "_dadaRs.rds"))

  # merge reads
  mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE, minOverlap = 12, trimOverhang = TRUE) 
  bind_rows(mergers, .id="Sample") %>%
    mutate(Sample = str_replace(Sample, pattern="_S.*$", replacement="")) %>%
    write_csv(paste0("output/logs/",runs[i],"/",runs[i], "_mergers.csv"))
  
  #Construct sequence table
  seqtab <- makeSequenceTable(mergers)
  saveRDS(seqtab, paste0("output/rds/", runs[i], "_seqtab.rds"))

  # Track reads
  getN <- function(x) sum(getUniques(x))
  dada_out[[i]] <- cbind(sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN)) %>%
    magrittr::set_colnames(c("dadaFs", "dadaRs", "merged")) %>%
    as.data.frame() %>%
    rownames_to_column("Sample_ID") %>%
    mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement=""))
}

# Track reads
merged <- dada_out %>%
  purrr::set_names(runs) %>%
  bind_rows(.id="run")

write_csv(merged, "output/logs/merged.csv")
```

## Merge Runs, Remove Chimeras and filter

All the below filters increase the proportion of reads classified to lower levels compared to higher levels

```{r merge runs and remove chimeras}
seqtabs <- list.files("output/rds/", pattern="seqtab.rds", full.names = TRUE)

# If multiple seqtabs present, merge.
if(length(seqtabs) > 1){
  st.all <- mergeSequenceTables(tables=seqtabs)
} else if(length(seqtabs) == 1) {
  st.all <- readRDS(seqtabs)
}

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)
message(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))
saveRDS(seqtab.nochim, "output/rds/seqtab_nochim.rds")


#cut to expected size allowing for some codon indels
seqtab.cut <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% 200:210]
message(paste0("Identified ",
               length(colnames(seqtab.nochim))  - length(colnames(seqtab.cut)),
               " incorrectly sized sequences out of ", length(colnames(seqtab.nochim)) , " input sequences."))
message(paste(sum(seqtab.cut)/sum(seqtab.nochim),"of the abundance remaining after cutting to expected size"))
saveRDS(seqtab.cut, "output/rds/seqtab_cut.rds")


#Filter for homology with the target marker
fwh_ref <-  ape::read.dna("reference/fwh_insecta_aligned_curated.fasta", format="fasta")
model <- aphid::derivePHMM(fwh_ref)

seqs <- as.DNAbin(DNAStringSet(colnames(seqtab.cut)))
phmm_filt <- taxreturn::clean_seqs(seqs, minscore = 100, shave = FALSE, model = model)
codon_filt <- codon_filter(phmm_filt)

#Filter sequences containing stop codons

seqtab.final <- seqtab.cut[,colnames(seqtab.cut) %in% as.character(codon_filt)]
saveRDS(seqtab.final, "output/rds/seqtab_final.rds")

# summarise cleanup
cleanup <- st.all %>%
  as.data.frame() %>%
  pivot_longer( everything(),
    names_to = "OTU",
    values_to = "Abundance") %>%
  group_by(OTU) %>%
  summarise(Abundance = sum(Abundance)) %>%
  mutate(length  = nchar(OTU)) %>%
  mutate(type = case_when(
    !OTU %in% getSequences(seqtab.nochim) ~ "Chimera",
    !OTU %in% getSequences(seqtab.cut) ~ "Incorrect size",
    !OTU %in% getSequences(seqtab.final) ~ "Stop codons",
    TRUE ~ "Real"
  )) 
write_csv(cleanup, "output/logs/chimera_summary.csv")

#Read Tracker
chimera_filtered <- as.data.frame(cbind(rowSums(st.all),
                                rowSums(seqtab.nochim),
                                rowSums(seqtab.cut),
                                rowSums(seqtab.final))) %>%
              magrittr::set_colnames(c("seqtab", "chimera_filt", "size_filt", "seqtab_final")) %>%
              rownames_to_column("Sample_ID") %>%
              mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement=""))

write_csv(chimera_filtered, "output/logs/chimera_filtered.csv")

# Get filter stats for paper
#total ASV's
nrow(st.all)
#Chimeric ASV's
nrow(st.all) - nrow(seqtab.nochim)
#wrong size
nrow(seqtab.nochim) - nrow(seqtab.cut)
#Non-homologous or stop codons
nrow(seqtab.cut) - nrow(seqtab.final)


# Output length distribution plots
gg.abundance <- ggplot(cleanup, aes(x=length, y=Abundance, fill=type))+
              geom_histogram(stat="identity") + 
              ggtitle("Abundance of sequences")

gg.unique <- ggplot(cleanup, aes(x=length, fill=type))+
            geom_histogram() + 
            ggtitle("Number of unique sequences")

pdf(paste0("output/logs/seqtab_length_dist.pdf"), width = 11, height = 8 , paper="a4r")
  plot(gg.abundance / gg.unique)
try(dev.off(), silent=TRUE)
```

## Assign taxonomy with IDTAXA & Exact matching

We will use the IDTAXA algorithm of Murali et al 2018 - https://doi.org/10.1186/s40168-018-0521-5

This requires training on a curated reference database - The pre-trained file can be found in the reference folder, alternatively see the taxreturn scripts to curate a reference database and train a new classifier.

Folllowing assignment with IDTAXA, we will also use exact matching with a reference database to assign more sequences (including the synthetic positive controls) to species level

```{r IDTAXA}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")
ranks <-  c("Root", "Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") # ranks of interest

#Classify using IDTAXA
trainingSet <- readRDS("reference/idtaxa.rds")
dna <- DNAStringSet(getSequences(seqtab.final)) # Create a DNAStringSet from the ASVs
ids <- IdTaxa(dna, trainingSet, processors=1, threshold = 60, verbose=TRUE) 

# Output plot of ids
pdf(paste0("output/logs/idtaxa.pdf"), width = 11, height = 8 , paper="a4r")
  plot(ids)
try(dev.off(), silent=TRUE)

#Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
tax <- t(sapply(ids, function(x) {
    taxa <- paste0(x$taxon,"_", x$confidence)
    taxa[startsWith(taxa, "unclassified_")] <- NA
    taxa
  })) %>%
  purrr::map(unlist) %>%
  stri_list2matrix(byrow=TRUE, fill=NA) %>%
  magrittr::set_colnames(ranks) %>%
  magrittr::set_rownames(getSequences(seqtab.final)) %>%
  as.data.frame()  %T>%
  write.csv("output/logs/idtaxa_results.csv") %>%  #Write out logfile with confidence levels
  mutate_all(str_replace,pattern="(?:.(?!_))+$", replacement="") %>%
  taxreturn::propagate_tax(from="Phylum") %>% #Propagate high order ranks to unassigned ASV'
  magrittr::set_rownames(getSequences(seqtab.final)) %>%
  as.matrix()

# Fix bad taxonomic assignments
tax <- read_rds("output/rds/tax_IdTaxa.rds")

#tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_albomicans")] <-
#"Drosophila_immigrans"
#tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_sulfurigaster")] <-
#"Drosophila_immigrans"
#tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_mauritiana/simulans")] <-
#"Drosophila_simulans"
#tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <-
#"Carpophilus_nr.dimidiatus"
#tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
#tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxa.rds") 
```

## Summarise Top hit distribution with reference database

```{R top hit dist}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")

seqs <- insect::char2dna(colnames(seqtab.final))
names(seqs) <- colnames(seqtab.final)

out <- blast_top_hit(query=seqs, db="reference/old/merged_final_bftrimmed.fa.gz", threshold=60 )
bckup <- out

# Colour by IDTAXA assignment
tax <- readRDS("output/rds/tax_IdTaxa.rds")

joint <- out %>% 
  ungroup %>% # add ungroup to defaults
  dplyr::select(qseqid, pident, Species) %>%
  rename(blastspp = Species) %>%
  left_join(tax %>% as.data.frame(stringsAsFactors=FALSE) %>% rownames_to_column("qseqid"), by="qseqid") %>%
  pivot_longer(cols=c("Root","Kingdom","Phylum","Class","Order","Family","Genus","Species"),
               names_to = "rank",
               values_to = "name") %>%
  mutate(pos =  match(rank, c("Root","Kingdom","Phylum","Class","Order","Family","Genus","Species"))) %>%
  mutate(name = replace(name, str_detect(name, "__"),NA)) %>%
  drop_na() %>%
  group_by(qseqid) %>%
  top_n(1,pos) %>%# Get lowest classified rank
  ungroup()

gg.tophit <- joint %>%
  mutate(rank = factor(rank, levels = c("Root","Kingdom","Phylum","Class","Order","Family","Genus","Species"))) %>%
  ggplot(aes(x=pident, fill=rank, order=))+ 
  geom_histogram(colour="black", binwidth = 1, position = "stack") + 
  ggtitle("Top hit identity distribution") + 
  xlab("Top hit identity") + 
  ylab("Sequences") + 
  scale_x_continuous(breaks=seq(60,100,2)) +
  scale_fill_brewer(name = "IDTAXA Assignment", palette = "Spectral")


# These are mostly Arachnids? and other outgroups? were these in the IDTAXA DB?
test <- joint %>% filter(rank=="Root")

``` 


## Make phylogenetic tree

```{r phylogenetic tree}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")

seqs <- getSequences(seqtab.final)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

library(phangorn)
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)

#Fit NJ tree
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit <- pml(treeNJ, data=phang.align)

#Fit ML tree
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))

# Write phytree to disk
saveRDS(fitGTR, "output/rds/phytree.rds") 

#Output newick tree
write.tree(fitGTR$tree, file="output/tree.nwk")
```


# Make Phyloseq object

```{r create PS, eval = FALSE}
seqtab <- readRDS("output/rds/seqtab_final.rds")

# Add FCID to run 1 samples
rownames(seqtab)[!str_detect(rownames(seqtab), "Rep") ] <- paste0("CB3DR_", rownames(seqtab)[!str_detect(rownames(seqtab), "Rep") ])

# Add FCID to run2 samples
rownames(seqtab)[str_detect(rownames(seqtab), "Rep") & 
                   !str_detect(rownames(seqtab), "HLVKYDMXX") & 
                   !str_detect(rownames(seqtab), "DL|CL")] <- paste0("CK3HD_", rownames(seqtab)[str_detect(rownames(seqtab), "Rep") & 
                   !str_detect(rownames(seqtab), "HLVKYDMXX") & 
                   !str_detect(rownames(seqtab), "DL|CL")])

	
# Add FCID to run 3 samples
rownames(seqtab)[str_detect(rownames(seqtab), "Rep") & 
                   !str_detect(rownames(seqtab), "HLVKYDMXX") & 
                   str_detect(rownames(seqtab), "DL|CL")] <- paste0("CJKFJ_", rownames(seqtab)[str_detect(rownames(seqtab), "Rep") & 
                   !str_detect(rownames(seqtab), "HLVKYDMXX") & 
                   str_detect(rownames(seqtab), "DL|CL")])

#Reformat sample IDs
rownames(seqtab)  <- rownames(seqtab) %>%
  str_remove("\\..*$") %>%
  str_replace("\\_S[0-9].*\\_...", replacement="_") %>%
  str_replace("_$", "_1") %>%
  str_replace("1in10", "1:10")

tax <- readRDS("output/rds/tax_IdTaxaExact.rds") 
seqs <- DNAStringSet(colnames(seqtab))
names(seqs) <- seqs
phy <- readRDS("output/rds/phytree.rds")$tree


##### Rename problematic samples
#Could do this with the new dplyr functionality
rownames(seqtab)  <- rownames(seqtab) %>%
 str_replace_all("D250M1-", "D250M4REP-") %>% # Works
 str_replace_all("D250M4-", "D250M2REP-") %>% # Works
 str_replace_all("D250M5-", "D250M3REP-") %>% #FAILED library
 str_replace_all("D250M3-", "D250M1REP-") %>% #FP suzukii - low reads
 str_replace_all("D250M2-", "D250M5REP-") %>% #Works
 str_replace_all("D500M1-", "D500M4REP-") %>% #Works 
 str_replace_all("D500M4-", "D500M1REP-") %>% #FP Suzukii
 str_replace_all("D500M5-", "D500M2REP-") %>% #Works
 str_replace_all("D500M3-", "D500M3REP-") %>% #Works but low reads for Suz + Biarmipes 
 str_replace_all("D500M2-", "D500M5REP-") %>% #Works
 str_replace_all("D1000M1-", "D1000M3REP-") %>% #Works
 str_replace_all("D1000M4-", "D1000M1REP-") %>% #Works
 str_replace_all("D1000M5-", "D1000M2REP-") %>% #Works
 str_replace_all("D1000M3-", "D1000M5REP-") %>% #Works
 str_replace_all("D1000M2-", "D1000M4REP-") %>% #Works
 str_replace_all("CM10-", "CM9REP-") %>%
 str_replace_all("CM11-", "CM10REP-") %>%
 str_replace_all("CM9-", "CM11REP-") %>%
 str_replace_all("CML2-", "CML6REP-")%>%
 str_replace_all("CML3-", "CML2REP-")%>%
 str_replace_all("CML4-", "CML3REP-")%>%
 str_replace_all("CML5-", "CML4REP-")%>%
 str_replace_all("CML6-", "CML5REP-")%>%
 str_replace_all("CT5-", "CT4REP-")%>%
 str_replace_all("CT4-", "CT5REP-") %>%
str_replace_all("REP", "")

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/sample_info2.csv", header=TRUE) %>%
  magrittr::set_rownames(.$sample_id)

# Will probably need to rename the seqtabs and append the flowcell number onto the samples before they are merged

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax), 
               sample_data(samdf),
               otu_table(seqtab, taxa_are_rows = FALSE),
               phy_tree(phy),
               refseq(seqs))

if(nrow(seqtab) > nrow(sample_data(ps))){warning("Warning: All samples not included in phyloseq object, check sample names match the sample metadata")}

rownames(samdf)[which(!rownames(sample_data(ps))  %in% rownames(samdf))]
rownames(sample_data(ps))[which(!rownames(samdf)  %in% rownames(sample_data(ps)))]

# Rename all taxa
taxa_names(ps) <- paste0("SV", seq(ntaxa(ps)),"-",tax_table(ps)[,7])

saveRDS(ps, "output/rds/ps_idtaxaExact.rds") 

#Rename synthetic orders
tax_table(ps)[,2][which(str_detect(tax_table(ps)[,7], "Synthetic"))] <- "Arthropoda"

#Subset to Drosophila (Remove carpophilus)
ps <- ps %>%
  subset_samples(!str_detect(sample_name, "CM[0-9]|CT[0-9]|CML[0-9]")
  ) %>%
  subset_taxa(Phylum == "Arthropoda") %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) 
dir.create("output/csv")
dir.create("output/csv/unfiltered/")

##Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
seqateurs::summarise_taxa(ps, "Species", "sample_name") %>%
  filter(!str_detect(sample_name, "NTC")) %>%
  spread(key="sample_name", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

seqateurs::summarise_taxa(ps, "Genus", "sample_name") %>%
  spread(key="sample_name", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

##Output fasta of all ASV's - Name each one by abundance + taxonomic assignment
seqateurs::ps_to_fasta(ps, "output/all_taxa.fasta")
```


## Output fate of reads through pipeline

```{r read fate}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  dplyr::select(sample_id, rank_names(ps), Abundance) %>%
  pivot_longer(rank_names(ps),
               names_to="rank",
               values_to="name") %>%
  group_by(sample_id, rank) %>% 
  mutate(name = replace(name, str_detect(name, "__"), NA)) %>% 
  dplyr::summarise(reads_classified = sum(Abundance * !is.na(name))) %>%
  pivot_wider(names_from = "rank",
              values_from = "reads_classified")

# plot just the taxonomic assignment

read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  janitor::clean_names() %>%
  left_join(sum_reads, by="sample_id")
write_csv(read_tracker, "output/logs/read_tracker.csv")

gg.readtracker <- read_tracker %>%
  dplyr::rename(trimmed_reads = output_reads) %>%
  dplyr::mutate(input_reads = input_reads / 2,
                trimmed_reads = trimmed_reads / 2) %>%
  pivot_longer(3:tail(colnames(.), 1),
               names_to = "type",
               values_to = "value") %>%
  mutate(stage= case_when(
    type %in% c("input_reads","input_bases", "ktrimmed_bases", "ktrimmed_reads", "trimmed_reads", "output_bases") ~ "BBDuk",
    type %in% c("filter_input","filter_output") ~ "FilterAndTrim",
    type %in% c("dadaFs","dadaRs", "merged") ~ "DADA",
    type %in% c("seqtab","chimera_filt", "size_filt", "seqtab_final") ~ "Seqtab",
    type %in% c("Root", "Kingdom", "Phylum", "Class","Order", "Family", "Genus", "Species") ~ "taxonomy",
  )) %>%
  ggplot(aes(x=type, y=value, fill=stage)) +
  geom_bar(stat="identity") +
  scale_x_discrete(limits = c("input_reads", "trimmed_reads", "filter_input", "filter_output",
                              "dadaFs", "dadaRs", "merged","seqtab", "chimera_filt", "size_filt", "seqtab_final",
                              "Kingdom", "Phylum", "Class","Order", "Family", "Genus", "Species")) +
  facet_wrap(~Sample_ID) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0.5)) +
  scale_fill_brewer(palette="Spectral")
  
pdf(paste0("output/logs/read_tracker.pdf"), width = 11, height = 8 , paper="a4r")
  gg.readtracker
try(dev.off(), silent=TRUE)

```


### Summary statistics

```{r summarise taxa}
# N unique species and samples
speedyseq::psmelt(ps) %>%
  summarise(n_extracts = n_distinct(extract_id), n_samples = n_distinct(sample_id))

# Spread of reads
speedyseq::psmelt(ps) %>%
  group_by(extract_id) %>%
  summarise(Abundance = sum(Abundance)) %>%
  ungroup() %>%
  summarise(mean = mean(Abundance), 
            se = sd(Abundance)/sqrt(length(Abundance)),
            max = max(Abundance),
            min = min(Abundance))

# Spread of ASVs
speedyseq::psmelt(ps) %>%
  group_by(extract_id) %>%
  dplyr::filter(Abundance > 0) %>%
  summarise(counts = n_distinct(OTU)) %>%
  ungroup() %>%
  summarise(mean = mean(counts), 
            se = sd(counts)/sqrt(length(counts)),
            max = max(counts),
            min = min(counts))

#Fraction of reads assigned to each taxonomic rank
speedyseq::psmelt(ps) %>%
  dplyr::select(rank_names(ps), Abundance) %>%
  pivot_longer(rank_names(ps),
               names_to="rank",
               values_to="name") %>%
  group_by(rank) %>% 
  mutate(name = replace(name, str_detect(name, "__"), NA)) %>% 
  dplyr::summarise(reads_classified = sum(Abundance * !is.na(name))) %>%
  mutate(frac_reads = reads_classified / sum(sample_sums(ps))) %>%
  mutate(rank = factor(rank, rank_names(ps))) %>%
  arrange(rank)

#Fraction of ASV's assigned to each taxonomic rank
tax_table(ps) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
    pivot_longer(rank_names(ps),
               names_to="rank",
               values_to="name") %>%
  group_by(rank) %>%
  mutate(name = replace(name, str_detect(name, "__"), NA)) %>% 
  dplyr::summarise(OTUs_classified = sum(!is.na(name))) %>%
  mutate(frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(rank = factor(rank, rank_names(ps))) %>%
  arrange(rank)

# Unique taxa at each rank
speedyseq::psmelt(ps) %>%
  dplyr::select(rank_names(ps)) %>%
    pivot_longer(rank_names(ps),
               names_to="rank",
               values_to="name") %>%
  mutate(name = replace(name, str_detect(name, "__"), NA)) %>% 
  drop_na() %>%
  group_by(rank) %>%
  summarise_all(list(n_distinct)) %>%
  mutate(rank = factor(rank, rank_names(ps))) %>%
  arrange(rank)
```

### Prevalence assesment

```{R prevalence}
#Prevalence matrix
prevdf <- ps %>%
    otu_table %>%
  apply(2, function(x) ifelse(x > 0, 1, 0)) %>%
  colSums() %>%
  as.data.frame() %>%
  rownames_to_column("OTU") %>%
  magrittr::set_colnames(c("OTU", "prevalence")) %>%
  left_join(taxa_sums(ps) %>%
              as.data.frame %>%
  rownames_to_column("OTU")%>%
  magrittr::set_colnames(c("OTU", "abundance"))) %>%
  left_join(tax_table(ps)%>%
               as.data.frame %>%
  rownames_to_column("OTU"))
  
#Prevalence plot
gg.prev <- prevdf %>%
  ggplot(aes(x=abundance, y=prevalence / nsamples(ps), colour=Genus)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_x_log10() +
  facet_wrap(~Order) +
  base_theme+
  theme(legend.position="none") +
  labs(
    x = "Total Abundance",
    y = "Prevalence [Frac. Samples]",
    title = "Prevalence of orders across dataset",
    subtitle = "Colored by Genus")

gg.prev

pdf(file="fig/prevalence.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.prev)
try(dev.off(), silent=TRUE)
```

# Filtering 

## Taxon filtering

```{R taxon filt}
get_taxa_unique(ps, "Order")

ps # Check the number of taxa prior to removal
ps0 <- ps %>%
  subset_taxa(
    Phylum == "Arthropoda" & 
    Class %in% c("Insecta", "Arachnida", "Collembola")
  )
ps # Confirm that the taxa were removed
get_taxa_unique(ps0, "Phylum")
get_taxa_unique(ps0, "Class")
get_taxa_unique(ps0, "Order")
```

## Minimum read filtering

```{r minimum reads}
### Remove positive controls
##check mocks
#ps1 <- subset_samples(ps1, !str_detect(sample_names(ps1), "POS"))
#message((nsamples(ps1) - nsamples(ps1)), " outlier samples dropped")

#ps1 <- prune_samples(sample_sums(ps1) >0 , ps1)

#Plot rarefaction curve
out <- rarecurve(otu_table(ps0), step=10000)

rare <- lapply(out, function(x){
  b <- as.data.frame(x)
  b <- data.frame(OTU = b[,1], count = rownames(b))
  b$count <- as.numeric(gsub("N", "",  b$count))
  return(b)
})
names(rare) <- sample_names(ps0)

rare <- map_dfr(rare, function(x){
  z <- data.frame(x)
  return(z)
}, .id = "sample")

# threshold for read removal
threshold = 1000

gg.rare <- ggplot(data = rare)+
  geom_line(aes(x = count, y = OTU, group=sample), alpha=0.5)+
  geom_point(data = rare %>% 
               group_by(sample) %>% 
               top_n(1, count),
             aes(x = count, y = OTU, colour=(count > threshold))) +
  geom_label(data = rare %>% 
               group_by(sample) %>% 
               top_n(1, count),
             aes(x = count, y = OTU,label=sample, colour=(count > threshold)),
             hjust=-0.05)+
  scale_x_continuous(labels =  scales::scientific_format()) +
  geom_vline(xintercept=threshold, linetype="dashed") +
  labs(colour = "Sample kept?") +
  xlab("Sequence reads") +
  ylab("Observed ASV's")

gg.rare

#Write out figure
pdf(file="fig/rarefaction.pdf", width = 11, height = 8 , paper="a4r")
  plot(gg.rare)
try(dev.off(), silent=TRUE)

#Remove all samples under the minimum read threshold 
ps1 <- prune_samples(sample_sums(ps0)>=threshold, ps0) 
ps1 <- filter_taxa(ps1, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
message(nsamples(ps0) - nsamples(ps1), " Samples and ", ntaxa(ps0) - ntaxa(ps1), " taxa under read threshold Dropped")
```

## Overlap between replicates

Venn diagrams? - Heatmap of presence absense

Species accumulation curve- adding more extraction reps, pcr reps, biological samples, sequence depth

Want to show - Different extraction replicates adds more - but this is probably due to contamination
PCR replicates doesnt add more

Visualise overlap, then merge choosing a majority rules approach

```{r overlap between replicates}

# Mean pairwise distance between PCR replicates
mpd_pcr <- ps1 %>%
  otu_table() %>%
  as("matrix") %>%
  as_tibble(rownames = "sample_id") %>%
  left_join(sample_data(ps1) %>%
              as("data.frame") %>%
              dplyr::select(sample_id, extract_id, type, fcid)) %>%
  group_by(extract_id) %>%
  add_tally()  %>%
  filter(n > 1,
         fcid=="HLVKYDMXX",
         !str_detect(extract_id, "BLANK|SynMock|POS")) %>%
  group_modify(~ {
      .x %>% 
        dplyr::select(-any_of(c("sample_id", "sample_name", "extract_id", "type", "fcid"))) %>%
        as.matrix() %>%
        vegdist(method="jac", binary = T) %>%
        as.matrix() %>%
        as_tibble()%>%
        pivot_longer(everything(),
                     names_to = "sample",
                     values_to = "dist")%>%
        dplyr::filter(dist > 0) %>%
        summarise(dist = mean(dist)) 
    })

# Mean pairwise distance between extraction replicates
mpd_extraction <- ps1 %>%
   merge_samples(group = "extract_id", fun="sum") %>%
  otu_table() %>%
  as("matrix") %>%
  as_tibble(rownames = "extract_id") %>%
  left_join(sample_data(ps1) %>%
              as("data.frame") %>%
              as_tibble()%>%
              dplyr::select(sample_id, sample_name, extract_id, type, fcid)) %>%
  group_by(sample_name) %>%
  add_tally() %>%
  filter(n > 1,
         fcid=="HLVKYDMXX",
         !str_detect(extract_id, "BLANK|SynMock|POS")) %>%
  group_modify(~ {
      .x %>% 
        dplyr::select(-any_of(c("sample_id", "sample_name", "extract_id", "type", "fcid"))) %>%
        as.matrix() %>%
        vegdist(method="jac", binary = T) %>%
        as.matrix() %>%
        as_tibble()%>%
        pivot_longer(everything(),
                     names_to = "sample",
                     values_to = "dist")%>%
        dplyr::filter(dist > 0) %>%
        summarise(dist = mean(dist)) 
    })

# This shoudl be trap types instead
mpd_traps <- ps1 %>%
   merge_samples(group = "sample_name", fun="sum") %>%
  otu_table() %>%
  as("matrix") %>%
  as_tibble(rownames = "sample_name") %>%
  left_join(sample_data(ps1) %>%
              as("data.frame") %>%
              as_tibble()%>%
              dplyr::select(sample_id, sample_name, extract_id, type, fcid)) %>%
  group_by(type) %>%
  add_tally() %>%
  filter(n > 1,
         fcid=="HLVKYDMXX",
         !str_detect(extract_id, "BLANK|SynMock|POS")) %>%
  group_modify(~ {
      .x %>% 
        dplyr::select(-any_of(c("sample_id", "sample_name", "extract_id", "type", "fcid"))) %>%
        as.matrix() %>%
        vegdist(method="jac", binary = T) %>%
        as.matrix() %>%
        as_tibble()%>%
        pivot_longer(everything(),
                     names_to = "sample",
                     values_to = "dist")%>%
        dplyr::filter(dist > 0)  %>%
        summarise(dist = mean(dist, na.rm=TRUE)) 
    })


# Mean pairwise distance between all samples
mpd_samples <- ps1 %>%
   merge_samples(group = "sample_name", fun="sum") %>%
  otu_table() %>%
  as("matrix") %>%
  as_tibble(rownames = "sample_name") %>%
  left_join(sample_data(ps1) %>%
              as("data.frame") %>%
              as_tibble()%>%
              dplyr::select(sample_name, extract_id, type, fcid) %>%
              filter(fcid=="HLVKYDMXX"))%>%
    filter(!str_detect(extract_id, "BLANK|SynMock|POS"),
         fcid=="HLVKYDMXX") %>%
  dplyr::select(-any_of(c("sample_id", "extract_id", "type", "fcid"))) %>%
  distinct() %>%
  column_to_rownames("sample_name") %>%
  as.matrix() %>%
  vegdist(method="jac", binary = T) %>%
  as.matrix() %>%
  as_tibble()%>%
  pivot_longer(everything(),
              names_to = "sample",
              values_to = "dist")%>%
  dplyr::filter(dist > 0)

# Plot together:
gg.mpd <- do.call("list", mget(grep("mpd_",names(.GlobalEnv),value=TRUE))) %>%
  bind_rows(.id="type") %>%
  ungroup %>%
  select(type, dist) %>%
  mutate(type = type %>% 
           str_remove("^.+_") %>%
           factor(levels = c("samples", "traps", "extraction", "pcr"))) %>%
  ggplot(aes(x = type, y = dist, fill=type)) + 
  geom_boxplot() +
  base_theme + 
  scale_fill_brewer(palette= "Paired") +
  labs(x = "Type",
       y = "Jaccard dissimilarity")

gg.mpd 

pdf(file="fig/mean_pairwise.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.mpd)
try(dev.off(), silent=TRUE)
```

## Abundance counts for different replicates
```{r}
# Heatmap of abundance of pcr replicates
gg.repmap <- ps1 %>% 
  speedyseq::psmelt() %>%
  filter(fcid=="HLVKYDMXX", Abundance > 0) %>%
  mutate(extrep = extract_id %>% str_extract("ex.*$")) %>%
  ggplot(aes(x = replicate, y = sample_name, fill = log10(Abundance))) +
  geom_tile()+
  scale_fill_viridis_c()+
  facet_grid(~extrep)+
  base_theme+
  labs(x = "PCR Replicate",
       y = "Sample Name")

gg.repmap

pdf(file="fig/replicate_abundances.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.repmap)
try(dev.off(), silent=TRUE)
```


## Merge technical replicates

```{r merge replicates}
# Merge replicates
ps.merged <- ps1 %>%
    merge_samples(group = "sample_name", fun="sum")

#This loses the sample metadata - Need to add it agian
sample_data(ps.merged) <- sample_data(ps1) %>%
  as("data.frame")  %>%
  mutate(sample_id = sample_name) %>%
  mutate(sample_name = sample_name %>%
           str_remove_all("BF1-BR1-|SauronS878-HexCOIR4-|fwhF2-HexCOIR4-|fwhF2-fwhR2n-") %>%
           str_remove("HLVKYDMXX_")) %>%
  dplyr::filter(!duplicated(sample_id)) %>%
  magrittr::set_rownames(.$sample_id)
```


## Filter index switching

We will filter index switching using a logistic regression model trained on the mocks

```{r index switch removal}
# Subset to mocks
ps_model <- ps.merged

# Check Drosophila_suzukii in sample D500M1 - these are definitely misplaced and ruinign the modelling

#Rename taxa to match expected
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_albomicans")] <-
"Drosophila_immigrans"
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_nasuta")] <-
"Drosophila_immigrans"
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_sulfurigaster")] <-
"Drosophila_immigrans"
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Drosophila_mauritiana/simulans")] <-
"Drosophila_simulans"
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <-
"Carpophilus_nr.dimidiatus"
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
tax_table(ps_model)[,7][which(tax_table(ps_model)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"

ps_model <- ps_model %>%
  speedyseq::tax_glom(taxrank = "Species") %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) 

exp <- read_csv("sample_data/expected_quant_merged.csv") %>%
  dplyr::rename(sample_name = Sample) %>%
  pivot_longer(-sample_name,
               names_to= "taxon",
               values_to= "expected") %>%
  mutate(taxon = str_replace(taxon, pattern=" ",replacement="_"),
         sample_name = str_remove(sample_name, "-exp*.$")) %>%
  dplyr::filter(!is.na(sample_name),
  !str_detect(sample_name, "CM[0-9]|CT[0-9]|CML[0-9]|Syn"),
  expected > 0) %>%
  distinct()

#Get observed
sam <- speedyseq::psmelt(ps_model) %>%
  janitor::clean_names() %>%
  mutate(taxon = species) %>%
  #filter(!str_detect(taxon, "__")) %>%#Remove unclassified
  dplyr::select(otu, sample_name, sample_id, taxon, abundance, target_subfragment, fcid, material_type = type) %>%
  #mutate(sample_name = sample_name %>%
  #         str_remove_all("BF1-BR1-|SauronS878-HexCOIR4-|fwhF2-HexCOIR4-|fwhF2-fwhR2n-") %>%
  #         str_remove("HLVKYDMXX_")) %>%
  mutate(extract_id = sample_name) %>%
  mutate(sample_name = str_remove(sample_name, "-exp*.$"))

#Create a mocks table to train the model on
mock_train <- sam %>%
  filter(abundance > 0) %>%
  mutate(type = case_when( 
         (sample_name %in% exp$sample_name) & (material_type == "DrosMock") & !str_detect(taxon, "__") ~ "train",
         TRUE ~ "test")) %>%
  left_join(exp, by = c("sample_name","taxon"))  %>%
  mutate(outcome = case_when(
    abundance > 0 & is.na(expected) & type== "train" ~ "FP",
    abundance > 0 & !is.na(expected) & type=="train" ~ "TP",
    #abundance == 0 & expected > 0 ~ "FN",
    TRUE ~ as.character(NA)
  )) %>%
  mutate(outcome = case_when(
    outcome == "FP" & abundance > 10000 ~ as.character(NA), #Deal with obvious misannotated expecteds
    TRUE ~ outcome
  )) %>%
  group_by(extract_id,fcid, target_subfragment) %>%
  dplyr::mutate(abundance_clr = abundance) %>%
  mutate_at(vars(abundance_clr ), ~metacal::clr(. + 0.5) ) %>%
  ungroup() %>%
  dplyr::select(fcid, sample_id, extract_id, sample_name, otu, taxon, abundance, abundance_clr, outcome) 

# Create modelling recipe
switch_recipe <- recipe(outcome ~ ., data = mock_train) %>%
  update_role(sample_id, sample_name, extract_id, otu, taxon, abundance, new_role = "ID") %>%
  #step_string2factor(all_nominal(), -all_outcomes(), -has_role("ID")) %>%
  step_dummy(fcid) %>%
  #step_other(all_predictors(), -all_numeric(), threshold = 1, other = "other") %>%
  step_zv(all_predictors()) #%>%
  #step_novel(all_predictors(), -all_numeric(), -has_role("ID"))%>%
  #step_unknown(all_predictors(), -all_numeric(), -has_role("ID")) 

# See if the new replacement even worked

#Prep a recipe
rec_prepped <- prep(switch_recipe)

rec_prepped 

# Define model
logit_spec <- logistic_reg(mode = "classification") %>%
  set_engine(engine = "glm") 

# Create workflow
logit_wf <- workflow() %>%
  add_model(logit_spec) %>%
  add_recipe(switch_recipe)

# Fit workflow
logistic_glm <- logit_wf  %>%
  fit(mock_train)

# See how it went on the trainigng dataset
predictions_glm <- logistic_glm %>%
  predict(new_data = mock_train) %>%
  bind_cols(bake(rec_prepped, new_data =  mock_train))

# Confusion matrix
predictions_glm %>%
  conf_mat(outcome, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 8)

# Calculate metrics
multimetric <- metric_set(accuracy, bal_accuracy, sens, yardstick::spec, precision, recall, ppv, npv)
multimetric(predictions_glm, truth = outcome, estimate = .pred_class)  

# Make an ROC curve
## get probabilities on test set
predictions_prob <- logistic_glm %>%
  predict(new_data = mock_train, type="prob") %>%
  bind_cols(bake(rec_prepped, new_data =  mock_train))

# plot curve
roc_data <- roc_curve(predictions_prob, truth = outcome, .pred_FP) 
roc_data %>%  
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) + 
  coord_equal()

#Get area under curve
roc_auc(predictions_prob, truth = outcome, .pred_FP) 

# Check for disagreements within mock
test <- predictions_glm %>%
  dplyr::select(.pred_class,abundance, abundance_clr, sample_id,  otu, outcome)  %>%
  mutate(diff = !.pred_class == outcome)

# Filter predicted switching
keeptab <- predictions_glm %>%
  dplyr::select(.pred_class, sample_id, otu) %>%
  mutate(.pred_class = case_when(
    .pred_class == "TP" ~ 1,
    .pred_class == "FP" ~ 0
  )) %>%
  #mutate(across(where(is.factor), ~ as.character(.x))) %>%
  pivot_wider(names_from = otu,
              values_from = .pred_class,
              values_fill = 0) %>%
  dplyr::slice(match(rownames(otu_table(ps_model)), sample_id)) %>%
  column_to_rownames("sample_id") %>%
  dplyr::select(colnames(otu_table(ps_model))) %>%
  as.matrix()

#get current OTU table
otutab <- otu_table(ps_model) %>%
  as("matrix") 

#Replace taxa predicted as False positives
ps2 <- ps_model
otu_table(ps2) <- otu_table(otutab * keeptab, taxa_are_rows = FALSE)

# Check staxa that were dropped
dropped <- taxa_sums(ps2) %>%
  as_tibble(rownames = "OTU") %>%
  left_join(taxa_sums(ps1) %>%
  as_tibble(rownames = "OTU") %>%
    dplyr::rename(pre_filt = value)) %>%
  filter(value == 0) 

# Remove zero taxa
ps2 <- filter_taxa(ps2, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
message(nsamples(ps_model) - nsamples(ps2), " Samples and ", ntaxa(ps_model) - ntaxa(ps2), " taxa under read threshold Dropped")
```


## Write out final phyloseq
```{r save final ps}
saveRDS(ps2, "output/rds/ps_filtered.rds")
```

# Reproducability Receipt

```{details, echo = FALSE, details.summary = 'Reproducability receipt'}
# datetime
Sys.time()
#repository
git2r::repository()
sessioninfo::session_info()
```
