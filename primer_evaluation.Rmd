---
title: "primer_evaluation"
author: "Alexander Piper"
date: "09/08/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Introduction


## Load packages
```{r setup}
#Set required packages
.cran_packages <- c("usethis",
                    "tidyverse",
                    "spider", 
                    "insect",
                    "ape",
                    "RColorBrewer",
                    "seqinr",
                    "patchwork",
                    "ShortRead",
                    "foreach",
                    "doParallel",
                    "TmCalculator",
                    "castor",
                    "furrr",
                    "tictoc")

.bioc_packages <- c("DECIPHER",
                    "ggtree",
                    "Biostrings")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

# Github packages
devtools::install_github("alexpiper/taxreturn")
library(taxreturn)
library(PrimerMiner)

# SOurce internal functions
source("R/helper_functions.R")

```

# Assemble pest list

* EPPO global database https://gd.eppo.int/ - DONE
* US APHIS - https://www.aphis.usda.gov/aphis/home/ - DONE
* QBank - https://qbank.eppo.int/arthropods/organisms - DONE
* Global invasive species database - http://www.iucngisd.org/gisd/search.php - DONE
* Global register of introduced or invasive species http://www.griis.org/ - DONE
* VectorBase: https://www.vectorbase.org/organisms - DONE
* DAWR top 40 - http://www.agriculture.gov.au/pests-diseases-weeds/plant - DONE
* PHA National biosecurity status report -  http://www.planthealthaustralia.com.au/national-programs/national-plant-biosecurity-status-report/ - DONE
* Ashfaq & Herbert 2016 - DNA barcodes for bio-surveillance: regulated and economically important arthropod plant pests - DONE
* CABI - https://t.co/LGjlFoOazd - DONE
* http://www.europe-aliens.org - DONE

```{r Curate pest lists}
dat <- list.files("primer_evaluation/pestlist/", pattern = ".csv", full.names = TRUE) %>%
  purrr::set_names() %>%
  map_dfr(read_csv, .id = "Source", col_types = cols("Species" = col_character())) %>%
  mutate(Source = str_remove(basename(Source), pattern="\\.csv")) %>%
  mutate(Species = Species %>%  
           str_remove_all("Ã¿") %>% #Resolve weird characters
           iconv(from = 'UTF-8', to = 'ASCII//TRANSLIT')%>% 
           str_remove_all("\\?") %>%
           str_remove_all("\\((.*?)\\)") %>% # remove things between brackets ie: Hygromia (Hygromia) cinctella
           str_squish() # remove excess whitespace
         ) %>%
  filter(str_count(Species, " ") > 0 ) %>% #Remove non-binomial 
  separate(Species, into=c("Genus", "Species"), sep=" ", extra="merge") %>% # Fix duplicated genus names
  mutate(Species = str_remove(Species, pattern=Genus) %>% str_squish()) %>%
  unite(col=Species, Genus, Species, sep = " ") %>%
  unique()

# Map to OTT taxonomy ids
taxreturn::download_ott_taxonomy()
dat_resolved <- dat %>% mutate(Species = map_to_ott(Species, dir="ott3.2", from="ncbi", resolve_synonyms=TRUE, filter_bads=TRUE, remove_na = TRUE, quiet=FALSE)) %>%
  filter(!is.na(Species))

lineage <- get_ott_lineage(dat_resolved$Species, dir="ott3.2")  %>%
  bind_cols(dat_resolved) %>% 
  select(-Species)%>%
  rename_all(funs(str_to_sentence(.))) %>%
  filter(Class=="Insecta") %>%
  drop_na()

#Write out final list of pests 
write_csv(lineage, "primer_evaluation/pestlist.csv")
```

## Figure 1

```{r Figure 1}
lineage <- read_csv(
  "primer_evaluation/pestlist.csv",
  col_types  = cols(
  Acc = col_character(),
  Kingdom = col_character(),
  Phylum = col_character(),
  Class = col_character(),
  Order = col_character(),
  Family = col_character(),
  Genus = col_character(),
  Species = col_character(),
  Source = col_character()
)) %>%
  unique()

# Fig 1a - Upset plot of species share between the different databases

sources <- as.character(unique(lineage$Source))
upsetlist <- list()
for (i in 1:length(sources)){
  upsetlist[[i]]= lineage$Species[which(lineage$Source==sources[i])] 
  names(upsetlist)[[i]] <- sources[i]
}

Fig1a <- upset(fromList(upsetlist),nsets=length(upsetlist), order.by = "freq")
print(Fig1a)

## Figure 1b - summary of families within the dataset 

library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(10)

Fig1b <- group_by(lineage, Order, Source) %>%
  summarise(Genus = n_distinct(Genus), Species = n_distinct(Species)) %>%
  ungroup() %>%
  mutate(Order = fct_reorder(Order, -Species)) %>% 
  ggplot(aes(x = Order, y = Species, fill = Source)) +
  geom_bar(stat = "identity")  +
  theme_pubr() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,vjust=0)) +
  scale_fill_manual(values=col) +
  ylim(-500,2500) +
  theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    plot.margin = unit(rep(-1,4), "cm")      # Adjust the margin to make in sort labels are not truncated!
  ) +
  coord_polar(start=0) +
  geom_text(data = . %>%
              dplyr::select(Order, Species) %>%
              group_by(Order) %>%
              summarise(value = sum(Species)),
            aes(x=Order, y=value + 20, label=Order),
            color="black", fontface="bold", alpha=0.6, inherit.aes = FALSE)

# Summaries for article text

# Unique taxa
lineage %>% 
  #select(-Source, Acc) %>%
  summarise(Species = n_distinct(Species),
            Genus = n_distinct(Genus),
            Family = n_distinct(Family),
            Order = n_distinct(Order),
            )

# Sum of reference DB's
lineage %>% 
  group_by(Source) %>%
  summarise(Species = n_distinct(Species)) %>%
  arrange(Species)

# Proportion of sequences unique
lineage %>%
  add_count(Source, name = "DB_total") %>%
  group_by(Species) %>%
  add_tally(name = "n_occurances") %>%
  ungroup() %>%
  filter(n_occurances==1) %>%
  group_by(Source, DB_total)%>%
  summarise(n = n_distinct(Species)) %>%
  mutate(freq = n / DB_total)%>%
  arrange(freq)
```

# Primer info

```{r primer binding and constraints}
#alignment was then manually curated in geneious primer
folmer_curated <-  ape::read.dna("reference/Folmer_insecta_fullength_withprime_curated.fa",format="fasta")
model <- aphid::derivePHMM(folmer_curated)

# Be worth validating how the score is calculated?
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  select(name, strand, seq, citation, issues) %>%
  left_join(.$seq %>% purrr::map(get_binding_position, model, tryrc = TRUE, minscore=8) %>%
  bind_rows() %>%
  rename(seq = primer), by="seq") %>%
  unique()

primers <- primers %>%
  left_join(.$seq %>%
  purrr::map_df(get_primer_statistics, metrics="all", disambiguate=TRUE))

write_csv(primers, "primer_evaluation/primer_candidates.csv")
```

## Primer Presence in seqs

```{r check pres}
primerHits <- function(primer, fn, max.mismatch=0, with.indels=FALSE) {
      if(stringr::str_detect(primer, "I")) {
        message(paste0("Warning: Inosine (I) bases detected in primer ", primer," these will be converted to N!"))
        primer <- primer %>% str_replace_all("I", "N")
        }
    # Counts number of sequences in which the primer is found
    nhits <- vcountPattern(primer, sread(readFasta(fn)), max.mismatch=max.mismatch, fixed = FALSE, with.indels = with.indels)
    return(sum(nhits > 0))
}

primers <- read_csv("primer_evaluation/primer_candidates.csv") 

fn="reference/merged_final.fa.gz"

out <- vector("list", length=nrow(primers))
for (i in 1:nrow(primers)){
  
  if(primers$strand[i] == "F"){
    query <- primers$seq[i]
    
  } else  if(primers$strand[i] == "R"){
    query <- rc(primers$seq[i])
  }
  print(i)
  df <- tibble(
    name = primers$name[i],
    primer = query,
    strand = primers$strand[i],
    #Hamming distance (no indels in COI)
    h0 = primerHits(query, fn, max.mismatch=0, with.indels=FALSE),
    h1 = primerHits(query, fn, max.mismatch=1, with.indels=FALSE),
    h2 = primerHits(query, fn, max.mismatch=2, with.indels=FALSE),
  )
  out[[i]] <- df
}

names(out) <- primers$seq
out <- bind_rows(out)
write_csv(out, "primer_evaluation/primer_presence.csv")

# Plotting
out <- read_csv("primer_evaluation/primer_presence.csv") %>% 
  pivot_longer(cols=starts_with(c("h", "l")),
               names_to = "measure",
               values_to = "seqs"
               ) %>%
  filter(measure=="h2") %>%
  mutate(name = factor(name)) %>%
  arrange(name)

gg.primerpresF <- out %>%
  left_join(primers) %>%
  filter(strand=="F") %>%
    mutate(name = fct_reorder(name, seqs, .desc = TRUE)) %>%
  ggplot(aes(x=name, y=seqs, fill=seqs)) + 
  geom_bar(stat="identity", position="dodge") +
  coord_flip() + 
  labs(x="Forward Primers", y=NULL)  +
  theme_classic()+
  scale_fill_gradient(low = "firebrick", high = "darkslateblue", 
                      na.value = "grey", oob = scales::squish) +
  theme(legend.position = "none",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

gg.primerpresR <- out  %>%
  left_join(primers) %>%
  filter(strand=="R") %>%
    mutate(name = fct_reorder(name, seqs, .desc = TRUE)) %>%
  ggplot(aes(x=name, y=seqs, fill=seqs)) + 
  geom_bar(stat="identity", position="dodge") +
  coord_flip() + 
  labs(x="Reverse Primers", y="# Sequences containing primers") +
  theme_classic()+
  scale_fill_gradient(low = "firebrick", high = "darkslateblue", 
                      na.value = "grey", oob = scales::squish) +
  theme(legend.position = "none",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

gg.primerpres <- gg.primerpresF / gg.primerpresR

gg.primerpres

pdf(file="fig/primerpres.pdf", width = 8, height = 11 , paper="a4")
  plot(gg.primerpres)
try(dev.off(), silent=TRUE)
```

# Primer placement

## Whole alignment entropy
```{r Align}
# Get whole alignment entropy
lengthfilt <- insect::readFASTA("reference/merged_final_aligned.fa.gz")

ent <- taxreturn::alignment_entropy(lengthfilt, maskgaps=1, countgaps=FALSE, method="ML", unit="log")
values <- as.data.frame(ent, stringsAsFactors=FALSE) %>%
  rownames_to_column("pos") %>%
  magrittr::set_colnames(c("pos", "value"))

write_csv(values, "primer_evaluation/whole_alignment_entropy.csv")
```

## Entropy by order
```{r subsetting}
## Read in aligned & Subset to only fullength
seqs <- insect::readFASTA("reference/merged_final_aligned.fa.gz")
seqs <- seqs[lengths(seqs)==712]

# querylevel
queryrank <- "order"

# Get unique querieranks
queries <- names(seqs)  %>% 
  str_split_fixed(";", n = 8) %>% 
  as_tibble() %>% 
  magrittr::set_colnames(c("acc", "kingdom", "phylum",
                           "class", "order", "family", 
                           "genus", "species")) %>%
  filter(class=="Insecta") %>%
  pull(queryrank) %>%
  unique()

#Make lists to store everything
entlist <- vector("list", length=length(queries))
names(entlist) <- queries

for (i in 1:length(queries)){
  print(i)
  query <- queries[i]
 print(query)
  names <- names(seqs)  %>% 
  str_split_fixed(";", n = 8) %>% 
  as_tibble() %>% 
  magrittr::set_colnames(c("acc", "kingdom", "phylum",
                           "class", "order", "family", 
                           "genus", "species")) %>%
    filter_at(vars(queryrank), any_vars(.== query)) %>%
    unite(names,everything(),sep=";")
  
  subset <- seqs[names(seqs) %in% names$names]
  if (length(subset) > 0){
    nseqs <- length(subset)
    message(nseqs, " sequences for ", query)
    
    # Get entropies
    ent <- taxreturn::alignment_entropy(as.list(subset), maskgaps=1, countgaps=FALSE, 
                                        method="ML", unit="log")
    entlist[[i]] <- as.data.frame(ent, stringsAsFactors=FALSE) %>%
      rownames_to_column("pos") %>%
      magrittr::set_colnames(c("pos", "value")) %>%
      mutate(nseqs = nseqs)
      }
}

ent_out <- bind_rows(entlist, .id="names")
write_csv(ent_out,paste0("primer_evaluation/", queryrank,"_ent_out.csv"))

```

## Figure 2
```{r entropy}
# Read in results
values <- read_csv("primer_evaluation/order_ent_out.csv") %>% 
  filter(nseqs > 20) # Filter to only those above 20 seqs

#Set moving average function - Adjust smoothing (n=5?)
ma <- function(x, n=3){stats::filter(x, rep(1/n, n), sides=2)}

ent <- values %>%
  mutate(value = value %>% 
           na_if("") %>%
           replace_na(0)) %>%
  mutate(ma = ma(value, n = 3)) %>%
  mutate(annot = case_when(
    pos %in% seq(from=1, to=2, by=1) ~ "Loop 0",
    pos %in% seq(from=3, to=78, by=1) ~ "Helix 1",
    pos %in% seq(from=79, to=103, by=1)~ "Loop 1-2",
    pos %in% seq(from=104, to=211, by=1)~ "Helix 2",    
    pos %in% seq(from=212, to=235, by=1)~ "Loop 2-3",   
    pos %in% seq(from=213, to=304, by=1)~ "Helix 3", 
    pos %in% seq(from=305, to=373, by=1)~ "Loop 3-4", 
    pos %in% seq(from=374, to=466, by=1)~ "Helix 4", 
    pos %in% seq(from=467, to=499, by=1)~ "Loop 4-5", 
    pos %in% seq(from=500, to=598, by=1)~ "Helix 5", 
    pos %in% seq(from=599, to=634, by=1)~ "Loop 5-6", 
    pos %in% seq(from=635, to=712, by=1)~ "Helix 6", 
  )) %>%
  mutate(structure = case_when(
    str_detect(annot, "Helix") ~ "Helix",
    str_detect(annot, "Loop") ~ "Loop",
  ))  

## plot entropy by order
gg.line <- ent %>%
  group_by(pos) %>%
  mutate(median = median(ma)) %>%
  ggplot(aes(x = pos, y=ma, colour=names)) + 
  geom_line(aes(x = pos, y=ma),size=1, inherit.aes = FALSE) +
  facet_wrap(~names)

# Plot entropy of COI Gene
gg.box <- ent %>%
  group_by(pos) %>%
  mutate(median = median(ma)) %>%
  ggplot(aes(x = pos, y=ma, group=pos, colour=structure)) + 
  geom_boxplot(outlier.shape = NA, alpha=0.8) +
  geom_line(aes(x = pos, y=median),size=1, inherit.aes = FALSE) + #, colour="black"
  #?geom_tufteboxplot(median.type = "point", whisker.type = "line", hoffset = 0) +
  theme_classic() +    
  theme(legend.position = "bottom") +
  ylab("Entropy") +
  xlab("Position within COI folmer region") +
  scale_color_manual(values=c("#ca0020", "#0571b0")) +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0)) 

# Sliding window of entropy
sw <- function(x, width, interval = 1){
  win <- seq(1,  length(x) - width, by = interval) #Get all possible windows
  out <- vector("numeric", length=length(x))
  for(i in 1:length(win)){
  out[[i]] <- sum(x[win[i]:(win[i] + width)])
  }
  out[out==0] <- NA
  return(out)
}

ent_sw <- values %>%
  group_by(names) %>%
  mutate(sw220 = sw(value, width=220, interval = 1)/220) %>%
  mutate(sw420 = sw(value, width=420, interval = 1)/420) %>%
  mutate(sw20 = sw(value, width=20, interval = 1)/20) %>% #For primers
  ungroup()%>%
  pivot_longer(starts_with("sw"),
               names_to = "windowsize",
               values_to = "sw") 

gg.density <-  ent_sw %>%
  dplyr::rename(Order = names) %>%
  mutate(Order = case_when(
     Order %in% c("Coleoptera", "Diptera", "Hemiptera", "Hymenoptera", "Lepidoptera") ~ Order,
    !Order %in% c("Coleoptera", "Diptera", "Hemiptera", "Hymenoptera", "Lepidoptera") ~ "Other"
    )) %>%
  ggplot(aes(x = pos, y=1)) +
    geom_tile(aes(fill=sw))+
    scale_fill_viridis_c(option="plasma") + 
  facet_wrap(windowsize~Order, ncol=1, strip.position ="right") +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0))  +
  theme_void() +
  theme(legend.position = "bottom",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())


## Primers
primers <- read_csv("primer_evaluation/primer_candidates.csv")

Fprimerpos <- seq(1,nrow(primers %>% filter(strand=="F")),1)/1000
Rprimerpos <- seq(1,nrow(primers %>% filter(strand=="R")),1)/1000

gg.primers <- ggplot(data=values[values$pos==1,], aes(x = as.numeric(pos))) +
  geom_segment(data = primers %>% filter(strand=="F"),
               aes(x = start, xend = end,
                   y = Fprimerpos, yend = Fprimerpos,
                   colour=strand), size = 1, arrow = arrow(length = unit(0.3,"cm")),
               show.legend = FALSE) +
  geom_text(data = primers %>% filter(strand=="F"),
            aes(x = start, y = Fprimerpos,
                label = name , colour=strand),
            hjust = 1, show.legend = FALSE) +
  geom_segment(data = primers %>% filter(strand=="R"),
               aes(x = end, xend = start,
                   y = Rprimerpos, yend = Rprimerpos,
                   colour=strand), size = 1,
               arrow = arrow(length = unit(0.3,"cm")),
               show.legend = FALSE) +
  geom_text(data = primers %>% filter(strand=="R"),
            aes(x = end, y = Rprimerpos,
                label = name, colour=strand),
            hjust = 0, show.legend = FALSE)  +
  scale_x_continuous(limits = c(0, 712), breaks=seq(0,700,50), expand=c(0,0)) +
  theme_classic()  +
  theme(legend.position = "none",
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank())

Fig2 <- gg.box / gg.density / gg.primers 
```

# Predicted mismatch

```{r Primerminer, message=FALSE}
library(PrimerMiner)
## Target sequences to test against
seqs <- insect::readFASTA("reference/merged_final_aligned.fa.gz")
names(seqs) <- names(seqs) %>% str_replace_all(" ", "_")
writeFASTA(seqs, "reference/merged_final_aligned_target.fa.gz")

target <- "reference/merged_final_aligned_target.fa.gz"

# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues)

dir.create("primer_evaluation/PrimerMiner")

for (i in 1:nrow(primers)) {
  if(primers$strand[i]=="F"){
  evaluate_primer(target,
   as.character(primers$seq[i]), primers$start[i], primers$end[i],
   forward = TRUE, gap_NA = TRUE, N_NA=TRUE,
   mm_position = "Position_v1", mm_type = "Type_v1", adjacent = 2,
   save = paste0("primer_evaluation/PrimerMiner/", primers$name[i],".csv")
  )
  } else if(primers$strand[i]=="R"){
  evaluate_primer(target,
   as.character(primers$seq[i]), primers$start[i], primers$end[i],
   forward = FALSE, gap_NA = TRUE, N_NA=TRUE,
   mm_position = "Position_v1", mm_type = "Type_v1", adjacent = 2,
   save = paste0("primer_evaluation/PrimerMiner/", primers$name[i],".csv")
  )
 }
}

```

## Figure 3

```{r figure 4}
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues)

#Read in all files
files <- sort(list.files(list.dirs("primer_evaluation/PrimerMiner/"), pattern = ".csv", full.names = TRUE))

dat <- files %>% 
  purrr::set_names() %>%
  map_dfr(vroom::vroom, col_select= list("Template", "sequ", "sum"),
          .id = "source", progress=FALSE) %>%
  mutate(Template = str_remove(Template, ";$"))%>%
  separate(col=Template, into=c("Acc","Kingdom","Phylum","Class","Order","Family","Genus","Species"), sep=";") %>%
  dplyr::rename(Sequence = sequ) %>% 
  dplyr::mutate(primer = str_remove(basename(source), ".csv")) 

# set summary level
#sumlevel <- "Family"

summaries <- dat %>%
  tidyr::separate(Acc, into=c("Acc", "tax_id"), sep="\\|") %>%
  group_by(primer, Genus, tax_id) %>% #Group by higher to minimize plotting
  summarise(sum = mean(sum)) %>%
  ungroup()%>%
  mutate(dup = paste0(primer, Genus)) %>% 
  filter(!duplicated(dup)) %>% #remove any duplicates
  dplyr::select(-dup) %>%
  mutate(dir = case_when(
    primer %in% unique(primers %>% filter(strand=="F") %>% pull(name)) ~ "Forward",
    primer %in% unique(primers %>% filter(strand=="R") %>% pull(name)) ~ "Reverse"
    ))  

# Arrange on tree
tree <- rncl::read_newick_phylo("reference/ultrametric_insecta_tree.nwk")

# Prune tree to genus
tips_to_keep <- dat %>%
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, Species) %>%
  dplyr::filter(!duplicated(Species)) %>%
  dplyr::filter(Class=="Insecta") %>%
  filter(Genus %in% (summaries %>% pull(Genus))) %>%
  group_by(Genus) %>%
  dplyr::slice(1) %>%
  ungroup() %>%
  mutate(name_check = case_when(
    str_extract(Species, "^.+?(?=_)") == Genus ~ TRUE,
    !str_extract(Species, "^.+?(?=_)") == Genus ~ FALSE
  )) %>%
  filter(name_check) %>%
  dplyr::select(-name_check)

tree  <- castor::get_subtree_with_tips(tree,
                                          omit_tips=setdiff(tree$tip.label, tips_to_keep$Species),
                                          collapse_monofurcations=TRUE,
                                          force_keep_root=TRUE)$subtree

#Prune names to genus
tree$tip.label <- tree$tip.label %>% str_extract("^.+?(?=_)")

Ntips 	<- length(tree$tip.label)
Nnodes 	<- tree$Nnode
cat(sprintf("Tree has %d nodes, %d tips and %d edges\n",Nnodes,Ntips,nrow(tree$edge)));

# create internal node labels
tree$node.label <- NA
if(is.na(tree$node.label)){
	cat(sprintf("Adding node labels to full tree..\n"))
	tree$node.label = paste("node.", 1:Nnodes, sep = "") # don't use underscores, because some tree readers (e.g. rncl) interpret them as spaces
}

# replace zero-length edges
if(any(tree$edge.length==0)){
  epsilon = 0.1*min(tree$edge.length[tree$edge.length>0])
	cat(sprintf("Note: Some edges have length zero, which may break some of the HSP routines. Replacing zero-lengths with a tiny positive length (%g)..\n",epsilon))
	tree$edge.length[tree$edge.length==0] = epsilon
}


## Get values for higher nodes with castor
uprimers <- unique(summaries$primer)
p_weights <- vector("list", length=length(uprimers))
for (i in 1:length(uprimers)){
  print(paste0("Processing Primer ",i, " of ", length(uprimers), ": ", uprimers[i]))
  tip_states <- summaries %>%
    dplyr::filter(primer==uprimers[i], Genus %in% tips_to_keep$Genus) %>%
    group_by(Genus) %>%
    summarise(values = mean(sum, na.rm=TRUE))%>%
    ungroup() %>%
    column_to_rownames("Genus")
    
  row2tip <- match(rownames(tip_states), tree$tip.label)
  tip_states <- tip_states[!is.na(row2tip),,drop = FALSE]
  hsp_states <- castor::hsp_independent_contrasts(tree = tree,
                                              tip_states = tip_states$values,
                                              weighted = TRUE,
                                              check_input = TRUE)$states
  
   p_weights[[i]] <- tip_states %>%
    rownames_to_column("Genus") %>%
    mutate(hsp = hsp_states[1:Ntips])
}
names(p_weights) <- uprimers

imputed <- summaries %>%
    left_join(bind_rows(p_weights,.id = "primer"), 
               by = c("primer", "Genus")) %>%
  filter(!is.na(hsp))

primer_orders <- imputed %>%
  group_by(primer) %>%
  dplyr::summarise(hsp = mean(hsp, na.rm=TRUE), sum = mean(sum, na.rm=TRUE)) %>%
  arrange(hsp)

# LAbel branches
tax_groups <- tips_to_keep %>%
dplyr::rename(label = Genus) %>%
select(label, Order) %>%
filter(label %in% tree$tip.label) %>%
group_by(Order)

group_name <- group_keys(tax_groups)  %>%
mutate(group_name = Order %>% str_remove_all("\\[|\\]"))

cls <- tax_groups %>%
group_split() %>%
purrr::map(pull, label) %>%
set_names(group_name$group_name)

tree2 <- groupOTU(tree, cls)

# Plot tree
p1 <- ggtree(tree2, ladderize=TRUE, aes(colour=values)) +
  geom_text2(aes(subset=!isTip, label=group %>% na_if(0)), hjust=0, check_overlap=TRUE)

weights_p1 <- p1$data %>%
  left_join(imputed %>%
              group_by(Genus) %>%
              summarise(values = mean(sum, na.rm=TRUE))%>%
              dplyr::rename(label = Genus)
    )  


tip_states <- weights_p1 %>%
  dplyr::filter(isTip) %>%
  pull(values)
names(tip_states) <- weights_p1 %>%
  dplyr::filter(isTip) %>%
  pull(label)

weights_p1$values[!weights_p1$isTip] <- castor::asr_independent_contrasts(tree=tree,
                                                        tip_states=tip_states)$ancestral_states

p2 <- p1 %<+% weights_p1 + geom_tippoint(aes(colour=values)) +
  scale_color_gradient(low="darkslateblue", high="firebrick") +
  theme(legend.position = "none") + 
  scale_y_continuous(expand=c(0,0))+ 
  scale_x_discrete(expand=c(0,0)) 

## make a clade label list
#tax_groups <- tips_to_keep %>%
#  dplyr::rename(label = Genus) %>%
#  select(label, Order) %>%
#  filter(label %in% tree$tip.label) %>%
#  group_by(Order) 
#
#group_name <- group_keys(tax_groups)  %>%
#  mutate(group_name = Order %>% str_remove_all("\\[|\\]"))
#
#cls <- tax_groups %>% 
#  group_split() %>% 
#  purrr::map(pull, label) %>%
#  set_names(group_name$group_name) %>%
#  purrr::map(get_mrca_of_set, tree=tree)
#
#p3 <- p2
#for(i in 1:length(cls)){
#p3 <- p3 + geom_cladelabel(node=cls[[i]], label=names(cls[i]), align=T, angle=270, hjust='center', #offset.text=.5, barsize=1.5)
#}
# Plot heatmap
gg.mismatch <- imputed %>%
  left_join(p2$data %>% dplyr::select(label, y) %>% dplyr::rename(Genus = label)) %>%
  mutate( primer = factor(primer, levels=primer_orders$primer)) %>%     
  ggplot(aes(x = primer, y = y, fill = hsp)) +
    geom_raster() +
    #scale_fill_distiller(palette = "Spectral", limits = c(0, 200)) +
    scale_fill_gradient(low = "darkslateblue", high = "firebrick", na.value = "grey", limits = c(0, 200), oob = scales::squish) +
    facet_grid(~dir, scales="free", space="free", drop=TRUE) +
    #theme_classic() + 
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank(),
      strip.text.y = element_text(angle = 0),
      #strip.background = element_rect(fill="grey10"),
      legend.position = "none"
    ) + 
  scale_y_continuous(expand=c(0,0))+ 
  scale_x_discrete(expand=c(0,0))

# Density plot of mismatch
gg.density <- imputed %>%
  left_join(p2$data %>% dplyr::select(label, y) %>% dplyr::rename(Genus = label)) %>%
  group_by(Genus, y) %>%
  summarise(values = mean(sum, na.rm=TRUE)) %>%
  ggplot(aes(x = y, y=values, fill=values, colour=values)) +
  geom_point(size=0.01, alpha=0.5)+
  #geom_smooth(span = 0.1)+
  scale_colour_gradient(low="darkslateblue", high="firebrick") + 
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  theme_void() +
  theme(legend.position = "none")+
  coord_flip()

#Fig3 <- p2 + gg.mismatch + plot_layout(widths = c(1,2))

Fig3 <- p2 + gg.mismatch + gg.density + plot_layout(widths = c(1,3,0.25))

pdf(file="fig/fig3.pdf", width = 8, height = 11 , paper="a4")
  plot(Fig3)
try(dev.off(), silent=TRUE)
 

  
#- Add a summary metric for mean mismatch for each primer
#- Add a summary metric for evennes of mismatch for each primer
#- Add a summary metric for amount of missing data imputed. Is there a way to highlight what data was imputed or not?
#- Add a metric for amount of degeneracy in each primer
```


# Identification sucess

In order to evaluate the taxonomic resolution of different barcode regions, alignments were trimmed to primer regions using the in silico PCR function from the insect package and  the identificaiton sucess functions from the SPIDER r package were used

Metrics to evaluate primers on:
 *  OTU clustering of each primer at different ranks, see drop off
 *  OTU clustering, subset to pests only (ie fraction of pests in their own cluster)
 *  Datasets subset to pest genera (or families), evaluated with spider nearNeighbour, BestCloseMatch, ThreshID, Monophyly
 * Leave one out BLAST/RDP?
 * P(LCR) for each primer?
 
 
## Virtual PCR

```{r Virtual PCR}
# Load seqs
seqs <- readDNAStringSet("reference/merged_final_aligned.fa.gz")
seqs <- seqs[lengths(seqs)==712]

# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  select(-score, -issues)

# Get all possible primer combinations
combos <- expand_grid(primers %>% filter(strand=="F") %>% pull(seq),
                      primers %>% filter(strand=="R") %>% pull(seq)
                      ) %>%
  mutate_if(is.factor, as.character)%>%
  unique() %>%
  magrittr::set_colnames(c("Fseq", "Rseq")) %>%
  left_join(primers %>% filter(strand=="F") %>% rename_all(. %>% paste0("F",.)), by="Fseq") %>%
  left_join(primers %>% filter(strand=="R") %>% rename_all(. %>% paste0("R",.)), by="Rseq") %>%
  mutate(amplicon = Rstart - Fend) %>%
  filter( amplicon > 100)

dir.create("primer_evaluation/amplicons")


# Setup multithreading
library(foreach)
library(doParallel)
cores=12
cl <- parallel::makeCluster(cores)
registerDoParallel(cl)

li <- foreach(p=1:nrow(combos)) %dopar% {
  #get primer names
  primernames <- paste0(combos$Fname[p], "_",  combos$Rname[p])
  
  # cut down alignments
  amplicon <- Biostrings::subseq(seqs, start=combos$Fend[p]+1, end = combos$Rstart[p]) #may need to add 1 to start
  
  if (any(!lengths(amplicon)== combos$amplicon[p])){
    warning(paste0("Amplicons of ",primernames, " are not the same length"))
  }
  
  maxgaps <- 9 # dont allow any more than 9 gaps
  rem <- names(amplicon)[Biostrings::letterFrequency(amplicon, "-") > maxgaps]
  amplicon <- amplicon[!names(amplicon) %in% rem]
  message(paste0(length(rem), " Sequences with more than ", maxgaps, " gaps removed from ", primernames))
  
  #write out sequences
  Biostrings::writeXStringSet(amplicon, file=paste0("primer_evaluation/amplicons/", primernames,".fa.gz"), compress=TRUE)
}
#close cluster
parallel::stopCluster(cl)

```
 
## Clustering ID Success


```{r clustering.R}
#load packages ---------------------------------------------------
.cran_packages <- c("tidyverse",
                    "spider", 
                    "insect",
                    "ape",
                    "seqinr",
                    "ShortRead",
                    "foreach",
                    "doParallel")

.bioc_packages <- c("DECIPHER", "Biostrings")

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE, warn.conflicts=FALSE)

# Github packages
library(taxreturn)

# get job name ---------------------------------------------------
args = commandArgs(trailingOnly=TRUE)

if (length(args)==0) {
  stop("At least one argument must be supplied (input file) \n", call.=FALSE)
}
print(args)
file <- args[1]
job_name <- basename(file) %>% str_remove("\\..*$")
message("file = ", file)
message("job_name = ", job_name)

message("wd = ", getwd())
message("files in wd: \n")
list.files()

# Run code ---------------------------------------------------

# Read in amplicon
amplicon <- insect::readFASTA(file)
print(amplicon)

# Testing
amplicon <- amplicon[1:1000]

#Get db
db <- taxreturn::get_ott_taxonomy(dir="ott3.2")
# Set random seed for kmeans initialisation 
set.seed(606)

## Get distance clustering statistic
thresholds <- rev(seq(0.97, 1, 0.01))
threshlist <- vector("list", length=length(thresholds))

for (i in 1:length(thresholds)){
threshlist[[i]] <- taxreturn::get_mixed_clusters( 
 x = amplicon, db=db,
 rank = "species",
 threshold = thresholds[i],
 rngseed = 606,
 return = "all",
 confidence= 0, quiet = FALSE) 
}
names(threshlist) <- thresholds 
dplyr::bind_rows(threshlist) %>%
  write_csv(paste0(job_name,"_clustering.csv"))
```


## Spider ID Sucess
```{r Spider}
# install and load packages ---------------------------------------------------
.cran_packages <- c("tidyverse", "spider", "insect", "ape",
                    "seqinr", "ShortRead", "foreach", "doParallel")

.bioc_packages <- c("DECIPHER", "Biostrings")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

# Github packages
devtools::install_github("alexpiper/taxreturn")
library(taxreturn)


# get job name ---------------------------------------------------
args = commandArgs(trailingOnly=TRUE)

if (length(args)==0) {
  stop("At least one argument must be supplied (input file).n", call.=FALSE)
}
print(args)
file <- args[1]
job_name <- basename(file) %>% str_remove("\\..*$")
message("file = ", file)
message("job_name = ", job_name)

# Run code ---------------------------------------------------

# Load resources
model <- readRDS("folmer_fullength_model.rds")
pestlist <- read_csv("primer_evaluation/pestlist.csv")

## Read in seqs
seqs <- insect::readFASTA(file)

#Filter to pest genera and remove singletons
names <- names(seqs)  %>% 
  str_split_fixed(";", n = 8) %>% 
  as_tibble() %>% 
  magrittr::set_colnames(c("acc", "kingdom", "phylum",
                          "class", "order", "family", 
                          "genus", "species")) %>%
  filter(genus %in% pestlist$Genus)%>%
  group_by(species) %>%
  add_tally %>%
  filter(n > 1) %>%
  select(-n) %>%
 unite(names,everything(),sep=";") #%>%
  
subset <- seqs[names(seqs) %in% names$names]
print(subset)

# Testing
subset <- subset[1:1000]

# Map to model
lengthfilt <- taxreturn::map_to_model(subset, model, minscore = 100, shave= TRUE, pad=TRUE, check_indels=TRUE, maxNs=Inf, cores=2, quiet=FALSE)

# All should be same length, if not -warn and subset to most frequent value
seqLength <- lengths(lengthfilt)
uniqx <- unique(na.omit(seqLength))
if(length(uniqx) > 1) {message("Warning, more than one length in lengthfilt")}
freqlen <- uniqx[which.max(tabulate(match(seqLength, uniqx)))]
lengthfilt <- as.matrix(lengthfilt[which(seqLength == freqlen)])

# Genus and species names
aa <- Biostrings::strsplit(dimnames(lengthfilt)[[1]], split = ";")
Genus <- sapply(aa, function(x) paste(x[7], sep = "_"))
Spp <- sapply(aa, function(x) paste(x[8], sep = "_"))

# Create distance matrix
Dist <- dist.dna(lengthfilt, pairwise.deletion = TRUE)

# Nearest neighbour metric
nn <- tibble(
  query = labels(lengthfilt),
  Spp = Spp,
  result = nearNeighbour(Dist, Spp),
  nn_spp = enframe(nearNeighbour(Dist, Spp, names = TRUE)) %>% pull(value))
  
# Closematch metric
cm <- tibble(
  query = labels(lengthfilt),
  Spp = Spp,
  result99 = bestCloseMatch(Dist, Spp, threshold = 0.01),
  result98 = bestCloseMatch(Dist, Spp, threshold = 0.02),
  result97 = bestCloseMatch(Dist, Spp, threshold = 0.03),
  result96 = bestCloseMatch(Dist, Spp, threshold = 0.04),
  result95 = bestCloseMatch(Dist, Spp, threshold = 0.05))

cm_names <- tibble(
  query = labels(lengthfilt),
  Spp = Spp,
  names99 = bestCloseMatch(Dist, Spp, names = TRUE, threshold = 0.01),
  names98 = bestCloseMatch(Dist, Spp, names = TRUE, threshold = 0.02),
  names97 = bestCloseMatch(Dist, Spp, names = TRUE, threshold = 0.03),
  names96 = bestCloseMatch(Dist, Spp, names = TRUE, threshold = 0.04),
  names95 = bestCloseMatch(Dist, Spp, names = TRUE, threshold = 0.05)) 

cm_names <- cm_names %>%
  mutate(names99 = map(names99, ~set_names(., paste0("closematch99_",seq_along(.))))) %>%
  mutate(names98 = map(names98, ~set_names(., paste0("closematch98_",seq_along(.))))) %>%
  mutate(names97 = map(names97, ~set_names(., paste0("closematch97_",seq_along(.))))) %>%
  mutate(names96 = map(names96, ~set_names(., paste0("closematch96_",seq_along(.))))) %>%
  mutate(names95 = map(names95, ~set_names(., paste0("closematch95_",seq_along(.))))) %>%     
  unnest_wider(names99) %>%     
  unnest_wider(names98) %>%     
  unnest_wider(names97) %>%     
  unnest_wider(names96) %>%     
  unnest_wider(names95)

closematch <- dplyr::left_join(cm, cm_names, by=c("query", "Spp"))

# Monophyly metric
Tr <- nj(Dist)
maxInt <- max(Tr$edge.length[Tr$edge[, 2] > length(Tr$tip.label)])
nodeRoot <- Tr$edge[which(Tr$edge.length == maxInt), 2]
TrRoot <- root(Tr, node = nodeRoot, resolve.root = TRUE)
TrRoot$tip.label <- Spp
mono <- monophyly(TrRoot, Spp, singletonsMono = TRUE)

out <-  tibble(
  primer = job_name,
  Spp = Spp,
  query = dimnames(lengthfilt)[[1]],
  mono=mono[match(Spp, unique(Spp))]
) %>%
  left_join(nn, by=c("query", "Spp")) %>%
  left_join(closematch, by=c("query", "Spp")) 

out %>% 
  write_csv(paste0(job_name,"_spider.csv"))
```

## SLURM job submit

Index jobs
```{bash generate job index}
#!/bin/bash
/usr/bin/ls -d $PWD/* | sed -e '1p' -e '/.fa.gz/!d' | sort > sequence_index.txt
```

Submit array

njobs=$(cat sequence_index.txt | wc -l)
sbatch --array=1-808 submit_clustering.slurm


```{bash job script}
#!/bin/bash
#SBATCH --job-name=id_metrics       
#SBATCH --ntasks=1 
#SBATCH --cpus-per-task=4
#SBATCH --mem=40GB
#SBATCH --time=240:00:00
#SBATCH --mail-user=alexander.piper@agriculture.vic.gov.au
#SBATCH --mail-type=ALL
#SBATCH --account=pathogens
#SBATCH --export=none


#Check if job is launched as array
if [ -z "$SLURM_ARRAY_TASK_COUNT" ]; then 
  echo SLURM_ARRAY_TASK_COUNT unset; 
  echo You must launch this job as an array
  echo see https://slurm.schedmd.com/job_array.html
  echo for info on how to run arrays
  exit 1
fi

Index=sequence_index.txt

# Make sure that sequence index file is there before we do anything
if [[ ! -f "${Index}" ]]; then
  echo "Error sequence index file ${Index} does not exist"
  exit 1
fi

#Gather info on our samples
FullSampleName=$(sed -n ${SLURM_ARRAY_TASK_ID}p ${Index})
SequencePath=$(dirname ${FullSampleName})
Sample=$(basename ${FullSampleName} .fa.gz)

echo ${FullSampleName}
echo ${Sample}

# Double check that array index is valid
if [[ ! -f "${FullSampleName}" ]]; then
  echo "Error array index doesnt match up with index file"
  echo "Array index is  ${SLURM_ARRAY_TASK_ID}"
  exit 1
fi

# Goto tmp to do our working out
echo $TMPDIR
cd $TMPDIR
tmp_dir=$(mktemp -d -t ci-XXXXXXXXXX)
cd $tmp_dir

#Copy data files, database and decompress all
cp ${FullSampleName} .
cp ${SequencePath}/clustering.R .
#cp ${SequencePath}/spider.R .
cp -r ${SequencePath}/ott3.2 .
pwd
ls

#Decompress all
pigz -p8 -d ./*.gz

#Load modules
module purge
module load R

#Run clustering R script and send sample name to it
Rscript clustering.R $Sample.fa

#Run spider R script and send sample name to it
#Rscript spider.R $Sample.fa.gz

# Output useful job stats
/usr/local/bin/showJobStats.scr | gzip > ${Sample}-jobstats.gz

#Make a directory call ${Sample} and cp all gz files into that directory
mkdir output
mkdir output/jobstats
cp ./*_clustering.csv output
cp ./*-jobstats.gz output/jobstats

#Put all output files back where we started
cp -r output ${SLURM_SUBMIT_DIR}
rm -rf $tmp_dir

```
 
 
 # Clustering - need to write out results as RDS, or can i do it in a way that doesnt require masses of NA's
 Actually all the mixed should be in the cluster ID anyway?
 # Write out summaries as CSV?
  
  Only go from 1 to 0.95 as well?
  
  This function requires a bit of a rewrite before running again
  Need an option to just return all mixed clusters, pivoting longer?
  
  Options: return = suggested, or return=all

## Figure 4

```{r figure 4}
# Summarise number of mixed clusters that contain pests at %
pestlist <- read_csv("primer_evaluation/pestlist.csv") 

# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  mutate(seq = str_replace_all(seq, "I","N")) 

# Get all possible primer combinations
combos <- expand_grid(primers %>% filter(strand=="F") %>% pull(seq),
                      primers %>% filter(strand=="R") %>% pull(seq)
                      ) %>%
  mutate_if(is.factor, as.character)%>%
  unique() %>%
  magrittr::set_colnames(c("Fseq", "Rseq")) %>%
  left_join(primers %>% filter(strand=="F") %>% rename_all(. %>% paste0("F",.)), by="Fseq") %>%
  left_join(primers %>% filter(strand=="R") %>% rename_all(. %>% paste0("R",.)), by="Rseq") %>%
  mutate(amplicon = Rstart - Fend) %>%
  filter( amplicon > 100)

# Check missing
complete <- list.files(path="primer_evaluation/amplicons/output/", pattern = "clustering.csv") %>%
  basename %>%
  str_remove("_clustering.csv")

setdiff(paste0(combos$Fname, "_",  combos$Rname), complete)

# read in mixed clusters

# need to summarise for species, and for seqs
library(furrr)
plan(multiprocess)
mixed_clusters <- fs::dir_ls(path="primer_evaluation/amplicons/output/", glob = "*clustering.csv")%>%
  furrr::future_map(function(x){
  #purrr::map(function(x){
    df <-  vroom::vroom(x, delim=",")
    all <- df %>%
      dplyr::group_by(threshold) %>% 
      dplyr::filter(!duplicated(tax_name)) %>% 
      dplyr::summarise(mixed=n()) %>%
      dplyr::mutate(type="all")
    pests <- df %>%
      dplyr::group_by(threshold) %>% 
      filter(tax_name %in% pestlist$Species) %>%
      dplyr::filter(!duplicated(tax_name)) %>% 
      dplyr::summarise(mixed=n()) %>%
      dplyr::mutate(type="pest")
    out <- bind_rows(all, pests)
    return(out)
    }) %>%  
    #},.progress = TRUE) %>%
  bind_rows(.id="source") %>%
  mutate(source = str_remove(basename(source), "_clustering.csv"))

amplified <- fs::dir_ls(path="primer_evaluation/amplicons/", glob = "*.fa.gz") %>%
   #purrr::map(function(x){
    furrr::future_map(function(x){
     df <- fasta.index(x) %>%
        mutate(species = desc %>% 
                 str_remove("(?:[^;]*;){7}") %>% #match 7th ;
                 str_remove(";$")) # match last ;
     all <- df %>%
      dplyr::group_by(filepath) %>%
      dplyr::summarise(seqs_amplified = n(), spp_amplified = n_distinct(species)) %>%
      dplyr::mutate(type="all")
     pests <- df %>%
      dplyr::group_by(filepath) %>%
      dplyr::filter(species %in% pestlist$Species) %>%
      dplyr::summarise(seqs_amplified = n(), spp_amplified = n_distinct(species)) %>%
      dplyr::mutate(type="pest")
     out <- bind_rows(all, pests)
    return(out)
   }) %>%
  bind_rows() %>%
  mutate(source = str_remove(basename(filepath), ".fa.gz")) %>%
  dplyr::select(-filepath)

joint <- mixed_clusters %>%
  left_join(amplified) %>%
  mutate(combos = source) %>% 
  tidyr::separate(source, into=c("Fname", "Rname"), sep="_", extra="merge")
vroom::vroom_write(joint, "primer_evaluation/amplicons/mixed_clusters_summary.csv", delim=",")


# Count reads
joint <- vroom::vroom("primer_evaluation/amplicons/mixed_clusters_summary.csv", delim=",") %>%
  left_join(combos) %>% 
  mutate(success =  (spp_amplified - mixed)/spp_amplified)

# Summarise identification sucess by length on 2d scatter
gg.id <- joint %>% 
  dplyr::filter(threshold %in% c(0.97, 1)) %>%
  ggplot(aes(x=amplicon, y=spp_amplified, colour=combos))+
  geom_jitter(width = 5, height=0.025)+
  #geom_smooth()+
  #geom_text(aes(label=combos), check_overlap = TRUE) +
  geom_vline(xintercept = 220) +
  geom_vline(xintercept = 420)+
  #annotate("text", x = 220, y = 1, label = "2x150 (NovaSeq)")+
  #annotate("text", x = 420, y = 1, label = "2x250 (MiSeq)")+
  #scale_color_gradient(low="firebrick", high="darkslateblue") +
  facet_wrap(threshold~type, scales="free") +
  theme(axis.text.x = element_text(angle=45, hjust=1),
        legend.position = "none") + 
  labs(x="Amplicon length", y="Proportion sucessfully identified")

gg.id

library(plotly)
ggplotly(gg.id)
```


# Off-target amplification
First identify all sequences that have homology to both forward and reverse primers, with the hit sequences placed so that they can actually form a PCR product. 
First perform a blast search for each primer individually against NT database (or can you do a kmer search with BBDUK?)
Get accession numbers, get taxonomy for accession numbers
then compare the lists for forward and reverse primers.
Any accession number that occurs in both lists needs to be investigated as a potential cross-reacting sequence
Get taxonomy for all cross reactign sequences, filter to those that are not insecta
Retrieve all sequences for cross reacting, make new database from those
Then do in-silico PCR with insect and count how many successfully amplify

```{r off target}
dir.create("primer_evaluation/off_target/individual", recursive = TRUE)
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  dplyr::filter(!duplicated(name)) %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  mutate(seq =case_when(
    strand=="F" ~ seq,
    strand=="R" ~ rc(seq) #Works without RC but RC seems faster
  )) 

# Disambiguate primers and write out individuals
seqs <- DECIPHER::Disambiguate(DNAStringSet(primers$seq))
names(seqs) <- primers$name

for (i in 1:length(seqs)){
  out <- unlist(seqs[i])
  #Sample 100 random permutations for degenerate primers
  out <- sample(out, 100, replace = TRUE) %>%
    unique()
  names(out) <- make.unique(names(out), sep="_")
  writeXStringSet(out, paste0("primer_evaluation/off_target/individual/",names(out[1]),".fa"))
}


dir.create("primer_evaluation/off_target/joined", recursive = TRUE)
# Get all possible primer combinations
combos <- expand_grid(primers %>% filter(strand=="F") %>% pull(seq),
                      primers %>% filter(strand=="R") %>% pull(seq)
                      ) %>%
  mutate_if(is.factor, as.character)%>%
  unique() %>%
  magrittr::set_colnames(c("Fseq", "Rseq")) %>%
  left_join(primers %>% filter(strand=="F") %>% rename_all(. %>% paste0("F",.)), by="Fseq") %>%
  left_join(primers %>% filter(strand=="R") %>% rename_all(. %>% paste0("R",.)), by="Rseq") %>%
  mutate(amplicon = Rstart - Fend) %>%
  filter( amplicon > 100) %>%
  mutate(synthetic = paste0(Fseq,"-", rc(Rseq) )) 

# Write out all combos - takes a while due to extremely high degeneracy!
for(i in 1:nrow(combos)){
  print(i)
  out <- DECIPHER::Disambiguate(DNAStringSet(combos$synthetic[i]))
  names(out) <- paste0(combos$Fname[i], "_",  combos$Rname[i])
  out <- unlist(out)
  # Pad with 20 N's
  out <- DNAStringSet(sapply(out, str_replace_all, pattern= "-", replacement="NNNNNNNNNNNNNNNNNNNN"))
  
  names(out) <- make.unique(names(out), sep="_")
  writeXStringSet(out, paste0("primer_evaluation/off_target/joined/", names(out[1]),".fa"))
}

# For the combos should i just take a random sample of 1000? as this is pretty ridiculous

```

# Off target chunked

Output all primer combos, fasta files in chunks of 100
Do as a BLAST array job
pull the taxid, and query name
get only unique taxid
save that as the results
Calculate off targets as proportion of insecta to total sequences amplified

Probably dont need to do it for all combos? just do for forward and reverse. Define it as a limitation 

```{r off target}
dir.create("primer_evaluation/off_target/individual_chunked", recursive = TRUE)
# Load primers
primers <- read_csv("primer_evaluation/primer_candidates.csv") %>%
  dplyr::filter(!duplicated(name)) %>%
  mutate(seq = str_replace_all(seq, "I","N")) %>% 
  mutate(seq =case_when(
    strand=="F" ~ seq,
    strand=="R" ~ rc(seq) #Works without RC but RC seems faster
  )) 

# Disambiguate primers and write out individuals
seqs <- DECIPHER::Disambiguate(DNAStringSet(primers$seq))
names(seqs) <- primers$name

for (i in 1:length(seqs)){
  out <- unlist(seqs[i])
  names(out) <- make.unique(names(out), sep="_") 
  # Split into chunks of 100
  chunks <- split(out, ceiling(seq_along(out)/100))

  #write out chunks as seperate files
  for (c in 1:length(chunks)){
    filename <- paste0(names(seqs[i]),"_", names(chunks[c]))
    writeXStringSet(chunks[[c]],
                    paste0("primer_evaluation/off_target/individual_chunked/",filename,".fa.gz"),
                    compress = TRUE)
  }
}
```


See primerblast paper:

To evaluate specificity, artificial search sequences were generated by concatenating both primer sequences with a 20 base spacer. This ensures that each primer will be treated separately in the BLAST search and thus achieves the equivalent effect of performing a separate BLAST search for each primer. To create a database of potential non-target sequence ampliciations These artificial sequences as well as just the forward and reverse primers were searched against the local NCBI nr database using BLASTn

From primerserver code : https://github.com/billzt/PrimerServer/blob/master/script/_run_specificity_check.pl
blastn -task blastn-short -query $query_file -db $db_file -evalue 30000 "
                    ." -word_size 7 -perc_identity60 -dust no -ungapped -reward 1 -penalty -1 "
                    ." -max_hsps 500 -outfmt '6 qseqid qstart qend sseqid sstart send sstrand' "
                    ." -out $query_file.$db_name.out -num_threads $run_cpu";


Index jobs
```{bash generate job index}
#!/bin/bash
/usr/bin/ls -d $PWD/*.fa.gz | sort > sequence_index.txt
```

Submit array

njobs=$(cat sequence_index.txt | wc -l)

sbatch --array=1-930 primerblast.slurm

## BLAST
```{bash blast}
#!/bin/bash
#SBATCH --job-name=BLASTn       
#SBATCH --ntasks=1 
#SBATCH --cpus-per-task=4
#SBATCH --mem=20GB
#SBATCH --time=240:00:00
#SBATCH --mail-user=alexander.piper@agriculture.vic.gov.au
#SBATCH --mail-type=ALL
#SBATCH --account=pathogens
#SBATCH --export=none

#Check if job is launched as array
if [ -z "$SLURM_ARRAY_TASK_COUNT" ]; then 
  echo SLURM_ARRAY_TASK_COUNT unset; 
  echo You must launch this job as an array
  echo see https://slurm.schedmd.com/job_array.html
  echo for info on how to run arrays
  exit 1
fi
Index=sequence_index.txt

# Make sure that sequence index file is there before we do anything
if [[ ! -f "${Index}" ]]; then
  echo "Error sequence index file ${Index} does not exist"
  exit 1
fi

#Gather info on our samples
BlastDB=/group/blastdb/nt
FullSampleName=$(sed -n ${SLURM_ARRAY_TASK_ID}p ${Index})
SequencePath=$(dirname ${FullSampleName})
Sample=$(basename ${FullSampleName} .fa.gz)

# Double check that array index is valid
if [[ ! -f "${FullSampleName}" ]]; then
  echo "Error array index doesnt match up with index file"
  echo "Array index is  ${SLURM_ARRAY_TASK_ID}"
  exit 1
fi

# Goto tmp to do our processing
echo $TMPDIR
cd $TMPDIR
tmp_dir=$(mktemp -d -t ci-XXXXXXXXXX)
cd $tmp_dir

#Copy data files, database and decompress all
cp ${FullSampleName} .
pigz -p8 -d ./*.gz
pwd
ls

#Load modules
module purge
module load BLAST+

###START###

echo ${BlastDB}
echo ${Sample}
date

blastn -task blastn-short \
-query ${Sample}.fa \
-db  ${BlastDB} \
-out ${Sample}.out \
-evalue 30000 \
-word_size 7 \
-perc_identity 60 \
-dust no \
-ungapped \
-reward 1 \
-penalty -1 \
-max_hsps 1 \
-max_target_seqs 1000000 \
-outfmt '6 qseqid qstart qend sseqid staxid sstart send sstrand' \
-num_threads 8

pigz ${Sample}.out

# Output useful job stats
/usr/local/bin/showJobStats.scr | gzip > ${Sample}-jobstats.gz

#Make a directory call ${Sample} and cp all output files into that directory
mkdir ${Sample}_output
cp ./*.gz ${Sample}_output

date

# put all output files back where we started
cp -r ${Sample}_output ${SLURM_SUBMIT_DIR}
```

## Extract fasta of all unique BLAST hits
```{bash}
echo $() >  hits.txt
for d in ./*_output/ ; do (cd "$d" && zcat *.out.gz | awk '{print $4}' | awk '{split($0,a,"|"); print a[2]}' | sort -u >> ../hits.txt ); done
cat hits.txt | sort -u > merged_hits.txt

blastdbcmd -entry_batch merged_hits.txt -db /group/blastdb/nt -out merged_hits.fa 

```


## Extract taxid of all unique BLAST hits

This loops through all out files in the output subfolders and merges the unique tax_ids into hits

The awk call splits the primer name to remove the chunk ID (ie primer_1 > primer 1 ), then prints just the primer and tax_ids, then call unique
```{bash}
echo $() >  hits.txt
for d in ./*_output/ ; do (cd "$d" && zcat *.out.gz | awk '{split($1,a,"_"); print a[1], $5}'| sort -u >> ../hits.txt ); done
cat hits.txt | sort -u > merged_hits.txt
pigz merged_hits.txt
rm hits.txt

```


## process hits

```{r process blast taxonomy}
hits <- vroom::vroom("primer_evaluation/merged_hits.txt.gz", col_names=c("primer", "tax_id"))

db <- taxreturn::get_ncbi_lineage()

test <- hits %>% 
  left_join(db, by="tax_id") %>%
  mutate(is_arthropod = case_when(
    phylum=="Arthropoda" ~ TRUE,
    !phylum=="Arthropoda" ~ FALSE
  ), is_insect = case_when(
    class=="Insecta" ~ TRUE,
    !class=="Insecta" ~ FALSE
  )) %>%
  group_by(primer)%>%
  summarise(freq_arthropod=(sum(is_arthropod, na.rm = TRUE)/n()),
            freq_insect=(sum(is_insect, na.rm = TRUE)/n()),
            n=n()
            ) %>%
  arrange(freq_arthropod)
  


table(test$phylum)
```



# BBTOOLS CUTPRIMERS

BBTools has is a pair of programs for cutting (and outputting to a file) the sequence between primer pairs, but not for cutting out primers themselves. Is that what you are after?

The usage is like this:

Assume your set of primers for one end is AAAAAA and AAAAAT, and for the other end is GGGGGG and GGGCCC:

module load BBmap
msa.sh in=test.fa out=primer1.sam literal=ATTGGWGGWTTYGGAAAYTG replicate=t
msa.sh in=amplicons.fq out=primer2.sam literal=GGGGGG,GGGCCC



msa.sh in=merged_final.fa.gz out=primer1.sam literal=ATTGGWGGWTTYGGAAAYTG replicate=t

That will find the optimal alignment for the optimal primer, and output one line per amplicon (twice). Then run this:

cutprimers.sh in=amplicons.fq out=middle.fq sam1=primer1.sam sam2=primer2.sam

"middle.fq" will contain the sequence between the primers, one per amplicon, using the best alignment. I designed this to cut V4 regions from full-length 16s.

# PrimerTree

Note: Detailed information about installing and running PrimerTree can be found at https://github.com/jimhester/primerTree

Could use a slurm array to submit each seperately with all combinatons. 
Also worth having a function to count degeneracy for the constraints section
```{r off target}
primers <- read_csv("primer_evaluation/primer_candidates.csv")
library(primerTree)


forward <- "GGDRCWGGWTGAACWGTWTAYCCNCC"
rev <- "TATDGTRATDGCHCCNGC"

test <- search_primer_pair(
  forward,
  rev,
  api_key ="1c0a0c4afa28448650a1450662a22c68f208",
  num_permutations = 20
)
ranks = c("kingdom", "phylum", "class", 
    "order", "family", "genus", "species")

lineage <- test3[["taxonomy"]] %>%
  select(all_of(ranks))%>% 
  tidyr::unite(col = pathString, 
  !!ranks, sep = "/") %>%
  dplyr::mutate(pathString = paste0("Root/", pathString)) %>%
  data.tree::as.Node(.)

tree <- ape::read.tree(textConnection(data.tree::ToNewick(lineage, heightAttribute = NULL)))

library(ggtree)

ggtree(tree)


dir.create("PrimerTree")
#for (i in 1:nrow(dat.I)) {
  assign(paste("PT", dat.I$Name[i], sep = "."), search_primer_pair(name = dat.I$Name[i], dat.I$F.seq[i], dat.I$R.seq[i], num_permutations = 50, num_aligns = 1000))
  saveRDS(paste("PT", dat.I$Name[i], sep = "."),paste0("PrimerTree/PT.",dat.I$Name[i],".rds"))
}
#, clustal_options = c(exec='clustal-omega-1.2.2-win64/clustalo.exe')

# 3.2. - Inspect the sequence length distribution for each primer pair and remove any sequence records with a length deviating from the
#        majority of the sequences.

#Below code requires clustal files in R install directory

seq_lengths(`PT.AgPestF1-AgPestR1a`) # No obvious outliers
`PT.AgPestF1-AgPestR1a` <- filter_seqs(`PT.AgPestF1-AgPestR1a`, min_length = 200)

seq_lengths(`PT.AgPestF2-AgPestR2`) # No obvious outliers
`PT.AgPestF2-AgPestR2` <- filter_seqs(`PT.AgPestF2-AgPestR2`, min_length = 200)

seq_lengths(`PT.fwhF2-fwhR2n`) # No obvious outliers
`PT.fwhF2-fwhR2n` <- filter_seqs(`PT.fwhF2-fwhR2n`, min_length = 200)

seq_lengths(`PT.SauronS878-BR1`) # No obvious outliers
`PT.SauronS878-BR1` <- filter_seqs(`PT.SauronS878-BR1`, min_length = 200)

#Plot trees
t1 <- plot(`PT.AgPestF1-AgPestR1a`, ranks='class', main='PT.AgPestF1-AgPestR1a', rotate=45, size=1)
t2 <- plot(`PT.AgPestF2-AgPestR2`, ranks='class', main='PT.AgPestF2-AgPestR2', rotate=45, size=1)
t3 <- plot(`PT.fwhF2-fwhR2n`, ranks='class', main='PT.fwhF2-fwhR2n', rotate=45, size=1)
t4 <- plot(`PT.SauronS878-BR1`, ranks='class', main='PT.SauronS878-BR1', rotate=45, size=1)

Fig3 <- t1+t2+t3+t4
#on this plot we can see: 
#Off target amplifications
#Longer branch length = higher resolution


# 3.3. - Evaluate the taxonomic coverage and the specificity of the primers within the Actinopterygii class (i.e. Actinopteri class based
#        on the NCBI nomenclature). Also evaluate the taxonomic resolution of the primers at the genus level and correct for length of the
#        barcode to allow for comparisons between primers. Add additional columns to the input file for all calculated statistics.

dat.I$PT.Specificity <- rep("", nrow(dat.I))
dat.I$PT.PWDistance <- rep("", nrow(dat.I))
dat.I$PT.Length <- rep("", nrow(dat.I))
dat.I$PT.Resolution <- rep("", nrow(dat.I))
dat.I$PT.Order <- rep("", nrow(dat.I))
dat.I$PT.Family <- rep("", nrow(dat.I))
dat.I$PT.Genus <- rep("", nrow(dat.I))

for (i in 1:nrow(dat.I)) {
  tmp1 <- paste("PT", dat.I$Name[i], sep = ".")
  TAXID.All <- length(unique(as.data.frame(get(tmp1)$taxonomy)$taxId))
  TAXID.Act <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$taxId))
  dat.I$PT.Specificity[i] <- round((TAXID.Act / TAXID.All) * 100, digits = 2)
  dat.I$PT.PWDistance[i] <- as.numeric(calc_rank_dist_ave(get(tmp1), ranks = c("genus")))
  dat.I$PT.Length[i] <- as.integer(mean(get(tmp1)$BLAST_result$product_length))
  dat.I$PT.Resolution[i] <- as.numeric(dat.I$PT.PWDistance[i]) / as.numeric(dat.I$PT.Length[i])
  dat.I$PT.Order[i] <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$order))
  dat.I$PT.Family[i] <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$family))
  dat.I$PT.Genus[i] <- length(unique(subset(as.data.frame(get(tmp1)$taxonomy), subset = class == "Insecta")$genus))
}

#   B. Primer specificity (i.e. the percentage of unique Actinopterygii species out of the total number of unique species recovered)

gg.specificity <- ggplot(dat.I, aes(x = factor(Name, levels = Name), y = as.numeric(PT.Specificity))) +
  geom_bar(stat = "identity", fill = "gray50") +
  geom_hline(aes(yintercept = 90), linetype = "dashed", colour = "red3") +
  coord_cartesian(ylim = c(30, 100)) +
  ggtitle("B") +
  ylab("% of unique Actinopterygii species") +
  theme_classic() +
  theme(
    plot.title = element_text(size = 8, colour = "black", face = "bold"),
    axis.title.x = element_blank(),
    axis.title.y = element_text(size = 7, colour = "black"),
    axis.text.x = element_text(size = 7, colour = "black", angle = 90, hjust = 1, vjust = 0.5),
    axis.text.y = element_text(size = 7, colour = "black"),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_line(size = 0.25, colour = "black"),
    axis.ticks.length = unit(0.1, "cm")
  )
gg.specificity

#   C. Taxonomic coverage (i.e. no. of Actinopterygii orders for which sequences were obtained)

gg.coverage <- ggplot(dat.I, aes(x = factor(Name, levels = Name), y = as.numeric(PT.Order))) +
  geom_bar(stat = "identity", fill = "gray50") +
  geom_hline(aes(yintercept = 30), linetype = "dashed", colour = "red3") +
  coord_cartesian(ylim = c(0, 40)) +
  ggtitle("C") +
  ylab("No. of Actinopterygii orders") +
  theme_classic() +
  theme(
    plot.title = element_text(size = 8, colour = "black", face = "bold"),
    axis.title.x = element_blank(),
    axis.title.y = element_text(size = 7, colour = "black"),
    axis.text.x = element_text(size = 7, colour = "black", angle = 90, hjust = 1, vjust = 0.5),
    axis.text.y = element_text(size = 7, colour = "black"),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_line(size = 0.25, colour = "black"),
    axis.ticks.length = unit(0.1, "cm")
  )
gg.coverage

```


```{r sessioninfo}
sessionInfo(package = NULL)
```
