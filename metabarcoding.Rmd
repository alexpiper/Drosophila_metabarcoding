---
title: "Drosophila Metabarcoding"
author: "A.M. Piper"
date: "2019/04/05"
output:
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
  
---


```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE,fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/workprojects/PHD/Metabarcoding/Dros_metabarcoding')
setwd('C:/Users/ap0y/Dropbox/workprojects/PHD/Metabarcoding/Dros_metabarcoding')
opts_chunk$set(dev = 'png')
```

# Introduction 

```{r install & Load packages} 
#Set required packages
.cran_packages <- c("ggplot2", "gridExtra","tidyverse","scales","stringdist","patchwork","vegan","ggpubr","seqinr","viridis")
.bioc_packages <- c("dada2", "phyloseq", "DECIPHER","Biostrings","ShortRead","psadd")
#.github_packages <- c("metacal", "taxreturn", "piperline")

#.inst <- .cran_packages %in% installed.packages()
#if(any(!.inst)) {
#   install.packages(.cran_packages[!.inst])
#}
#.inst <- .bioc_packages %in% installed.packages()
#if(any(!.inst)) {
#  if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#  BiocManager::install(.bioc_packages[!.inst], ask = F)
#}
#
#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

#devtools::install_github("benjjneb/dada2")
#devtools::install_github("alexpiper/taxreturn")
#devtools::install_github("thomasp85/patchwork")

library(taxreturn)
```

# Calculate index switch rate

While using unique dual-indices will allow detection and removal of the majority of index switch reads, there will still be low level undetectable index switching present at a rate of obs/exp^2 (ref- ). to determine this rate, we will first calculate the unexpected index combinations compared to the expected. 

Fastq files contain the index information for each read in the read header, and therefore to get all undetermined indices, both switched and otherwise erroneous we can summarise the index sequences for each read as contained in the fasta header:

@M03633:307:000000000-D4262:1:1101:19524:28535 1:N:0:**GAGACGAT+GTTCTCGT**
ATACTGTGCGTACTGCAGATCGGAAGAGCACACGTCTGAACTCCAGTCACGAGACGATATCTCGTATGCCGTCTT 
+ 
BBBB?FFFBABAEGGGGGGGGGGGGGFHHHHHHGHHHGHHHHHHHHHHHHGGEEEEEAFGGHGHFAHHHHGGGH

First we need to demultiplex the data again allowing no mismatches - This needs to be done with bcl2fastq rather than the default illumina miseq lociratefastq workflow, as the workflow doesnt include indices in fastq file headers

BASH:
```{bash demultiplex 1 mismatch}
###BASH###

#raise amount of available file handles
ulimit -n 4000

#Miseq Run 1 - Testing Primers
/home/ap0y/usr/local/bin/bcl2fastq -p 12 --runfolder-dir /group/sequencing/190331_M03633_0310_000000000-CB3DR  --output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run1_CB3DR --sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run1_CB3DR/SampleSheet_CB3DR.csv --no-lane-splitting --barcode-mismatches 0

#Miseq Run 2 - Testing replicate tagged primers & SynMock
/home/ap0y/usr/local/bin/bcl2fastq -p 12 --runfolder-dir /group/sequencing/190628_M03633_0331_000000000-CK3HD  --output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run2_CK3HD --sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run2_CK3HD/SampleSheet_CK3HD.csv --no-lane-splitting --barcode-mismatches 0

#Miseq Run 3 - Ladder spike ins
/home/ap0y/usr/local/bin/bcl2fastq -p 12 --runfolder-dir /group/sequencing/190722_M03633_0336_000000000-CJKFJ/  --output-dir /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run3_CJKFJ --sample-sheet /group/pathogens/Alexp/Metabarcoding/dros_metabarcoding/data/run3_CJKFJ/SampleSheet_CJKFJ.csv --no-lane-splitting --barcode-mismatches 0
```


Then we can summarise the switch rate by counting unassigned reads

First you need to change the grep call to specifically call the sequencers ID:

Agribio miseq 1:
Agribio miseq 2: @M03633
Agribio NovaSeq: @A00878

```{bash undetermined}
###BASH###
#Run 1
#summarise undetermined reads from R1 undetermined file
zcat Undetermined_S0_L001_R1_001.fastq.gz | grep '^@A00878' | cut -d : -f 10 | sort | uniq -c | sort -nr > undetermined.txt

# summarise correctly determined reads from all other R1 files 
rm determined.txt
ls | grep "R1_001.fastq.gz" | sort | grep -v 'Undetermined' > test_ls_F

let files=$(grep -c "fastq.gz" test_ls_F)

declare -i x

x=1
while [ $x -le $files ] 
    do

query=$(sed -n "${x}p" test_ls_F)

sample_name=$(echo $query | awk -F . '{ print $1}')

stats=$(zcat $(echo $query) | grep '^@A00878' | wc -l)
echo $query $stats >> determined.txt

let x=x+1

done 

```

To differentiate unused indices arising from switching, from unused indices arrising from other phenomena, we can compare the undetermined count file to all possible combinations of i5 and i7 indices that could be produced through switching 

Can i just bind the read numbers onto the samplesheet here? - can get from a HTML file in /group/sequencing/191015_A00878_0012_AHLVKYDMXX/Unaligned-Single8-Lane1/Reports/html/HLVKYDMXX/all/all/all


```{r index switching, eval=TRUE}
#Read in original sample sheet
SampleSheet <- read_csv("Run4_nova/SampleSheet_run4.csv",skip=20)

##For special case of popgen spikein
SampleSheet <- read_csv("Run3_spikein/SampleSheet_run3.csv",skip=20)  %>%
  filter(str_detect(Sample_Name,pattern="-"))

#Create all possible switched combinations
combos <- unique(expand.grid(SampleSheet$index, SampleSheet$index2))
combos$indices <- paste0(combos$Var1,"+",combos$Var2)

#Determined reads from mock communities
determined <- read_table2("Run4_nova/determined.txt",col_names = FALSE) %>%
  set_colnames(c("Sample_Name","count")) %>%
  mutate(Sample_Name = Sample_Name %>% str_split_fixed("_S",n=2) %>% as_tibble() %>% pull(V1)) %>%
  mutate(Sample_Name = str_replace(Sample_Name, pattern="HLVKYDMXX_", replacement = "")) %>% #remove FCID from novaseq
  left_join(SampleSheet, by="Sample_Name") %>%
  unite(indices,c("index","index2"), sep="+") %>%
  select(c("Sample_Name","count","indices"))

#Undetermined reads from mock communities
undetermined <- read_table2("Run4_nova/undetermined.txt",col_names = FALSE) %>%
  set_colnames(c("count","indices")) %>%
  mutate(Sample_Name = "Undetermined_S0_R1_001.fastq.gz")

head(undetermined)

indices <- rbind(determined,undetermined)

#Calculate total read count for run
total_reads <- sum(indices$count)

#get unused combinations resulting from index switching
switched <- left_join(combos,indices,by="indices") %>%
  drop_na()
colnames(switched) <- c("i7","i5","indices","Sample_Name","count")

#get unused combinations resulting from other phenomena
other <- indices[!indices$indices %in% combos$indices, ]

#Count number of other undetermined
other_reads <- sum(other$count)

##Summary of index switching rate
exp_rate <- switched %>% 
  filter(!str_detect(Sample_Name,"Undetermined"))
obs_rate <- switched %>% 
  filter(str_detect(Sample_Name,"Undetermined"))

switch_rate <- (sum(obs_rate$count)/sum(exp_rate$count))
message(switch_rate)

#Rate of undetected switching should be switch_rate squared
filt_threshold <- switch_rate^2
message(paste0("The threshold for filtering will be: ",filt_threshold))

#Plot switching

gg.switch <- ggplot(data = switched, aes(x = i7, y = i5), stat="identity") +
  geom_tile(aes(fill = count),alpha=0.8)  + scale_fill_viridis(name = "reads", begin=0.1,trans = "log10")  + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5), legend.position = "none") +
  labs(title= "Novaseq run", subtitle = paste0("Total Reads: ", total_reads, " Switch rate: ", sprintf("%1.2f%%", switch_rate*100)))


#Plot pooling

gg.pooling <- ggplot(data=determined, aes(x=fct_rev(Sample_Name),y=count),stat="identity") + 
  geom_bar(aes(fill=count),stat="identity")  + 
  scale_fill_viridis(name = "reads", begin=0.1) + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5))+ 
  geom_hline(aes(yintercept = mean(count)))  +
  xlab("Sample")+
  ylab("number of reads") +
  labs(title= "Pooling", subtitle = paste0("Total Reads: ", total_reads, " Average reads: ",  sprintf("%.0f",mean(determined$count))," Standard deviation: ", sprintf("%.0f",sd(determined$count)))) +
  coord_flip()


##Special case - plot pooling of popgen spiked

popgen <- determined %>%
  filter(!str_detect(Sample_Name,pattern="-"))

gg.popgen <- ggplot(data=popgen, aes(x=fct_rev(Sample_Name),y=count),stat="identity") + 
  geom_bar(aes(fill=count),stat="identity")  + 
  scale_fill_viridis(name = "reads", begin=0.1) + 
  theme(axis.text.x = element_text(angle=90, hjust=1), plot.title=element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5))+ 
  geom_hline(aes(yintercept = mean(count)))  +
  xlab("Synthetic sample")+
  ylab("number of reads") +
  #labs(title= "Pooling", subtitle = paste0("Total Reads: ", total_reads, " Average reads: ",  #sprintf("%.0f",mean(determined$count))," Standard deviation: ", sprintf("%.0f",sd(determined$count)))) +
  coord_flip()


```


#Quality checks:

```{r}
#Sequencing run quality using BasecallQC package
#

## Sample quality using fastqc
library(ngsReports)
taxreturn::fastqc_install()
test <- taxreturn::fastqc(fq.dir=trimmedpath, threads=2)

# ngsreports of fastqc
fileDir <- file.path("data/run_4/FASTQC")
writeHtmlReport(fileDir, overwrite = TRUE, quiet=FALSE)
```

https://www.bioconductor.org/packages/release/bioc/vignettes/savR/inst/doc/savR.pdf


BasecallQC package https://bioconductor.org/packages/devel/bioc/vignettes/basecallQC/inst/doc/basecallQC.html

also fastqc: fastqcr


# Demultiplex by primer & trim

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Prior to begining this workflow, samples were demultiplexed and illumina adapters were removed by the MiSeq software, however primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

In this study there were 2 amplicons of different size, and 3 different replicate primers of each. For this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files.

## fwhF2-fwhR2n amplicon:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2T1_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	  GAGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T2_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	 TGTGGDACWGGWTGAACWGTWTAYCCHCC
    fwhF2T3_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	AGAAGGDACWGGWTGAACWGTWTAYCCHCC
    
REVERSE PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2nT1_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	ACGTRATWGCHCCDGCTARWACWGG
    fwhR2nT2_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	TCCGTRATWGCHCCDGCTARWACWGG
    fwhR2nT3_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	CTGCGTRATWGCHCCDGCTARWACWGG
    
```{r primer trimming , message=FALSE}
#Install bbmap
bbmap_install()

#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

#Demultiplex samples
runs <- dir("data/", pattern="run")

i=1

#Create vectors to track reads
trimmed <- vector("list", length=length(runs))
demux <- vector("list", length=length(runs))

for (i in seq(along=runs)){
  path <- paste0("data/", runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  
  run_data <- samdf %>%
    filter(FCID== runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Get primer sequences
  primers <- c(unique(run_data$F_seq), unique(run_data$R_seq))
  
  #Check if samples were twin tagged - these require extra round of demultiplexing
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
      demuxpath <- file.path(path, "demux") # Filtered forward files go into the path/filtered/ subdirectory
      dir.create(demuxpath)
      
      fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
      fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))
      
      demux[[i]] <- bbdemux(install="bin/bbmap", fwd=fastqFs, rev=fastqRs, Fbarcodes = unique(run_data$twintagF),
                    Rbarcodes = unique(run_data$twintagR), degenerate=TRUE, out.dir=demuxpath, threads=1 ,
                    mem=4,  hdist=0, overwrite=TRUE, tidylog = TRUE)
      
      demux_fastqs <- sort(list.files(paste0(demuxpath), pattern="_R1R2_", full.names = TRUE))
    
      trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd=demux_fastqs, 
                    primers = primers, 
                    degenerate = TRUE, out.dir="trimmed", trim.end = "left",
                    ordered=TRUE, mink=FALSE, hdist=2, 
                    maxlength= (max(run_data$readlength) - sort(nchar(primers), decreasing=FALSE)[1]) +5,
                    overwrite=TRUE, quality=FALSE, tidylog=TRUE)
      
      #Re-split interleaved fastq's
      trimmedpath <- file.path(demuxpath, "trimmed") # Filtered forward files go into the path/filtered/ subdirectory
      trimmed_fastqs <- sort(list.files(trimmedpath, pattern="_R1R2_", full.names = TRUE))
      bbsplit(install="bin/bbmap", files=trimmed_fastqs, overwrite=TRUE)
  
      
  } else if (twintagged == FALSE) {
    
    fastqFs <- sort(list.files(paste0(path), pattern="_R1_", full.names = TRUE))
    fastqRs <- sort(list.files(paste0(path), pattern="_R2_", full.names = TRUE))
    
    trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd=fastqFs, rev=fastqRs,
                    primers=primers, 
                    degenerate = TRUE, out.dir="trimmed", trim.end = "left",
                    ordered=TRUE, mink=FALSE, hdist=2,
                    maxlength=(max(run_data$readlength) - sort(nchar(primers), decreasing=FALSE)[1]) +5,
                    overwrite=TRUE, quality=FALSE, tidylog=TRUE)
    }
}
  
write_tsv(bind_rows(demux), "logs/demux.tsv")
write_tsv(bind_rows(trimmed), "logs/trimmed.tsv")
  
```
  
  
## Plot read quality & lengths
  
  
```{r QA plot, eval = TRUE, cache= TRUE}
runs <- dir("data/", pattern="run")
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)


for (i in seq(along=runs)){
  run_data <- samdf %>%
    filter(FCID== runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  
  if (twintagged == TRUE) {
    path <- paste0("data/", runs[i], "/demux/trimmed" )# CHANGE ME to the directory containing primer trimmed fastq files
  } else if (twintagged == FALSE) {
    path <- paste0("data/", runs[i], "/trimmed" )# CHANGE ME to the directory containing primer trimmed fastq files
  }
 
  ##Get trimmed files, accounting for empty files (28 indicates empty sample)
  trimmedFs <- sort(list.files(path, pattern="_R1_", full.names = TRUE))
  trimmedFs <- trimmedFs[file.size(trimmedFs)>28]

  #Plot an aggregate quality of random samples
  sampleF <- sample(trimmedFs, 12)
  sampleR <- sampleF %>% str_replace(pattern="_R1_", replacement = "_R2_")
  
  p1 <- plotQualityProfile(sampleF, aggregate = FALSE) + ggtitle(paste0(runs[i], " Forward Reads")) 
  p2 <- plotQualityProfile(sampleR, aggregate = FALSE) + ggtitle(paste0(runs[i], " Reverse Reads"))
  
  #output plots
  dir.create("output")
  dir.create("output/figures/")
  pdf(paste0("output/figures/", runs[i], "_prefilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  dev.off()
}

```

In gray-scale is a heat map of the frequency of each quality score at each base position. The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same lenghth, hence the flat red line).

The forward reads are good quality. We generally advise trimming the last few nucleotides to avoid less well-controlled errors that can arise there. These quality profiles do not suggest that any additional trimming is needed. We will truncate the forward reads at position 240 (trimming the last 10 nucleotides).

The reverse reads are of significantly worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as DADA2 incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities crash will improve the algorithm’s sensitivity to rare sequence variants. Based on these profiles, we will truncate the reverse reads at position 160 where the quality distribution crashes.

## Filter and trim

The max expected error function is used as the primary quality filter, and all reads containing N bases were removed

Should be using trunclength to make sure the amplicons are the same length despite heterogeneity filtering!


```{r filter and trim}
runs <- dir("data/", pattern="run")
filtered_out <- vector("list", length=length(runs))

samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

for (i in 1:length(runs)){
  
   run_data <- samdf %>%
    filter(FCID== runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  
  if (twintagged == TRUE) {
    path <- paste0("data/", runs[i], "/demux/trimmed" )
  } else if (twintagged == FALSE) {
    path <- paste0("data/", runs[i], "/trimmed" )
  }

  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  dir.create(filtpath)
  fastqFs <- sort(list.files(path, pattern="R1_001.*"))
  fastqRs <- sort(list.files(path, pattern="R2_001.*"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpath, fastqFs),
                                      rev=file.path(path, fastqRs), filt.rev=file.path(filtpath, fastqRs),
                                      maxEE=2, truncLen = 120, maxN = 0,
                                      rm.phix=TRUE, rm.lowcomplex=0,
                                      multithread=TRUE, compress=TRUE, verbose=TRUE)

  # post filtering plot
  filtFs <- sort(list.files(filtpath, pattern="R1_001.*", full.names = TRUE))
  #filtRs <- sort(list.files(filtpath, pattern="R2_001.*", full.names = TRUE))
  
  sampleF <- sample(filtFs, 12)
  sampleR <- sampleF %>% str_replace(pattern="R1_001", replacement = "R2_001")
  
  p1 <- plotQualityProfile(sampleF, aggregate = FALSE) + ggtitle(paste0(runs[i]," Forward Reads")) 
  p2 <- plotQualityProfile(sampleR, aggregate = FALSE) + ggtitle(paste0(runs[i]," Reverse Reads"))
  
  #output plots
  dir.create("output/figures/")
  pdf(paste0("output/figures/",runs[i],"_postfilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  dev.off()
}

filtered_out %>%
  map(as_tibble, rownames=NA) %>%
  map(rownames_to_column, var="Sample") %>%
  bind_rows() %>%
  write_tsv("logs/filtered.tsv")
  print(filtered_out)

```


# Sequence processing

## Infer sequence variants for each run

The divisive partition algorithm is initial- ized by placing all unique sequences into a single partition and assigning the most abundant sequence as the center of that partition. All unique sequences are then compared to the center of their partition, error rates are calculated and stored, and the abundance p-value is calculated for each unique sequence. If the smallest p-value, after Bonferroni correction, falls below the user-settable threshold OMEGA_A, a new partition is formed with the unique sequence with the smallest p-value as its center, and all unique sequences are compared to the center of that new partition. After a new partition is formed, every unique sequence is
allowed to join the partition most likely to have produced it (i.e., the partition that produces the highest expected number of that unique sequence). At that point, the division procedure iterates, with each iteration consisting of identifying the unique sequence with the smallest p-value, forming a new partition with that sequence as its center, and reshuffling sequences to their most likely partition. Division continues until all abundance p-values are greater than
OMEGA_A; i.e., all unique sequences are consistent with being produced by amplicon sequencing the center of their partition. The inferred composition of the sample is then the set of central sequences and the corresponding total abundances of those parti- tions (alternatively, each read is denoised by replacing it with the central


DADA2 depends on a param- eterized error model (the 16 × 41 transition probabilities, for example, p(A→C, 35)), but if parameters are not known a priori then DADA2 can estimate them from the data. Given an inferred partition of the amplicon sequences, DADA2 records the mismatches between every sequence and the center of its partition and counts each type of mismatch (for example, the number of A→C mismatches where Q = 35). The resulting table of observed mismatches represents the errors inferred by DADA2 and can be used to estimate the parameters of the error model. 

DADA2’s default parameter estimation method is to perform a weighted loess fit to the regularized log of the observed mismatch rates as a function of their quality, separately for each transition type (for example, A→C mismatches are fit separately from A→G mismatches).

The problem with novaseq data is the binned quality scores.

NovaSeq error rate conversions

0-2 -> 2
3-14 -> 12
15-30 -> 23
31-40 -> 37

However, the error rate estimation function is a modular part of the algorithm, and users can provide their own R function to estimate the parameters of the error model from the observed mismatches if they prefer a different method.

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check. In this plot you generally want to see if the fitted error rates (black line) reasonably fit the observations (black points) and generally decrease with increasing Q (towards right of plot)?

The purpose of priors is to increase sensitivity to a restricted set of sequences, including singleton detection, without increasing false-positives from the unrestricted set of all possible amplicon sequences that must be considered by the naive algorithm

NOTE: Try a comparison between using the conventional pooling, pseudo pooling, and using the reference database as a prior 

```{r Learn error rates }
runs <- dir("data/", pattern="run")
set.seed(100)

samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

for (i in seq(along=runs)){
  
  run_data <- samdf %>%
    filter(FCID == runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
    filtpath <- paste0("data/", runs[i], "/demux/trimmed/filtered" )
  } else if (twintagged == FALSE) {
    filtpath <- paste0("data/", runs[i], "/trimmed/filtered" )
  }
  
  filtFs <- list.files(filtpath, pattern="R1_001.*", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.*", full.names = TRUE)
  
  # Learn error rates from a subset of the samples and reads (rather than running self-consist with full dataset)
  # nread tells the function how many reads to use in error learning, this can be increased for more accuracy at the expense of runtime
  
  errF <- learnErrors(filtFs, multithread = 20, nbases = 1e+09, randomize = TRUE, qualityType = "FastqQuality", verbose=TRUE)
  errR <- learnErrors(filtRs, multithread = 20, nbases = 1e+09, randomize = TRUE, qualityType = "FastqQuality", verbose=TRUE)
  
  ##Print error plots to see how well the algorithm modelled the errors in the different runs
  print(plotErrors(errF, nominalQ=TRUE) + ggtitle(paste0(runs[i], " Forward Reads")))
  print(plotErrors(errR, nominalQ=TRUE) + ggtitle(paste0(runs[i], " Reverse Reads")))
  
  #check if any run uses a platform with binned quality scores - if so enforce monotonicity
  binnedqual <-  any(run_data$seq_platform %in% c("Novaseq", "Nextseq"))
  if (binnedqual == TRUE){
  enforce_mono <- function(err){
    err.mat <- getErrors(err, detailed=TRUE)
    for(trans in c("A2C", "A2G", "A2T", "C2A", "C2G", "C2T", "G2A", "G2C", "G2T", "T2A", "T2C", "T2G")) {
      #Transform each error rate that is below the model value at the max Q score (40) to the model value at that max Q score.
      err.mat$err_out[trans,] <- pmax(err.mat$err_out[trans,], err.mat$err_out[trans,ncol(err.mat$err_out)])
    }
    return(err.mat)
  }
  
  mono.errmatF <- enforce_mono(errF)
  mono.errmatR <- enforce_mono(errR)
  
  print(plotErrors(mono.errmatF, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Forward Reads"))
  print(plotErrors(mono.errmatR, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Reverse Reads"))

  errF <- mono.errmatF
  errR <- mono.errmatR
  }
  
  #Error inference and merger of reads - Using pseudo pooling for increased sensitivity
  dadaFs <- dada(filtFs, err=errF, multithread=20, pool="pseudo", verbose=TRUE)
  dadaRs <- dada(filtRs, err=errR, multithread=20, pool="pseudo", verbose=TRUE)
 
  mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE, minOverlap = 12)
  
  # Construct sequence table
  dir.create("output/rds/")
  seqtab <- makeSequenceTable(mergers)
  saveRDS(seqtab, paste0("output/rds/",runs[i], "_seqtab.rds"))
}
```

## Merge Runs, Remove Chimeras and filter

All the below filters increase the proportion of reads classified to lower levels compared to higher levels

```{r merge runs and remove chimeras}
seqtabs <- list.files("output/rds/", pattern="seqtab.rds", full.names = TRUE)

#Read in and rename undetermined - using dummy variables
for (i in seq(along=seqtabs)){
  assign(paste("st", i, sep = ""), readRDS(seqtabs[i]))
  labelling <- get(paste("st", i, sep = ""))
  labels <- rownames(labelling)%>%
    str_replace(pattern="Undetermined_", replacement = paste("Undetermined_",i,"_"))
  rownames(labelling)<-labels
  assign(paste("st", i, sep = ""), labelling)
}

st.all <- mergeSequenceTables(st1, st2, st3, st4, st5)

#Test collapsed
st.all <- collapseNoMismatch(st.all, minOverlap = 20, orderBy = "abundance",
                                  vec = TRUE, verbose = TRUE)

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)

#Check output of chimera removal
print(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

#Check complexity
hist(seqComplexity(seqtab.nochim), 100)

#Look at seqlengths
plot(table(nchar(getSequences(seqtab.nochim))))

#cut to expected size allowing for some codon indels
seqtab.nochim <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% 200:210]

#Filter for stop codons
seqs <- DNAStringSet(getSequences(seqtab.nochim))
codon_filt <- taxreturn::codon_filter(seqs)
seqtab.nochim <- seqtab.nochim[,colnames(seqtab.nochim) %in% codon_filt]

#Filter for homology with the target marker
fwh_ref <-  ape::read.dna("reference/fwh_insecta_aligned_curated.fasta", format="fasta")
model <- aphid::derivePHMM(fwh_ref)

seqs <- as.DNAbin(DNAStringSet(colnames(seqtab.nochim)))
homology_filt <- taxreturn::clean_seqs(seqs, minscore = 100, shave = FALSE, model = model)

seqtab.nochim <-  seqtab.nochim[,colnames(seqtab.nochim) %in% homology_filt]

dir.create("output/rds/")
saveRDS(seqtab.nochim, "output/rds/seqtab_final.rds") # CHANGE ME to where you want sequence table saved
```


## Assign taxonomy with IDTAXA & Exact matching

We will use the IDTAXA algorithm of Murali et al 2018 - https://doi.org/10.1186/s40168-018-0521-5

This requires training on a curated reference database - The pre-trained file can be found in the reference folder, alternatively see the taxreturn scripts to curate a reference database and train a new classifier.

Folllowing assignment with IDTAXA, we will also use exact matching with a reference database to assign more sequences (including the synthetic positive controls) to species level

```{r IDTAXA}
#Run
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")

trainingSet <- readRDS("reference/merged_arthropoda_idtaxa.rds")
dna <- DNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs

##Decide on threshold
ids <- IdTaxa(dna, trainingSet, processors=10,threshold = 60, verbose=TRUE)  #WARNING - assigning more than one processor currently crashes R

writeRDS(ids, "ids.RDS")
#plot(ids, trainingSet)


#delete existing file
cat("",file="idtaxa.csv")
for (i in 1:length(ids)){
 lines <- as.data.frame(t(cbind(ids[[i]]$taxon,ids[[i]]$confidence)))
 rownames(lines) <- c("taxa","confidence")
write.table(lines,file="idtaxa.csv",sep=",",append=TRUE, col.names=FALSE)
}

ranks <-  c("Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") # ranks of interest
#Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
tax <- t(sapply(ids, function(x) {
        taxa <- x$taxon
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))

library(stringi)
tax <- stri_list2matrix(lapply(tax, unlist), byrow=TRUE, fill=NA)

#Add sequences and column names to matrix
colnames(tax) <- ranks; rownames(tax) <- getSequences(seqtab.nochim)

#Subset to remove the root rank
tax <- subset(tax, select=c("Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species"))

#Propagate high order ranks to unassigned ASV's
tax <- propagate_tax(tax,from="Phylum") 


#Check Output
taxa.print <- tax # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxa.rds") 

tax <- readRDS("output/rds/tax_IdTaxa.rds") 

#Add missed species using exact matching

#
exact <- assignSpecies(seqtab.nochim, "reference/merged_rdp_species_synsadded.fa.gz", allowMultiple = TRUE, tryRC = TRUE, verbose = FALSE)

exact <- exact %>% 
  as_tibble() %>%
  mutate(binomial =  case_when(!is.na(Species) ~  paste0(Genus,"_",Species)))

###Assign synthetics using exact matching

exact <- assignSpecies(seqtab.nochim, "reference/inhouse_syns.fa", allowMultiple = TRUE, tryRC = TRUE, verbose = FALSE)

exact <- exact %>% 
  as_tibble() %>%
  mutate(binomial =  case_when(!is.na(Species) ~  paste0(Genus,"_",Species)))


#merge together
#For exact where Species is not NA, replace tax$Species where Species contains K__,P__,C__,O__,F__,G__
pattern <- c("K__","P__","C__","O__","F__","G__")
for (row in 1:nrow(tax)){
  if   (str_detect(tax[row,7], paste(pattern, collapse="|")) && !is.na(exact$binomial[row]) == TRUE ) {
  tax[row,7] <- exact$binomial[row]
  }
}

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxaExact.rds") 

```

## Make phylogenetic tree

```{r phylogenetic tree}
#seqtab.nochim <- readRDS("output/rds/seqtab_final_Run2.rds")

seqs <- getSequences(seqtab.nochim)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

library(phangorn)
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)

## negative edges length changed to 0!

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)

# Write taxonomy table to disk
saveRDS(fitGTR, "phytree.rds") 

```


## Track reads through process

```{r }

bbdemux_reads <- read_tsv("logs/bbdemux_tidy.tsv") %>% 
  separate(sample, into="sample", sep="_S", extra= "drop") %>%
  separate(sample, into=c("dir", "sample"), sep="X_", extra= "drop") %>%
  select(-dir) %>%
  dplyr::rename(input_reads_demux = input_reads)%>%
  dplyr::rename(input_bases_demux = input_bases)


bbtrim_reads <- read_tsv("logs/bbtrim_tidy.tsv") %>%
  separate(sample, into=c("sample", "rep"), sep="_Rep", extra= "drop")%>% 
  separate(sample, into="sample", sep="_S", extra= "drop") %>%
  separate(sample, into=c("dir", "sample"), sep="X_", extra= "drop")%>%
  select(-dir)  %>%
  dplyr::rename(input_reads_trim = input_reads)%>%
  dplyr::rename(input_bases_trim = input_bases)
  
sample_tracker <- right_join(bbdemux_reads, bbtrim_reads)

#could make the above functions parse out objects in a list (ie one object for each quality) and then i join them to the samplesheet here
 
```


## Make Phyloseq object

Following taxonomic assignment, the sequence table and taxonomic table are merged into a single phyloseq object alongside the sample info csv.

We then make a plot to evaluate the effectiveness of taxonomic assignment to each rank

```{r create PS, eval = FALSE}
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")

#Fix seqtab names -removing read name, sample number etc
rownames(seqtab.nochim) <- rownames(seqtab.nochim) %>% 
  str_split_fixed("_",n=Inf) %>%
    as_tibble() %>% 
  separate(V7, into="rep", sep = "\\.", extra = "drop") %>%
  unite(col=SampleID, c("V2","rep"),sep="-") %>%
  pull(SampleID) %>%
  str_replace(pattern="Rep", replacement="rep")

tax_plus <- readRDS("output/rds/tax_IdTaxaExact.rds") 

#### Rename problematic samples
rownames(seqtab.nochim)  <- rownames(seqtab.nochim) %>%
  str_replace_all("D250M1-", "D250M4REP-") %>%
  str_replace_all("D250M2-", "D250M5REP-") %>%
  str_replace_all("D250M3-", "D250M1REP-") %>%
  str_replace_all("D250M4-", "D250M2REP-") %>%
  str_replace_all("D250M5-", "D250M3REP-") %>%
  str_replace_all("D500M1-", "D500M4REP-") %>%
  str_replace_all("D500M2-", "D500M5REP-") %>%
  str_replace_all("D500M3-", "D500M1REP-") %>%
  str_replace_all("D500M4-", "D500M2REP-") %>%
  str_replace_all("D500M5-", "D500M3REP-") %>% # This should maybe be M2?
  str_replace_all("D1000M1-", "D1000M3REP-") %>%
  str_replace_all("D1000M2-", "D1000M4REP-") %>%
  str_replace_all("D1000M3-", "D1000M5REP-") %>%
  str_replace_all("D1000M4-", "D1000M1REP-") %>%
  str_replace_all("D1000M5-", "D1000M2REP-") %>%
  str_replace_all("REP", "")

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) %>%
  filter(!duplicated(sample_id)) %>%
  filter(FCID== "HLVKYDMXX") %>% # change for other runs
  magrittr::set_rownames(.$sample_id) %>%
  dplyr::select(c("sample_id", "ExtractID",
                  "geo_loc_name", "material", 
                  "target_subfragment", "F_primer", "R_primer",
                  "FCID", "seq_platform_ID"))
#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))

if(nrow(seqtab.nochim) > nrow(sample_data(ps))){warning("Warning: All samples not included in phyloseq object, check sample names match the sample metadata")}

rownames(samdf)[which(!rownames(sample_data(ps))  %in% rownames(samdf))]

saveRDS(ps, "output/rds/ps_idtaxaExact.rds") 

#Rename synthetic orders
tax_table(ps)[,2][which(str_detect(tax_table(ps)[,7], "Synthetic"))] <- "Arthropoda"

ps <- ps %>%
  subset_samples(material %in% c("Drosophila Adults", "Drosophila Larvae", "Mixed Adults", "Mixed Larvae", "Synthetic", "Blank")) %>%
  subset_taxa(Phylum == "Arthropoda") %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) 

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

##Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
library(data.table)
summarise_taxa(ps, "Species", "sample_id") %>%
  filter(str_detect(sample_id, "NTC")) %>%
  #filter(str_detect(Species, "Drosophila|Scaptodrosophila")) %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

summarise_taxa(ps, "Genus", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

##Output fasta of all ASV's - Name each one by abundance + Taxonomic assignment

ps_to_fasta(ps, "output/all_taxa.fasta", rank="Species")
```


### Summarise taxonomic assignment

```{r sum taxa}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name", rank_names(ps)) %>%
  group_by(Rank) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(Reads_classified = sum(Abundance * !is.na(Name))) %>%
  mutate(Frac_reads = Reads_classified / sum(sample_sums(ps))) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

#Fraction of ASV's assigned to each taxonomic rank
sum_otu <- tax_table(ps) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

print(sum_reads)
print(sum_otu)


ps_test <- speedyseq::tax_glom(ps, "Species")
sum_test <- tax_table(ps_test) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)


unique_sp <- unique(tax_table(ps_test)[,7]) %>% unname() %>% as.data.frame() %>% filter(!str_detect(V1, "__"))

```


## Process replicates 

In this section we will estimate within sample consistency using Kulczynski distance, which is a presence/absense distance measure.

To ensure the reproducibility of detection, all PCR replicates that had a high Kulczynski distance to other replicates within the same sample were removed.

Following this, we only retained ASV's that were present in at least 2 different replicates from each sample

Adapted from Mike mclarens code: https://github.com/benjjneb/dada2/issues/745

This could probably go before taxonomic assignment?

```{r replicates}
#ps <- readRDS("output/rds/ps_idtaxaExact.rds")

#Handle replicataes
rm_samples <- c("Undetermined")
ps1 <- subset_samples(ps, sample_names(ps) !=rm_samples) # Drop Undetermined reads
ps1 <- prune_samples(sample_sums(ps1)>=20, ps1) # Drop empty samples

#Calculate for each primer
#ps1 <- subset_samples(ps1,target_subfragment=="fwhF2-fwhR2n")
#ps1 <- subset_samples(ps1,target_subfragment=="fwhF2-HexCOIR4")

#Calculate kulczynski distance - A presence absense measure of detection 

  kdi <- phyloseq::distance(ps1, method="kulczynski")
  
  kdimap <- as.data.frame(as.matrix(kdi)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)
  
  #Make heatmap plot
  gg.kdimap <- ggplot(data = kdimap, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("kulczynski distance")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank(),
          legend.position = "none")

# Merge replicates
  ps.merged <- ps1 %>%
    merge_samples(group = "ExtractID")

## keeping only those ASVs that occur in 2 replicates
## Create a matrix of 0s and 1s indicating whether the taxon count should be
## allowed, or should be set to 0.
#ps.merged.ok <- ps1 %>%
#    transform_sample_counts(function (x) (x > 0) * 1) %>%  #Summarise presence/absense across reps
#    merge_samples(group = "ExtractID") %>% #Merge reps
#    transform_sample_counts(function (x) (x > 1) * 1) #Only keep those occuring in 2 or more replicates
#
###Test export
###    write.csv(psmelt(ps.merged.ok),"test.csv")
##    
### Multiply the counts by the 0-1 matrix
#newotu <- otu_table(ps.merged) * otu_table(ps.merged.ok)
#
#otu_table(ps.merged) <- otu_table(newotu, taxa_are_rows = FALSE)
#

#This loses the sample metadata - Need to add it agian
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE)  %>%
  #filter(FCID== "CK3HD2") %>% # change for other runs
  filter(!duplicated(ExtractID))  %>%
  magrittr::set_rownames(.$ExtractID) %>%
  dplyr::select(c("sample_id", "ExtractID",
                  "geo_loc_name", "material", 
                  "target_subfragment", "F_primer", "R_primer",
                  "FCID", "seq_platform_ID"))

sample_data(ps.merged) <- samdf
ps.merged <- filter_taxa(ps.merged, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table


# After merging kulczynski distance 

  kdi2 <- phyloseq::distance(ps.merged, method="kulczynski")
  
  kdimap2 <- as.data.frame(as.matrix(kdi2)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)
  
  #Make heatmap plot
  gg.kdimap2 <- ggplot(data = kdimap2, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("kulczynski distance post replicate merge")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1)) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank(),
          legend.position = "none")
  

```

## Check for concordance between mock


```{r PCA}

# Output drosophila summary
ps.merged %>% 
  subset_taxa(Family=="Drosophilidae") %>%
  microbiome::transform("compositional") %>%
  summarise_taxa("Species", "sample_id") %>%
  filter(!str_detect(sample_id, "NTC")) %>%
  #filter(str_detect(Species, "Drosophila|Scaptodrosophila")) %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/drosophila_spp_sum.csv")

# Read in expected table
exp <- read_csv("sample_data/expected_quant.csv") %>%
  gather(Species, Abundance, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  filter(str_detect(X1,"D100M|D250M|D500M|D1000M|DLarv")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Species","Actual"))

# PCA on exp
test <- exp %>%
  pivot_wider(
    names_from = Sample,
    values_from = Actual,
    values_fill = list(Actual=0),
  ) %>%
  column_to_rownames("Species") %>%
  #group_by(Sample, Species) %>%
  nest(data = everything()) %>%
  mutate(pca = map(data, ~ prcomp(.x, 
                                  center = TRUE, scale = TRUE)))

library(broom)
test$pca %>%
  map(~tidy(.x, data = .y, "pcs")) %>%
  as.data.frame() %>%
  ggplot(aes(x = PC, y=percent)) +
  geom_bar(stat="identity") + labs(x="PC",y="Variance Explained")

test %>% 
  mutate(pca_aug = map2(pca, data, ~augment(.x, data = .y))) %>%
  unnest(pca_aug)

test %>% 
  mutate(pca_aug = map2(pca, data, ~augment(.x, data = .y))) %>%
  unnest(pca_aug) %>%
  ggplot(aes(x=.fittedPC1, y=.fittedPC2)) + geom_point(aes(color=.rownames),size=3, alpha=0.7) +
  labs(x="PC1", y="PC2") +
  theme(legend.position="top")

# Mutate real one
sam <- ps.merged %>%
  speedyseq::tax_glom("Species") %>%
  transform_sample_counts(function (x) x/sum(x)) %>%
  speedyseq::psmelt() %>%
  mutate(Species = str_replace(Species, pattern="Drosophila_mauritiana/simulans", replacement= "Drosophila_simulans")) %>%
  filter(Sample %in% exp$Sample) %>%
  arrange(Abundance) %>%
  distinct() 

```


## Process synthetic mock positive control



Identification of taxa that are poorly represented in an unsupervised manner can identify taxa that will have little to no effect on downstream analysis. Sufficient removal of these "low prevalance" features can enhance many analysis by focusing statistical testing on taxa common throughout the data. However, for our dataset the problem with prevalence filtering for our dataset is that we dont have many replicates of each psyllid species, and therefore there are some high abundance but low prevalence ASV's we dont want to lose.

#

```{r prevalence-assessment}
# Calculate taxon prevalence across the data set
prevdf <- apply(X = otu_table(ps.merged), MARGIN = ifelse(taxa_are_rows(ps.merged), yes = 1, no = 2),FUN = function(x){sum(x > 0)})

# Add taxonomy and total read counts to prevdf - change this to tidyverse code
prevdf <- data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps.merged), tax_table(ps.merged))

#Prevalence plot
gg.prev <- subset(prevdf, Order %in% get_taxa_unique(ps.merged, "Order")) %>%
  ggplot(aes(TotalAbundance, Prevalence / nsamples(ps.merged),color=Genus)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_x_log10() +
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Order) +
  theme(legend.position="none") +
  ggtitle("Phylum Prevalence in All Samples\nColored by Family")

gg.prev
```

## ASV Filtering

* remove samples with low reads
* Remove all OTUS with an abundance less than 0.01 (or index switch estimate)
  -This could be better estimated with an ROC curve of false positives and false negatives in mock communities?
* Remove OTUs that are found in less than 20% of samples- For Quantitative analysis not for detection

# Determine filtering threshold

We will use 3 different methods to estimate the cross contamination rate:

* Firstly we will use the synthetic mock community positive control
* Secondly we will use the estimate from the number of expected vs unapplied index combinations
* Thirdly, we will use ROC analysis of false positives and false negatives for all taxa in the mocks.

## estimate cross contamination using positive control

```{r}
#Process positive controls
Syn_taxa <- c("Synthetic_Acrididae", "Synthetic_Aphididae", "Synthetic_Apidae", "Synthetic_Cerambycidae", "Synthetic_Crambidae", "Synthetic_Culicidae","Synthetic_Drosophilidae","Synthetic_Nitidulidae","Synthetic_Siricidae","Synthetic_Tephritidae", "Synthetic_Thripidae", "Synthetic_Tortricidae", "Synthetic_Triozidae", "Carpophilus_Synthetic1", "Carpophilus_Synthetic2", "Carpophilus_Synthetic3", "Carpophilus_Synthetic4", "Carpophilus_Synthetic5", "Carpophilus_Synthetic6","Drosophila_Synthetic1", "Drosophila_Synthetic2", "Drosophila_Synthetic3", "Drosophila_Synthetic4", "Drosophila_Synthetic5", "Drosophila_Synthetic6")
#Estimate switching from positive controls
pos_switchrate <- psmelt(subset_taxa(ps.merged, Species %in% Syn_taxa)) %>%
  group_by(geo_loc_name) %>%
  summarise(Abundance = sum(Abundance))

##Plot synthetics
ps_syn <- subset_samples(ps.merged, material=="Synthetic") %>%
  subset_taxa(Species %in% Syn_taxa) %>%
  filter_taxa(function(x) mean(x) > 0, TRUE) %>% #Drop missing taxa from table
  microbiome::transform('compositional')

#Colour scheme
library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(13)

gg.syn <- plot_bar(ps_syn, fill="Species") +
  facet_grid(~target_subfragment,scales="free") + 
  scale_fill_manual(values=col) +
  theme_bw()
```

# Estimate rate using Expected vs observed combinations


# Estimate rate using known exp vs observed in mocks

From the use of the mock community we can see that the majority cross contamination must have come from DNA extraction and prior processes. Therefore we will also get an estimate 

```{r thresholds}
#Flag True positives


# Read in expected table
exp <- read_csv("sample_data/expected_quant.csv") %>%
  gather(Species, Abundance, -X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  filter(str_detect(X1,"D100M|D250M|D500M|D1000M|DLarv")) %>%
  drop_na() %>%
  set_colnames(c("Sample","Species","Actual"))

# Mutate real one
sam <- ps.merged %>%
  transform_sample_counts(function (x) x/sum(x)) %>%
  speedyseq::tax_glom("Species") %>%
  speedyseq::psmelt() %>%
  mutate(Species = str_replace(Species, pattern="Drosophila_mauritiana/simulans", replacement= "Drosophila_simulans")) %>%
  filter(Sample %in% exp$Sample) %>%
  arrange(Abundance) %>%
  distinct() %>%
  left_join(exp, by=c("Sample", "Species")) %>%
  mutate(switched = case_when(
    Abundance > 0 & Actual == 0 ~ TRUE,
    Abundance == 0 & is.na(Actual) ~ FALSE,
    Abundance > 0 & Actual > 0 ~ FALSE
  ))

# Need to make a confusion matrix instead

## Plot true and false carsonellas by run - this shows may need to filter seperately by run, as seqrun2 has much higher than others.
gg.switch <- sam %>% 
  group_by(switched) %>%
  filter(!is.na(switched)) %>%
  filter(Abundance > 0) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ggplot(aes(x=seqrun, y=freq, group=switched, fill=switched)) +
  geom_bar(stat="identity")


#to check if index switching could explain the multiple OTU's check if top=FALSE carsonella exist as top in another sample in the same run

thresholds <- seq(0,0.1,0.0001)

df <- data.frame(thresh = thresholds, TP= thresholds, FP = thresholds)
for(i in 1:length(thresholds)){
  filt <- sam %>%
    filter(Abundance > thresholds[i]) %>%
    group_by(switched) %>%
    summarise(sum=n())
  df$FP[i] <- filt$sum[2]
  df$TP[i] <- filt$sum[1]
}

gg.filt <- df %>%
  bind_rows() %>%
  gather(key=type, value=n, -thresh) %>%
  ggplot(aes(x=thresh, y=n, fill=type)) + 
  geom_density(stat="identity", alpha=0.5) + 
  #scale_x_continuous(breaks=seq(0,0.05,0.001)) +
  theme(axis.text.x = element_text(angle=45,hjust=1)) + 
  geom_vline(xintercept = 0.0035)+ 
  geom_vline(xintercept = 0.001)

print(gg.filt)

## Filter run 2 
run2_ps <- ps2 %>% 
  subset_samples(seqrun==2)

run2_pass <- run2_ps %>%
    transform_sample_counts(function (x) x/sum(x)) %>%  # Convert to proportions
    transform_sample_counts(function (x) (x > 0.001) * 1)

newotu1 <- otu_table(run2_ps) * otu_table(run2_pass)

run2_newps <- run2_ps
otu_table(run2_newps) <- otu_table(newotu1, taxa_are_rows = FALSE) 

#Filter run 1 and 3 
  
run13_ps <- ps2 %>%
    subset_samples(!seqrun == 2)
                   
run13_pass <- run13_ps %>%
    transform_sample_counts(function (x) x/sum(x)) %>%  # Convert to proportions
    transform_sample_counts(function (x) (x > 0.0005) * 1)

newotu2  <- otu_table(run13_ps) * otu_table(run13_pass)

run13_newps <- run13_ps
otu_table(run13_newps) <- otu_table(newotu2, taxa_are_rows = FALSE) 

# Create new phyloseq and drop missing taxa
ps3 <- merge_phyloseq(run2_newps, run13_newps) %>%
  filter_taxa(function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps3 <- prune_samples(sample_sums(ps3) >0 , ps3) # Drop empty samples

#Count number of overall taxa pre and post filtering
print(paste(ntaxa(ps2) - ntaxa(ps3), " taxa Dropped when using filtering threshold of: ", 0.0035, " for run 2 and ", 0.001, "for run 1 and 3"))

# Count number of carsonella OTU's per sample pre and post filtering
n_carson <- speedyseq::psmelt(ps2) %>% 
  filter(Genus == "Candidatus_Carsonella") %>%
  filter(Abundance > 0) %>%
  dplyr::select(Sample.Name, OTU) %>%
  group_by(Sample.Name) %>%
  add_tally() %>%
  dplyr::select(-OTU) %>%
  unique() %>%
  mutate(type = "pre") %>%
  bind_rows(.,speedyseq::psmelt(ps3) %>% 
  filter(Genus == "Candidatus_Carsonella") %>%
  filter(Abundance > 0) %>%
  dplyr::select(Sample.Name, OTU) %>%
  group_by(Sample.Name) %>%
  add_tally() %>%
  dplyr::select(-OTU) %>%
  unique() %>%
  mutate(type = "post"))

gg.ncarson <- ggplot(n_carson, aes(x=reorder(Sample.Name, -n), y=n)) + 
  geom_bar(stat="identity") +
  facet_grid(~type) + 
  xlab("Sample.Name") +
  ylab("Number of carsonella OTU's per sample") +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0))

print(gg.ncarson)

#check if any samples dont have carsonella
table(!n_carson$Sample.Name %in% speedyseq::psmelt(ps2)$Sample.Name)

## Get the name of taxa that dont have carsonella after filtering

```


# Part 1: Primer comparison

## Comparison between primers for detection

Doesnt seem to be any scaptodrosophila? need to do exact matching

```{r}
#For run 1
#rm_samples <- c("Undetermined")
#ps1 <- subset_samples(ps, sample_names(ps) !=rm_samples) # Drop Undetermined reads
#ps1 <- prune_samples(sample_sums(ps1)>=20, ps1) # Drop empty samples
#ps2 <- tax_glom(ps1,taxrank="Species") # Change to ps.merged for later runs

#Agglomerate to species
ps2 <- speedyseq::tax_glom(ps.merged,taxrank="Species") # Change to ps.merged for later runs

#For run2 
rm_samples <- c("fwhF2-fwhR2n-CM4","fwhF2-HexCOIR4-CM4")
ps2 <- subset_samples(ps2, !sample_names(ps2) %in% rm_samples) # Drop Undetermined reads

ps2 <- filter_taxa(ps2, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps2 <- prune_samples(sample_sums(ps2)>=20, ps2) # Drop empty samples


#Rough barplot
col2 <- colorRampPalette(brewer.pal(11, "Spectral"))(71)

rm_samples <- sample_names(ps2)[which(str_detect(sample_names(ps2),"blank"))]
psbar <- subset_samples(ps2, !sample_names(ps2) %in% rm_samples) # Drop Undetermined reads

Fig1 <- plot_bar(psbar,fill="Species") +
  theme_bw() +
    #scale_fill_manual(values=col2) + 
  theme(legend.position="none",
        axis.text.x = element_text(angle=90))
#Change sample names for plotting
#
#sample_names(ps2) <- sample_names(ps2) %>%
##  str_split_fixed("-",n=3) %>%
#  as_tibble() %>%
#  pull(V3)

gg.hmap <- plot_heatmap(ps2, "jaccard", "jsd", taxa.label="Species", na.value=NA, taxa.order="Family")   +
  theme_bw() +
  theme(axis.text.x = element_text(angle=60, hjust=1),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        panel.spacing =unit(0.2, "lines"),
        legend.position = "bottom",
        legend.direction = "horizontal") +
  facet_grid(~target_subfragment,scales="free",space="free", drop=TRUE) +
    scale_fill_distiller(palette="Reds", direction= 1, trans = 'log10',  na.value = NA)


#Summarise this also with a jaccard distance comparison!
library(RColorBrewer)
col <- colorRampPalette(brewer.pal(11, "Spectral"))(20)

ps.ord <- subset_taxa(ps2, !Species %in% Syn_taxa)
ps.ord <- filter_taxa(ps.ord, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
ps.ord <- prune_samples(sample_sums(ps.ord)>=20, ps2) # Drop empty samples
GP.ord <- ordinate(ps.ord, "NMDS", "jaccard")
p1 = plot_ordination(ps.ord, GP.ord, type="samples", color="ExtractID", title="Comparison of 2 primer sets") + 
  theme_bw() +  
  theme(legend.position = "none") +
  #scale_color_manual(values=col)+ 
  geom_point(size=3) 

#,guide=FALSE
```

## Comparison between primers for bias

As part of the mock community analysis, we wish to determine taxonomic bias by looking at observed vs expected reads. To do this, we load dummy sequence, taxonomy, and sample data tables and create a seperate phyloseq object, which will later be merged

This loads a dummy sequence table, taxonomy table, and sample data table and merges it into the existing phyloseq object

Conducted as per: https://mikemc.github.io/metacal/articles/tutorial.html


The clr-transformed values are scale-invariant; that is the same ratio is expected to be obtained in a sample with few read counts or an identical sample with many read counts, only the precision of the clr estimate is affected. 

The G(x) cannot be determined for sparse data without deleting, replacing or estimating the 0 count values. Fortunately, there are acceptable methods of dealing with 0 count values as both point estimates using zCompositionsR package

```{r Drosophila bias}
#devtools::install_github("mikemc/metacal")
library(metacal)
library(tidyverse)

ps_bias <- subset_samples(ps2, geo_loc_name %in% c("Colony","Red Hill"))
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <- "Carpophilus_nr.dimidiatus"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"
ps_bias <- tax_glom(ps_bias, taxrank="Species")

primers <- as.character(unique(sample_data(ps2)$target_subfragment ))
plist <- vector("list", length(primers))
names(plist) = primers

i=1
for (i in 1:length(primers)){
  ps_primer <- subset_samples(ps_bias,sample_data(ps_bias)$target_subfragment  == primers[i])
  ps_primer <- filter_taxa(ps_primer, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
  
sam <- psmelt(ps_primer) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species)

exp <- read_csv("sample_data/Test_expected_quant.csv") %>%
  gather(Species,Abundance,-X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  mutate(X1 = str_split_fixed(X1,"-rep",n=2) %>%
           as_tibble()%>% 
           pull(V1)) %>%
  distinct %>%
  filter(str_detect(X1,"D100M")) %>% ###CHANGE TO CM for carpophilus D100M for DROS
  drop_na()
colnames(exp) <- c("Sample","Taxon","Actual")

#Join tables 
joint <- sam %>%
  filter(Taxon %in% exp$Taxon) %>%
  #filter(Abundance > 0) %>%
  left_join(exp, by = c("Sample","Taxon"))%>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual)

#Build error matrix
error_mat <- build_matrix(joint, Sample, Taxon, Error)

#Estimate bias
bias <- center(error_mat, enframe = TRUE) %>%
    dplyr::rename(Bhat = Center)

#Estimate uncertainty in bias estimate
bootreps <- bootrep_center(error_mat) %>%
    dplyr::rename(Bhat = Center)
bootreps.summary <- bootreps %>%
    group_by(Taxon) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))
bias0 <- left_join(bias, bootreps.summary, by = "Taxon")

#Test plot bias

p <- ggplot(bias0, aes(Taxon, y=Gm_mean-1,fill=Taxon)) +
    geom_bar(stat="identity")+
    geom_errorbar(aes(ymin = (Gm_mean-1) - (Gm_se-1), ymax = (Gm_mean-1) + (Gm_se-1), width=0.2)) +
    geom_point(aes(y=Gm_mean-1)) +
    scale_fill_brewer(palette="Spectral")+
    scale_colour_brewer(palette="Spectral") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0)) + 
  ylab("Bias") +
  ggtitle(primers[i]) +
  expand_limits(y = c(-2, 4)) +
  theme(legend.position = "none")
 
plist[[i]] = p
 
#Get pairwise bias
bias.pw <- bias %>%
    compute_ratios(group_vars = c()) %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":"))

#Get pairwise boostrap estimates
bootreps.pw <- bootreps %>%
    compute_ratios(group_vars = ".id")
summary.pw <- bootreps.pw %>%
    group_by(Taxon.x, Taxon.y) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))
bias.pw0 <- left_join(bias.pw, summary.pw, by = c("Taxon.x", "Taxon.y"))

#Plot bias estimates
ratios <- joint %>%
    compute_ratios %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":")) %>%
    filter(!is.nan(Error), Taxon.x < Taxon.y)
ratios.pred <- bias.pw0 %>%
    mutate(Pair = paste(Taxon.x, Taxon.y, sep = ":")) %>%
    filter(Taxon.x < Taxon.y)

gg.bias <- ggplot(ratios, aes(Pair, Error, color = Sample)) +
    geom_hline(yintercept = 1, color = "grey") +
    geom_pointrange(data = ratios.pred, aes(y = Bhat, 
            ymin = Bhat / Gm_se^2, ymax = Bhat * Gm_se^2), 
        color = "black") +
    geom_jitter(width = 0.2) +
    scale_y_log10() +
    coord_flip()

}
#Need to set a 

library(patchwork)
plist[[1]] + plist[[2]] 

plist[[1]] + plist[[2]] + plist[[3]] + plist[[4]] 

```


# Dros bias seperate primers
```{r Drosophila bias}
#devtools::install_github("mikemc/metacal")
library(metacal)
library(tidyverse)

##Test for plot of 3 primers - to change back remvoe below and change pcr_primers back to target_subregion below
ps2 <- tax_glom(ps,taxrank="Species")

ps_bias <- subset_samples(ps2, feature %in% c("Drosophilidae","Cherry"))
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Carpophilus_dimidiatus/nr.dimidiatus")] <- "Carpophilus_nr.dimidiatus"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp1")] <- "Brachypeplus_Sp"
tax_table(ps_bias)[,7][which(tax_table(ps_bias)[,7]=="Brachypeplus_Sp2")] <- "Brachypeplus_Sp"
ps_bias <- tax_glom(ps_bias, taxrank="Species")

primers <- as.character(unique(sample_data(ps2)$pcr_primers ))
plist <- vector("list", length(primers))
names(plist) = primers

i=1
for (i in 1:length(primers)){
  ps_primer <- subset_samples(ps_bias,sample_data(ps_bias)$pcr_primers  == primers[i])
  ps_primer <- filter_taxa(ps_primer, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table
  
sam <- psmelt(ps_primer) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species)

exp <- read_csv("sample_data/Test_expected_quant.csv") %>%
  gather(Species,Abundance,-X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  mutate(X1 = str_replace(X1, pattern="-rep",replacement="_Rep")) %>% # Remove for merged
 # mutate(X1 = str_split_fixed(X1,"-rep",n=2) %>%
 #          as_tibble()%>%    #Put back in for merged
 #          pull(V1)) %>%
#  distinct %>%
  filter(str_detect(X1,"D100M")) %>% ###CHANGE TO CM for carpophilus D100M for DROS
  drop_na()
colnames(exp) <- c("Sample","Taxon","Actual")

#Join tables 
joint <- sam %>%
  filter(Taxon %in% exp$Taxon) %>%
  #filter(Abundance > 0) %>%
  left_join(exp, by = c("Sample","Taxon"))%>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual)

#Build error matrix
error_mat <- build_matrix(joint, Sample, Taxon, Error)

#Estimate bias
bias <- center(error_mat, enframe = TRUE) %>%
    dplyr::rename(Bhat = Center)

#Estimate uncertainty in bias estimate
bootreps <- bootrep_center(error_mat) %>%
    dplyr::rename(Bhat = Center)
bootreps.summary <- bootreps %>%
    group_by(Taxon) %>%
    summarize(Gm_mean = gm_mean(Bhat), Gm_se = gm_sd(Bhat))
bias0 <- left_join(bias, bootreps.summary, by = "Taxon")

#Test plot bias

p <- ggplot(bias0, aes(Taxon, y=Gm_mean-1,fill=Taxon)) +
    geom_bar(stat="identity")+
    geom_errorbar(aes(ymin = (Gm_mean-1) - (Gm_se-1), ymax = (Gm_mean-1) + (Gm_se-1), width=0.2)) +
    geom_point(aes(y=Gm_mean-1)) +
    scale_fill_brewer(palette="Spectral")+
    scale_colour_brewer(palette="Spectral") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle=90,hjust=1,vjust=0)) + 
  ylab("Bias") +
  ggtitle(primers[i]) +
  expand_limits(y = c(-2, 4)) +
  theme(legend.position = "none")
 
plist[[i]] = p

}
#Need to set a 

library(patchwork)
(plist[[1]] + plist[[2]] + plist[[3]]) / (plist[[4]] + plist[[5]] + plist[[6]])
```


```{r calibrate bias}
sam <- psmelt(ps_primer) %>%
  arrange(Abundance)%>%
  mutate(Taxon = Species) #%>%
 #filter(str_detect(ExtractID,"D100M|Chery")) ###CHANGE TO CM for carpophilus D100M for DROS

exp <- read_csv("sample_data/Test_expected_quant.csv") %>%
  gather(Species,Abundance,-X1) %>%
  mutate(Species = str_replace(Species, pattern=" ",replacement="_")) %>%
  mutate(X1 = str_split_fixed(X1,"-rep",n=2) %>%
           as_tibble()%>%
           pull(V1)) %>%
  distinct %>%
  filter(str_detect(X1,"D100M")) %>% ###CHANGE TO CM for carpophilus D100M for DROS
  drop_na()
colnames(exp) <- c("Sample","Taxon","Actual")

#Subset to only those taxa desired for estimation
#controls <- c("fwhF2-fwhR2n-CM1","fwhF2-fwhR2n-CM2","fwhF2-fwhR2n-CM3","fwhF2-fwhR2n-CM5")#
#exp <- filter(exp, Sample %in% controls)
#sam <- sam %>%
#    mutate(Type = ifelse(Sample %in% controls, "Control", ""))

#Join tables 
joint <- sam %>%
  filter(Taxon %in% exp$Taxon) %>%
#  mutate(inmock = ifelse(Sample %in% controls, "Control", "")) %>%
  left_join(exp, by = c("Sample","Taxon"))%>%
  mutate(Actual = replace_na(Actual, 0)) %>%
  mutate(Observed0 = (Abundance + 0.5) * (Actual > 0)) %>%
  mutate(Error = Observed0 / Actual)

#Build error matrix from just control samples
error_mat <- build_matrix(joint, Sample, Taxon, Error) #%>% filter(Type=="Control")

#Estimate bias
bias <- center(error_mat, enframe = TRUE) %>%
    dplyr::rename(Bhat = Center)

#Subset to only taxa in controls
control_taxa <- unique(exp$Taxon) 
#Remove d immigrans
control_taxa <- control_taxa[!str_detect(control_taxa,pattern="Drosophila_immigrans")]

#Calibration
cal <- joint %>%
    left_join(bias, by = "Taxon") %>%
    mutate(Calibrated = Abundance / Bhat)


cal.prop <- cal %>%
    filter(Taxon %in% control_taxa) %>%
    group_by(Sample) %>%
    mutate_at(vars(Abundance, Calibrated,Actual), ~ . / sum(.)) 
#Visualise proportions before and after

plot_df <- cal.prop %>%
    #filter(Sample %in% samples) %>%
    gather("Type", "Proportion", "Abundance", "Calibrated", "Actual") %>%
    mutate(Type = factor(Type, c("Actual","Abundance", "Calibrated")))

gg.cal <- ggplot(plot_df, aes(Type, Proportion, fill = Taxon)) +
    geom_col() +
    facet_grid(~Sample) +
    theme_bw() +
    scale_fill_brewer(palette = "Spectral")


#also make another plot showing the bias in the estimates from the amount of controls used - see https://mikemc.github.io/mgs-bias-manuscript/analysis/costea2017-analysis.html


#Plot expected vs observed

correction <- cal.prop %>% select(Sample,Taxon,Actual,Abundance,Calibrated)

correction <- correction %>%
  gather(Type,Abundance,-Sample,-Taxon,-Actual)

g.cor <-ggplot(correction, aes(x=Actual,y=Abundance)) +
  geom_point(aes(fill=Type),size=3,alpha=0.8,shape=21,stroke=1,color="black") + 
  geom_abline(slope=1, intercept = 0) +
  stat_cor(aes(color=Type), label.x = 0.1)  + 
  xlim(0,1) + 
  ylim(0,1) + 
  scale_fill_manual(values=c("#A9A9A9","#ae0707")) + 
  theme_pubr() + 
  ylab("Observed")


#Might be worth trying this again with all genes in together?

#p1 <- g.cor
#p2 <- g.cor
#p3 <- g.cor
#p4 <- g.cor

pnocal  <- ggplot((correction %>% filter(Type=="Abundance")), aes(x=Actual,y=Abundance)) +
  geom_point(aes(fill=Type),size=3,alpha=0.8,shape=21,stroke=1,color="black") + 
  geom_abline(slope=1, intercept = 0) +
  stat_cor(aes(color=Type), label.x = 0.1)  + 
  xlim(0,1) + 
  ylim(0,1) + 
  scale_fill_manual(values=c("#A9A9A9","#ae0707")) + 
  theme_pubr() + 
  ylab("Observed")

#Output Pre- subset barplot

ps_presub <-  subset_samples(ps_primer, sample_names(ps_primer) =="fwhF2-fwhR2n-DM6SPD") # Drop Undetermined reads
ps_presub <- prune_samples(sample_sums(ps_presub)>=20, ps_presub) # Drop empty samples
ps_presub <- filter_taxa(ps_presub, function(x) mean(x) > 0, TRUE) #Drop missing taxa from table

col2 <- colorRampPalette(brewer.pal(11, "Spectral"))(26)

Fig1 <- plot_bar(ps_presub,fill="Species") +
  theme_bw() +
    scale_fill_manual(values=col2)


```

```{r}

# Part 2: Compositional data analysis
library(CoDaSeq)

#fwhf2-fwhr2n 
ps_primer <- subset_samples(ps.merged,sample_data(ps1)$target_subfragment =="fwhF2-fwhR2n")

# filter the dataset
f <- codaSeq.filter(as.data.frame(otu_table(ps_primer)), min.reads=1000, min.prop=0.005, min.occurrence=0, samples.by.row=FALSE)


# replace 0 values with an estimate
f.n0 <- cmultRepl(f, method="CZM", label=0)

# generate the CLR values
f.clr <- codaSeq.clr(f.n0)

#aitch <- acomp(as.data.frame(otu_table(ps_primer)) +1) # Add pseudocount
# proportions closed between 0 and 1 
#bal = clr(aitch) # isometric log-ratios 

adi = vegdist(f.clr, method="euclidean") # Aitchison dissimilarity matrix 

adimap <- as.data.frame(as.matrix(adi)) %>%
    rownames_to_column() %>%
    gather(key="colname",value="Distance",-rowname)

#Set exact matches to 1
#adimap$Distance[adimap$rowname==adimap$colname] <- 1

#gg.adimap <- NULL
gg.adimap <- ggplot(data = adimap, aes(x=rowname, y=colname, fill=-Distance)) + 
    geom_tile() + scale_fill_viridis() + 
    ggtitle(paste0("Aitchinson distance")) + 
    theme(axis.text.x=element_text(angle=90,hjust=1),
          axis.title = element_blank())


#Get group consistency

groups <- str_split_fixed(rownames(f.clr),"-",n=4) %>%
  as_tibble %>%
  unite(col="sample", c("V1","V2","V3"), sep="-") %>%
  pull(sample) %>%
  unique()



outlier <- codaSeq.outlier(f.clr)
  
outlier.list
for (i in groups){
  samples <- f.clr[which(str_detect(rownames(f.clr),pattern=groups[1])),]
  s.var <- codaSeq.outlier(samples,plot.me=FALSE)
  
  plot(density(s.var),main=groups[1],xlab="Var Fraction", ylab="Density")
  
}

#principle component

# perform a singular value decomposition
pcx <- prcomp(f.clr)
# plot a PCA biplot
# calculate percent variance explained for the axis labels
pc1 <- round(pcx$sdev[1]^2/sum(pcx$sdev^2),2)
pc2 <- round(pcx$sdev[2]^2/sum(pcx$sdev^2),2)
xlab <- paste("PC1: ", pc1, sep="")
ylab <- paste("PC2: ", pc2, sep="")
biplot(pcx, cex=c(0.6,0.4), var.axes=F,
    scale=1, xlab=xlab, ylab=ylab)


#Plot of the relationship between the OTUs from the SVD. Here each OTU is colored red if it was identified as having an effect size greater than 1

# calculation of SVD in R_Block_2
plot(pcx$rotation[,1], pcx$rotation[,2],
    main="loadings", xlab=xlab, cex=0.5,
    ylab=ylab, col=rgb(0,0,0,0.3), pch=19)
points(pcx$rotation[,1][high.e],
    pcx$rotation[,2][high.e], cex=0.8,
    col=rgb(1,0,0,0.5))
points(pcx$rotation[c("39306","39235"),1],
    pcx$rotation[c("39306","39235"),2],
    cex=0.5, col=rgb(0,0,1,1), pch=19)


#Cluster analysis

# anosim between groups using Aitchison distance
dist.clr <- dist(E.clr)
ano <- anosim(dist.clr, conds, permutations=999)
# coloring from https://rpubs.com/gaston/dendrograms
# make the dendrogram
hc <- as.dendrogram(hclust(dist.clr, method="ward.D2"))
hcd <- hclust(dist.clr, method="ward.D2")
# function to get color labels
colLab <- function(n) {
    if (is.leaf(n)) {
        a <- attributes(n)
        #labCol <- labelColors[clusMember[which(names(clusMember) == a$label)]]
        labCol <- if (grepl("ak", a$label) == TRUE ) "red" else "blue"
        attr(n, "nodePar") <- c(a$nodePar, lab.col = labCol)
    }
    n
}
# using dendrapply
clusDendro = dendrapply(hc, colLab)
plot(clusDendro, main="Aitchison distance, Ward.D2 Cluster")


#If we have found differences - Univariate tests using AlDex

par(mfrow=c(1,2))
plot(f.e$diff.win, f.e$diff.btw,
    pch=19, col=rgb(0,0,0,0.3),
    cex=0.5, main="effect plot",
    xlab="Dispersion", ylab="Difference")
points(f.e$diff.win[low.p],
    f.e$diff.btw[low.p],
    pch=19, col=rgb(0,0,1,0.5),
    cex=0.5)
points(f.e$diff.win[high.e],
    f.e$diff.btw[high.e],
    col=rgb(1,0,0,0.5), cex=0.8)
abline(0,1, lty=2, lwd=2,col="grey")
abline(0,-1, lty=2, lwd=2,col="grey")
plot(f.e$effect, f.t$we.eBH, log="y",
    pch=19, col=rgb(0,0,0,0.3),
    main="E vs p", xlab="effect size",
    ylab="E(p.adjust)", cex=0.5)
points(f.e$effect[low.p], f.t$we.eBH[low.p],
    pch=19, col=rgb(0,0,1,0.5),cex=0.5)
points(f.e$effect[high.e], f.t$we.eBH[high.e],
    col=rgb(1,0,0,0.5), cex=0.8)

```
